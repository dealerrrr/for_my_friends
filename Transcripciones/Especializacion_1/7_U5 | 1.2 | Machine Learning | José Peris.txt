Speaker 0 | 00:20.202
cómo evaluamos nuestro algoritmo. Aquí vamos a diferenciar dos grandes grupos y recordad que siempre hablando desde el punto de vista de aprendizaje supervisado cuando tiene etiqueta, cuando hay una mano detrás, cuando sabemos el resultado de antemano los grandes grupos, uno era la regresión y otro era la clasificación la regresión, pues cómo vamos a evaluarlo, pues bueno como podéis ver en la gráfica básicamente lo que tenemos son los data points y nuestro algoritmo se vería representado como es la línea verde que tenemos, es realmente esta línea verde, que esto es una regresión lineal, simplemente si os fijáis lo que ocurre es que pasa más o menos por el medio de la nube de puntos, ¿qué significa esto? Significa que esa recta tiene una fórmula matemática y esa fórmula matemática es la que nos permite predecir dónde va a caer el próximo punto. ¿Cómo medimos la precisión? Lo que hacemos es ver. nuestras predicciones frente a los resultados reales y lo que vamos a hacer es vamos a medir la distancia de cada punto real respecto a nuestra recta. y a partir de aquí midiendo esta distancia cada punto vamos a sacar una media y esa va a ser la media de nuestro error, esto cuando estamos hablando de regresión donde el valor sería 1824 euros, por ejemplo. ¿Cómo evaluamos la clasificación? La clasificación se evalúa mediante un instrumento llamado matriz de confusión y el nombre está muy buen puesto porque realmente estamos hablando que vamos a elaborar una matriz donde vamos a clasificar los diferentes resultados. En este caso, por ejemplo, vamos a tener dos columnas, una va a ser los resultados reales, y dos filas donde van a ser los resultados del algoritmo. Aquí, básicamente, lo que hacemos es contar cuántas veces hemos acertado y cuántas veces hemos fallado en la clasificación. Este concepto es un poco lioso, pero el hecho de que hemos pasado... a una pandemia ha ayudado a entenderlo un poco mejor, porque esto se basa de falsos positivos y falsos negativos. ¿Qué ocurre con el coronavirus, por ejemplo? Tenemos el caso de falso positivo y falso negativo, ¿no? A día de hoy todos sabemos ya lo que significa. Te pueden diagnosticar el coronavirus y no lo tenías. Eso sería un falso positivo. Pero también hay un falso negativo. te pueden diagnosticar que no tienes coronavirus y sí que lo tienes, ya has contagiado a todo tu entorno. Este es el concepto clave para la clasificación, porque cuando clasificamos si es sí o no, tenemos que ver si es un falso positivo o falso negativo. A continuación os voy a dejar aquí un ejemplo interactivo donde podéis un poco experimentar, experimentar lo que es el concepto de la matriz de confusión. Lo que vais a ver son dos grupos de imágenes, es un clasificador que representa imágenes que representan a comida y imágenes que no representan a comida. Lo que tenéis que hacer es clasificar las que van al lado. Tenéis que pensar que para evaluar una clasificación es muy importante tener en cuenta cuántos hemos acertado. En este caso miraríamos los... los true positive y los true negative y cuánto hemos fallado, los false positive o false negative. Lo que hace es comparar los que hemos acertado contra los que hemos fallado, lo que pasa que claro no es lo mismo fallar prediciendo una enfermedad que fallar prediciendo si una campaña de marketing va a funcionar sí o no, es aquí donde debemos establecer nuestro umbral. Existen otros métodos más avanzados pero a nivel general vamos a crear un método más avanzado. con estos conceptos base de medición y de evaluación de algoritmos. Vamos a seguir hablando de los dataset y sobre todo un concepto muy muy importante que es el entrenamiento y el test. Fijaros que estamos hablando que tenemos que medir y evaluar la predicción de nuestros algoritmos, ya sea en regresión como en clasificación. Para ello lo que vamos a hacer, nunca vamos a trabajar con el 100% de los datos. ¿A qué me refiero con esto? Imaginaros que me envían un dataset de 100.000 filas y 15 columnas, 15 variables. El primer paso que tenemos que hacer es... aparte de que primero haríamos un análisis del negocio, realizamos un proceso de ETL y lo siguiente que haríamos sería partir el dataset en entrenamiento y test. Hay diversas formas de partirlo, normalmente es un 80-20 aunque puede ser un 70-30. y hay formas de incluso partirlo más, entrenamiento, test y validación, pero en este punto en el que nos encontramos vamos a hablar de entrenamiento y test. Lo que vamos a hacer siempre es partir nuestro dataset, el 80% será de entrenamiento y el 20% será de test. ¿Esto qué significa? Que desde la fila 0 hasta la fila 80.000, van a ser nuestros datos de entrenamiento y desde la fila 80.000 hasta la 100.000 van a ser los datos de test. ¿Por qué? Porque lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos y una vez, una vez está entrenado, lo que vamos a hacer es imaginaros que íbamos, seguimos con el ejemplo, del algoritmo de predicción de si un cliente va a pagar un crédito o no, para saber si se lo podemos ofrecer o no. En este caso, lo que sucedería, que una vez nuestro algoritmo está entrenando con el 80% de los datos, nosotros simplemente cogeríamos los datos desde el test, ese 20% donde lo que veríamos realmente sería un ID de un usuario y todas sus características, todo su histórico, todo su histórico de cobros, de pagos, de ingresos de gasos con el banco y el algoritmo aquí haría una predicción. ¿Qué ocurre? Que como tenemos el dato real porque hemos partido en 80-20, podemos contrastar la predicción del algoritmo con el dato real, predicción contra realidad, siempre es este el patrón que seguimos, por tanto analizaríamos... dataPoint a dataPoint, ¿cuánto hemos acertado o cuánto hemos fallado? Si estamos hablando de regresión será un número, estaremos hablando de un error que este error pues haciendo la inversa es la precisión. Si estamos hablando de clasificación, en una primera instancia lo haríamos con una matriz de confusión y de forma más avanzada ya utilizaríamos análisis como el ROC o AUC. Pero nos vamos a quedar en esta base, en que para medir, para poder trabajar, tenemos que partir siempre en entrenamiento y en test. Vamos a hablar sobre la parte más importante. en Machine Learning y en Inteligencia Artificial, que son las variables. Recordad la estructura que es augmentada de un dataset, tendríamos muchas filas y tendríamos columnas. Estas columnas son las variables. Las variables realmente, de una forma entendible, serían aquellos parámetros que explican todo el contexto, es decir, vamos a llevarlo a cabo. un caso muy fácil, muy entendible como puede ser el caso de la hostelería. Bar, restaurante, un hotel... Pongamos que un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar a dos semanas vista. Si pensáis bien, si sabemos cuánto vamos a facturar, cuántos trabajadores vamos a necesitar, tenemos mucha ventaja competitiva porque realmente sabemos que no va a haber rotura. de stock, sabremos que va a haber un buen servicio, podemos preparar a sala y el personal para un determinado escenario, con lo cual estamos hablando de bastante retorno, esto es el roll de trabajar con Machine Learning y con IA. Sigamos en este caso, imaginaros que el caso común es que un cliente por ejemplo te dé su base de datos. y te dice, bueno, aquí tienes mi base de datos y lo que vas a tener es un timestamp, como hemos dicho anteriormente, porque tendremos fechas. tendremos un histórico de fechas, pongamos que tenemos 10 años y al lado tendremos una columna con la facturación de cada día. A día de hoy, lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo realmente a ojo. Entonces, ¿qué significa hacerlo a ojo? Pues bueno, que si calculamos un incremento o un descenso de x, porque este año es mejor o es peor. Esto nos lleva a que son predicciones muy inexactas, que rozan el 50%. Que 50% en Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada. Vamos a ver cómo configuraríamos este algoritmo, estas variables, para poder realizar un buen algoritmo. Lo primero es pensar con la lógica. Nosotros tenemos una fecha, un timestamp y una facturación, unos datos de facturación en euros. De una forma muy sencilla, ¿creéis que se factura lo mismo en hostelería un sábado o un viernes que un lunes o un martes? Evidentemente no, aquí un componente psicológico en el cual un lunes o un martes va a ser difícil que la gente acude más a los bares, sin embargo viernes o un sábado o un domingo sí, sabemos que obviamente esto va a afectar a la facturación. ¿En qué día de la semana estoy? Va a afectar directamente a mi facturación. Por tanto, el día de la semana es una variable muy importante que haciendo ETL la podemos añadir a nuestro dataset esto se traduciría si el día de la semana es el 1 o es el 7, o es el 2 o es el 3 ¿O qué otra variable podría afectar? Por ejemplo, la variable festivos por ejemplo, si es Halloween es obvio que ese día es probable que afecta la facturación o incluso la víspera Por tanto, el factor festivo en el calendario nacional es una variable también muy importante. Pero pensad que este bar está, este restaurante o este hotel está en España. Cuando está lloviendo, la gente va a los bares o a los restaurantes, le cuesta muchísimo más, nos cuesta salir de casa. Sin embargo, cuando hace sol, nos lanzamos a la calle. Por tanto, la... temperatura va a influir claramente también en nuestro objetivo, nuestro target, nuestra Y que se denomina Machine Learning que es la facturación. o por ejemplo las precipitaciones, la lluvia, que estaría también, lo podemos englobar dentro de esta misma variable, pero tenemos que tener en cuenta que hay otro tipo de variables que pueden afectar, porque imaginaros que alrededor de este restaurante se realiza un evento, hay un fin de semana de conciertos, donde vienen artistas de todo el mundo, ¿qué va a pasar? que va a haber mucha más gente alrededor Por lo tanto, esto en Matching Learning lo veríamos como evento, si hay evento sería un 1 y si no hay evento es un 0, por ejemplo. Es como traduciríamos esta información de forma binaria. Pero podrían afectar también otra serie de variables, como por ejemplo, qué día del mes es. No es lo mismo, no gastamos lo mismo el día 5 que el día 30. Los ánimos no son los mismos. Pero, por otro lado, también hay otras variables que podrían influir, que esto es Open Data, que son variables externas. Imaginaros que la tasa del paro se sitúa, juvenil, en un 40%. esto va a afectar también, esta variable externa, este OpenData va a afectar, o digamos que el PIP ha bajado muchísimo, todo este tipo, estos son lo que nosotros denominamos variables y es aquí donde está la génesis de la creación de un buen algoritmo y es realmente la clave, es comprobar qué variables son las que mejor explican este suceso o evento, a partir de aquí. ya podríamos, una vez hemos conformado nuestro dataset, hemos añadido esta serie de variables y hemos comprobado su importancia, podemos pasar a partir en entrenamiento y test y desarrollar nuestro algoritmo de nuestra caja de herramientas, tunearlo y conseguir la precisión adecuada. Para que os hagáis una idea, estaríamos hablando en términos cuantitativos, por ejemplo, en términos de precisión estaríamos hablando que si simplemente lo que vamos a tener es un timestamp de una columna con un valor de euros, pues podríamos situarnos en precisiones alrededor del 60%. En el momento que añadimos variables, variables que apuntan a qué paso en esos días y utilizamos Open Data, podríamos subir a precisiones del 90%, incluso del 95%. Lo cual, como podéis comprender, pues, como os decía, puede ser una ventaja competitiva muy potente. El campo del Machine Learning y la Inteligencia Artificial es un campo técnico, es cierto, pero no todo es desarrollo y tecnicismos. Realmente puede acceder gente desde el mundo del ámbito del negocio que va a tener que aprender una serie de técnicas que son perfectamente alcanzables. Al final, realmente lo que importa es una buena combinación de la parte táctica estratégica con la parte técnica.