Speaker 0 | 00:24.674
En el momento en el que ya no estás trabajando como tenemos nosotros ahora, que es una especie de laboratorio controlado, por así decirlo, en el que ya te vas a los entornos reales, mantener un clasificador de objetos, entrenado con los nuevos productos que van saliendo, posibles modificaciones que pueden tener, pues al final necesitas una base de datos, ya sea aumentada de manera artificial o aumentada de manera real, necesitas, digamos, un proceso muchísimo más costoso que el no tenerlo. Aparte, un supermercado normal, la cantidad de productos que puede tener es enorme y al final un detector de objetos, ahora mismo el state of the art, el top del top, puede tener un error del 15%, 10, 20 y no es algo que nosotros podamos jugar. Entonces el approach que hemos tomado es un poquito ahora, por ejemplo, modelos como Clip, que es, digamos, el cerebro de Dalí, toda esta generación de imágenes a partir de texto, pues el approach que siguen se llama Zero Shot, que se basa en que una vez que tú entrenas una red con unos datos, la entrenas de forma que luego es capaz de clasificar, por así decirlo, datos que nunca ha visto. Entonces nosotros tenemos, digamos, que el cerebro de la tienda a nivel de visión es un... una rendoronal que sabe que es un producto de un supermercado. Lo haya visto o no lo haya visto con anterioridad. Lo que hace la visión artificial no es leer el producto, ya que, evidentemente, no da la resolución de la cámara. O si pudiera dar, se necesitaría un material muchísimo más costoso. Aparte de que no sabemos dónde va a poner la mano el cliente. Por lo tanto, no nos la podemos jugar con algo que seguramente pase, que es que cuando tú coges un producto... lo tapas. Entonces, ¿en qué se basa la red neuronal? No lo sabemos, pero nosotros hemos entrenado una red neuronal que lo que hace es eso, le damos una foto de lo que tendría que haber en ese estante y lo comparamos con lo que tú nos has dado de información. Entonces, de esa forma sabemos si los dos productos forman parte de la misma clase o si difieren de clase o que se diferencian. Entonces, por lo tanto, jugando ya no sólo con cómo de similares son dos productos, sino también con... se diferencian podemos saber con una muy buena precisión de dónde la has cogido y qué es lo que has cogido. es simplemente quitarnos los píxeles que no forman parte del producto. Eso nos da más juego que un clasificador de objetos, por ejemplo, por cajas, como YOLO o SSD, porque nos quitamos automáticamente el ruido a la vez que sabemos los píxeles que forman parte del producto. Al final, nosotros lo que queremos es cualquier tipo de cosa que nos puede influir en la toma de decisiones, la queremos eliminar, porque imagínate que al final Puede ser que la diferencia entre un producto y otro sea una franja de color rojo y que justo la persona que lo coja lleve guantes de color rojo. Nos queremos quitar todo ese tipo de problemas. Y la máscara binaria nos está dando una muy buena... La segmentación al final nos da una muy buena solución. Pues nosotros ahora estamos trabajando con un sistema que se llama adaptación de dominios. Entonces nos permite eso, tener una dataset bastante humilde, porque la hemos recogido al final en un entorno muy controlado y somos pocos, pero nos permite crear datos artificiales. Y en la red neuronal se tiene en cuenta que en el proceso de entrenamiento que no se pare los dos tipos de datos como haría de forma natural, sino que a través de una penalización en el gradiente hacemos que los dos dominios se unan. dominios se unan. Entonces con eso perdemos un poco de precisión final pero como tenemos más datos lo recuperamos y conseguimos al final pues un modelo más robusto y con una precisión similar. Es un mapa de segmentación. Tenemos la imagen que detectamos nosotros con nuestros algoritmos y esa la segmentamos de manera manual. Hemos intentado pre-entrenarlo de manera semisupervisada y luego pasar a manera supervisada. Ahora mismo creo que la que estamos usando a día de hoy es completamente supervisada, pero cuando recojamos más datos, que es lo que nosotros estamos esperando para volver a entrenar todas las redes, probar diferentes arquitecturas muchísimo más potentes y luego además... Estamos trabajando con sensores de peso, por ejemplo, en cada producto tiene o cada X productos tenemos un... la cuantificación del peso, entonces también nos da, digamos, información extra sobre lo que ha escogido en caso de que no hay una buena imagen o al revés, utilizamos imagen en caso de que no haya habido, haya habido mucho ruido por ejemplo en las acciones. Los problemas que tenemos que resolver son tres, que es quién lo ha cogido, cuándo lo ha cogido y qué ha cogido. Entonces, digamos que ahora mismo el que nos estamos dando más, de bruces y es un poquito más complicado, es quién lo ha cogido. El saber quién eres tú, seguirte por la tienda, la tecnología que hay es bastante limitada con respecto al resto de redes neuronales, el proceso de investigación está tirando sobre todo por esa rama, que es la que nos hemos quedado más atrás, el saber cuándo lo has cogido nos lo dice la báscula, Lo bueno es que en cuanto detecta un cambio en el peso, entonces automáticamente las cámaras enfocan justo a ese punto. Y eso es muy bueno porque así no tenemos que detectar esqueletos, no tenemos que saber si lo que has metido es una mano, o yo qué sé, si ha sido un temblor en la tierra, o alguien que ha empujado una estantería. Y luego el que ha cogido, como te estaba comentando, es una mezcla entre esos sensores de peso que son muy buenos, pero efectivamente hoy en día pues la mayoría de productos, por ejemplo, de macarrones, espaguetis, pesan medio kilo. Pesan medio kilo o casi punto cero, o sea, se va muy poco hoy en día. Entonces, por lo tanto, el sensor de peso no nos da la suficiente información y ahí es donde entra la imagen. Nosotros somos capaces también de detectar acciones sospechosas, somos capaces de ser robustos, o sea, cuando dejas, por ejemplo, espaguetis, lo puedes dejar en una nevera, lo puedes dejar donde las galletas, donde tú quieras, y nosotros sabemos cuando otra persona lo coge, podemos avisar a un revisor de que has dejado un producto fuera de su sitio para que vaya exactamente donde está ese producto y que lo vuelva a dejar en su sitio, podemos avisar cuando un producto se ha acabado, porque, sobre todo, también digamos que realiza una acción sospechosa, como nosotros la llamamos, a partir de ahí tenemos cámaras. Por lo tanto, cualquier clasificador de acciones, que pueden parecer sospechosas o que intenta de alguna manera falsificar los sensores, nosotros somos completamente conscientes de ello y se pueden entrenar sistemas desde cero y va a ser muy complicado una falla de seguridad de ese tipo. ¡Gracias por ver el vídeo! Sí, al final, el reconocimiento facial estamos muy limitados, porque tampoco podemos poner una cámara, digamos, delante de la cara de las personas, nos parecía muy invasivo al principio, luego también es muy complicado relacionar la cara que yo estoy viendo, digamos, en un plano a nivel de los ojos, con una cámara que está en el techo, por ejemplo. Entonces, ahora la identificación que estamos haciendo es una mezcla entre tracking y reidentificación corporal entera. Nosotros sabemos cómo vamos vestidos, sé la cantidad de gente que hay en la tienda, aumento sus datos, para si acaso se cambian de ropa o lo que sea, y digamos que se hace a partir de ahí tu espacio, de quién eres tú. De tal manera que cualquier foto en la que te agaches, en la que te hayas quitado un gorro, por ejemplo, o la chaqueta, sigues estando en ese espacio y como te hacemos tracking, podemos detectar cualquier tipo de modificación. Las prendas o qué te describe a ti. Entonces podemos automáticamente actualizar. ¿Por qué estamos atrás? Porque ahora mismo aquí somos la gente que viene a ver la demo y tal, son dos personas, una persona, como mucho pues también hemos estado cuatro, entonces no hemos tenido en mi momento ese problema. Pero evidentemente ahora que ya nos estamos situando en el futuro de tener una tienda en la que pueden haber 50, 60 personas incluso. pues ahora mismo es lo que le estamos metiendo más caña. Otra parte interesante es cuando dos personas entran juntas con una cuenta, por ejemplo una madre con su hijo, un padre con su hija o lo que sea, pues saber que esas dos personas son la misma entidad, puedo así decirlo, en cuanto a... lista de la compra, todo eso afortunadamente ya está resuelto o tenemos la aproximación muy muy muy similar a la que va a ser al final. En cambio por ejemplo pues el saber quién eres pues aún estamos descubriendo y el otro día por ejemplo hicimos un hito de superar casi el doble precisión de lo que teníamos ahora y seguramente la semana que viene será aún más, entonces aún estamos descubriendo cosas nuevas en ese ámbito. Pues ahora mismo funcionando, digamos que casi en cada frame de la demo, puede haber perfectamente unas diez redes neuronales. Y también hay un par o tres de algoritmos de Machine Learning un poquito más clásico, incluso hay alguno que está basado más en, básicamente, unas operaciones realizadas con estadística bayesiana, simplemente no. Comparado con Deep Learning, pues evidentemente es más simple, pero no por ello más complejo. En su día, había un sistema enteramente, simplemente dependiente de la cámara, ahí había más redes neuronales incluso. pero también con sus limitaciones, que no podían, por ejemplo, las redes que estamos usando ahora están específicamente diseñadas para poder ejecutarse en sistemas muy pequeños, como por ejemplo, lo que pueda ser un Google Coral, lo que pueda ser un Nvidia Jetson, o alguna de estas single board computers que te van a permitir el tener un cómputo cada X metros de la tienda, un sistema centralizado, todo eso aún queda por decidir. Pero digamos que tenemos unas redes que están hechas para ser optimizadas. digamos, para que funcione a tiempo real y tal. Las redes antiguas no. Las redes antiguas eran cogidas de papers, implementadas, entrenadas, directamente cogidas los pesos de las implementaciones oficiales y simplemente cambiadas lo mínimo o lo justo para poderse utilizar en nuestro proyecto. No solo es que tenemos un sistema más inteligente, sino que además es más pequeño y se puede comprimir aún más para poder meterse dentro de sistemas embedidos. Yo creo que el reto más grande es depender de ideas felices. Al final tenías que ir a trabajar y tenías que tener la solución a este problema que seguramente nadie ha solucionado nunca. Por lo tanto, es coger ecuaciones que se utilizan en mecánica cuántica, por ejemplo, que es la de que nosotros no tenemos un sistema determinista, sino que tenemos unas probabilidades de existencia de algo. Entonces, digamos que al final la inspiración te venía de la... cosas más rebuscadas que supieras en tu consciente, que a lo mejor hace años que no la veías, de repente te daba la bombilla. pero el depender de ideas felices, el tener que solucionar problemas que no se han solucionado, utilizar redes neuronales o un sistema, por ejemplo, los sensores de peso, de una forma que no están hechos para funcionar, que es, por ejemplo, a muchas lecturas por segundo, los sensores de peso que nos hemos encontrado, pues por ejemplo, usarlos de esa manera nos ha costado mucho, todo eso al final son cosas que muy poca gente ha resuelto, que no hay nada de literatura al respecto, o muy poca literatura al respecto, y al final era depender de... de a ver que este fin de semana quién había dormido menos y el que tenía ya la idea feliz de decir, esto se va a hacer así. Y ha habido una suma de esas ideas felices que ha llegado a donde estamos ahora. El mayor problema que tenemos es a nivel de el hardware de las básculas, ¿vale? El sensor de peso pues digamos que da lecturas diferentes a diferentes temperaturas, la humedad no la lleva muy bien, entonces lo hemos resuelto y al final la temperatura va a ser un parámetro más en la digamos en el que nos da la aproximación del peso, la imagen. no pasa ningún problema, lo único problema que puede llegar a tener es la geometría diferente de las diferentes formas de mostrar el producto, pero también, pues como veréis, luego tenemos maquillaje, o tenemos productos colgados, entonces el tema de la diferente geometría lo hemos resuelto también y el tema del hielo, la descongelación es muy lenta, afortunadamente, entonces digamos que no modifica lo suficiente como para que de repente se produzca un salto y salten todas las cámaras y todo porque no es tan rápida. y luego en cuanto a la diferencia de peso, pues si no es una buena diferencia de peso, que nosotros hemos medido que no cambia tanto el peso, al final lo que da la otra parte es la imagen. Y luego encima tenemos la idea de montar un cebro por detrás, utilizando ya sea aprendizaje por refuerzo, o algunos sujetos de manera supervisada, o aún no lo tenemos muy claro, en el que se adapte el sistema a quien eres tú. y cuáles son tus acciones, de tal forma que pues aparte de que una persona media en un comercio se comporta de una manera, también vamos a poder saber cómo te comportas tú cuando vas a la compra, vamos a poder predecir qué acciones vas a realizar en la compra y vamos a poder predecir todo eso de tal forma que al final la pequeña parte que el sistema no pueda llegar la vamos a superar conociéndote quién eres tú y cómo se comporta un cliente. Nosotros ahora, con el sistema que tenemos, es lo suficientemente rápido, si bien es cierto que tendría que ser más rápido, pero estamos teniendo problemas en cuanto a colapso del puerto serial y cosas que nosotros, desgraciadamente, ninguno es experto, entonces, por lo tanto, tiene que entrar gente potente en esa parte, pero ahora la rapidez que tenemos nos permite, y la robustez que tenemos nos permite el enseñarte, ya sea en el futuro en la aplicación, ahora en una pantalla de televisión que tenemos encima de la demo, tu lista a tiempo real para que tú en el momento en el que cojas algo si quieres comprobar que todo vaya bien, lo puedas comprobar, veas que no está en su sitio o que vaya mal pues que puedas ir a una de las personas que esté trabajando en la tienda e informar de un error o desde la aplicación puedes informar del propio fallo y al final eso nos da muchísimo más libertad que por ejemplo. competidores como Amazon Go que te tienes que esperar pues unos 15 minutos, 5, 10, 20 a que te salga la lista de la compra. Nosotros ahora mismo con el sistema que tenemos trabajando pues lo vas a poder ver mucho antes de salir por la tienda. Evidentemente al salir por la tienda ya lo vas a tener pero mientras que estás cogiendo productos también. Lo primero que hice fue empaparme de toda la documentación que puede existir sobre el tema, ya sea de universidades que han intentado clasificar productos, seguir a personas por una tienda, un poco de todo. Luego, encontrar empresas o personas que hayan hecho proyectos similares, intentar leer las patentes, saber qué información se puede sacar del tema. Una vez que tienes la idea sobre todo de lo que no se puede hacer, hay veces que es más útil de lo que se puede hacer. Entonces, si tú tienes conocimientos, pues yo afortunadamente, mi background es estudiante de ingeniería, de electrónica y telecomunicaciones, entonces yo la parte de sensores, yo conocía hasta dónde podía llegar, cómo se puede usar, a qué velocidad se puede usar X sensores, cómo hay que acondicionarlos para X tarea. Entonces, cualquier tipo de información que creáis, que sea, por ejemplo, Estudiar un poquito de computación cuántica, a lo mejor lo haces como hobby, pero al final lo vas a acabar usando. A lo mejor, seguramente, en algún proyecto en el que tú estés, si tienes la... al final es mente abierta el no rechazar ningún tipo de dominio que hayas tocado, porque seguramente podrá entrar. Entonces, cuanta más información empapes, cuanto más reciente mantengas ese conocimiento, te actualices al día, si puede ser, pues evidentemente cada día es imposible porque la cantidad de papers... que salen esas, hay veces que yo también estoy saturado, pero sí que dedicarle una mañanita, un sábado, por ejemplo, un domingo por la mañana, pues a echar un ojo qué es lo que se ha visto, hay páginas web como Papers with Code, ArcShift, que te hacen el trabajo por ti, te dan la recopilación de cuál es el top, así lo que más se está hablando de Deep Learning, y es que sí o sí te va a servir. Por ejemplo, había un... Hace poco, creo que fue en Microsoft, sacó una red normal, hace un poco, creo que un par de meses, que lo que te hacía era, te resolvía sopas de letras, o creo que era crosswords. Yo me lo leí, pero no porque me interesara la resolución de sopas de letras y tal, sino cómo descartaba. de las posibles combinaciones las que no tenían las letras exactamente donde tocaban. Eso nos parece muy útil, por ejemplo, para decir, tienes esta lista a la compra, aquí hay cosas, ¿cómo descarto las cosas que no pueden ser porque son físicamente imposibles, o las que no se asemejan en peso, o las que no se asemejan en imagen? Pues toda esa información la vas a sacar incluso debajo de las piedras al final. O sea, no descartas absolutamente nada.