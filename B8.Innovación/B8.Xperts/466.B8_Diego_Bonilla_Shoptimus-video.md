---
title: Xperts | Diego Bonilla | Shoptimus
URL: https://app.web3mba.io/courses/take/bloque-8-innovacion/lessons/40238077-xperts-diego-bonilla-shoptimus
Tags/Keywords: #Bloque 8 #Innovacion #xperts #Diego Bonilla #Shoptimus
lang: es-AR
---
# Shoptimus
![[466.B8_Diego_Bonilla_Shoptimus.mp4]]
[Shoptimus](https://app.web3mba.io?wvideo=6blvckjnug)

En el momento en el que ya no estás trabajando como lo hacemos nosotros ahora, que es una especie de laboratorio controlado, por así decirlo, y te vas a los entornos reales, mantener un clasificador de objetos entrenado con los nuevos productos que van saliendo y posibles modificaciones que pueden tener, al final requiere una base de datos. Ya sea aumentada de manera artificial o aumentada de manera real, necesitas, digamos, un proceso mucho más costoso que el no tenerlo. Además, en un supermercado normal, la cantidad de productos que puede tener es enorme y, al final, un detector de objetos, en este momento el estado del arte, el mejor de los mejores, puede tener un error del 15%, 10% o 20%, y no es algo con lo que podamos jugar.

Entonces, el enfoque que hemos tomado es un poco diferente. Por ejemplo, modelos como CLIP, que es, digamos, el cerebro de DALL·E, toda esta generación de imágenes a partir de texto, siguen un enfoque que se llama Zero Shot, que se basa en que una vez que entrenas una red con unos datos, la entrenas de tal manera que luego es capaz de clasificar, por así decirlo, datos que nunca ha visto. Así, nosotros tenemos, digamos, que el cerebro de la tienda a nivel de visión es una red neuronal que sabe qué es un producto de un supermercado, lo haya visto o no lo haya visto con anterioridad.

Lo que hace la visión artificial no es leer el producto, ya que, evidentemente, no da la resolución de la cámara. O, si pudiera darla, se necesitaría un material mucho más costoso. Además, no sabemos dónde va a poner la mano el cliente. Por lo tanto, no podemos arriesgarnos con algo que seguramente suceda, que es que cuando tú coges un producto, lo tapas. Entonces, ¿en qué se basa la red neuronal? No lo sabemos, pero hemos entrenado una red neuronal que lo que hace es eso: le damos una foto de lo que debería haber en ese estante y lo comparamos con la información que tú nos has proporcionado. De esta forma, sabemos si los dos productos forman parte de la misma clase o si difieren de clase.

Por lo tanto, jugando no solo con cuán similares son dos productos, sino también con cómo se diferencian, podemos saber con muy buena precisión de dónde lo has cogido y qué es lo que has cogido. Es simplemente quitar los píxeles que no forman parte del producto. Eso nos da más flexibilidad que un clasificador de objetos, por ejemplo, por cajas, como YOLO o SSD, porque eliminamos automáticamente el ruido a la vez que sabemos los píxeles que forman parte del producto. Al final, lo que queremos es eliminar cualquier tipo de cosa que pueda influir en la toma de decisiones. Imagínate que la diferencia entre un producto y otro sea una franja de color rojo y que justo la persona que lo coja lleve guantes de color rojo. Queremos eliminar todo ese tipo de problemas. Y la máscara binaria nos está dando una muy buena solución. La segmentación, al final, nos ofrece una solución muy efectiva.

Ahora estamos trabajando con un sistema que se llama adaptación de dominios. Esto nos permite tener un conjunto de datos bastante humilde, porque lo hemos recogido en un entorno muy controlado y somos pocos, pero nos permite crear datos artificiales. En la red neuronal se tiene en cuenta que, en el proceso de entrenamiento, no se separen los dos tipos de datos como haría de forma natural, sino que, a través de una penalización en el gradiente, hacemos que los dos dominios se unan. Con esto, perdemos un poco de precisión final, pero como tenemos más datos, lo recuperamos y conseguimos, al final, un modelo más robusto y con una precisión similar.

Es un mapa de segmentación. Tenemos la imagen que detectamos nosotros con nuestros algoritmos y la segmentamos de manera manual. Hemos intentado preentrenarlo de manera semisupervisada y luego pasar a un enfoque supervisado. Ahora mismo, creo que la que estamos usando a día de hoy es completamente supervisada, pero cuando recojamos más datos, que es lo que estamos esperando para volver a entrenar todas las redes, probaremos diferentes arquitecturas mucho más potentes. Además, estamos trabajando con sensores de peso. Por ejemplo, en cada producto o cada ciertos productos, tenemos la cuantificación del peso, lo que también nos proporciona información extra sobre lo que se ha escogido en caso de que no haya una buena imagen o, al revés, utilizamos la imagen en caso de que haya mucho ruido, por ejemplo, en las acciones.

Los problemas que tenemos que resolver son tres: quién lo ha cogido, cuándo lo ha cogido y qué ha cogido. En este momento, el que nos está dando más problemas y es un poco más complicado es saber quién lo ha cogido. El saber quién eres tú y seguirte por la tienda, la tecnología que hay es bastante limitada en comparación con el resto de redes neuronales. El proceso de investigación se está enfocando sobre todo en esa rama, que es la que hemos dejado más atrás. El saber cuándo lo has cogido nos lo dice la báscula. Lo bueno es que, en cuanto detecta un cambio en el peso, automáticamente las cámaras enfocan justo a ese punto. Esto es muy útil porque así no tenemos que detectar esqueletos, no tenemos que saber si lo que has metido es una mano, o si ha sido un temblor en la tierra, o alguien que ha empujado una estantería.

En cuanto a quién ha cogido el producto, como te estaba comentando, es una mezcla entre esos sensores de peso, que son muy buenos, pero, efectivamente, hoy en día, la mayoría de productos, por ejemplo, macarrones o espaguetis, pesan medio kilo. Pesan medio kilo o casi cero, es decir, se va muy poco hoy en día. Por lo tanto, el sensor de peso no nos da la suficiente información y ahí es donde entra la imagen. También somos capaces de detectar acciones sospechosas y ser robustos. Por ejemplo, cuando dejas espaguetis, puedes dejarlos en una nevera, en la sección de galletas, donde tú quieras, y nosotros sabemos cuando otra persona lo coge. Podemos avisar a un revisor de que has dejado un producto fuera de su sitio para que vaya exactamente donde está ese producto y lo vuelva a colocar en su sitio. También podemos avisar cuando un producto se ha acabado, porque, sobre todo, también realizamos una acción sospechosa, como nosotros la llamamos. A partir de ahí, tenemos cámaras. Por lo tanto, cualquier clasificador de acciones que puedan parecer sospechosas o que intente de alguna manera falsificar los sensores, somos completamente conscientes de ello y se pueden entrenar sistemas desde cero. Será muy complicado que ocurra una falla de seguridad de ese tipo.

¡Gracias por ver el vídeo! Al final, el reconocimiento facial está muy limitado, porque tampoco podemos poner una cámara, digamos, delante de la cara de las personas. Nos parecía muy invasivo al principio. Luego, también es muy complicado relacionar la cara que estoy viendo, digamos, en un plano a nivel de los ojos, con una cámara que está en el techo, por ejemplo. Así que, ahora, la identificación que estamos haciendo es una mezcla entre seguimiento y reidentificación corporal completa. Sabemos cómo vamos vestidos, la cantidad de gente que hay en la tienda, aumentamos sus datos para, en caso de que se cambien de ropa o lo que sea, poder identificarte. De tal manera que cualquier foto en la que te agaches, en la que te quites un gorro, por ejemplo, o la chaqueta, sigues estando en ese espacio y, como te hacemos seguimiento, podemos detectar cualquier tipo de modificación en las prendas que te describen.

Entonces, podemos actualizar automáticamente. ¿Por qué estamos atrás? Porque ahora mismo aquí somos las personas que vienen a ver la demostración, y tal, son dos personas, una persona, como mucho, hemos estado cuatro. No hemos tenido en ese momento ese problema. Pero evidentemente, ahora que ya nos estamos situando en el futuro de tener una tienda en la que pueden haber 50, 60 personas incluso, es lo que le estamos metiendo más esfuerzo. Otra parte interesante es cuando dos personas entran juntas con una cuenta, por ejemplo, una madre con su hijo, un padre con su hija, o lo que sea. Saber que esas dos personas son la misma entidad, por así decirlo, en cuanto a la lista de la compra, todo eso afortunadamente ya está resuelto o tenemos una aproximación muy similar a lo que será al final. En cambio, por ejemplo, el saber quién eres, aún estamos descubriendo. El otro día, por ejemplo, hicimos un hito al superar casi el doble de precisión de lo que teníamos ahora y seguramente la semana que viene será aún más. Así que aún estamos descubriendo cosas nuevas en ese ámbito.

Ahora mismo, funcionando, digamos que casi en cada fotograma de la demostración puede haber perfectamente unas diez redes neuronales. También hay un par o tres de algoritmos de Machine Learning un poco más clásicos, incluso hay alguno que está basado más en, básicamente, unas operaciones realizadas con estadística bayesiana. Comparado con Deep Learning, evidentemente es más simple, pero no por ello menos complejo. En su día, había un sistema enteramente dependiente de la cámara, donde había más redes neuronales incluso, pero también con sus limitaciones. Las redes que estamos usando ahora están específicamente diseñadas para poder ejecutarse en sistemas muy pequeños, como por ejemplo, lo que pueda ser un Google Coral, un Nvidia Jetson, o alguna de estas single board computers que te van a permitir tener un cómputo cada ciertos metros de la tienda. Un sistema centralizado, todo eso aún queda por decidir. Pero digamos que tenemos unas redes que están hechas para ser optimizadas, para que funcionen en tiempo real y tal. Las redes antiguas no. Las redes antiguas eran tomadas de papers, implementadas, entrenadas, directamente cogidos los pesos de las implementaciones oficiales y simplemente cambiadas lo mínimo o lo justo para poderse utilizar en nuestro proyecto.

No solo es que tengamos un sistema más inteligente, sino que además es más pequeño y se puede comprimir aún más para poder integrarse dentro de sistemas embebidos. Yo creo que el reto más grande es depender de ideas felices. Al final, tenías que ir a trabajar y tenías que tener la solución a este problema que seguramente nadie ha solucionado nunca. Por lo tanto, es coger ecuaciones que se utilizan en mecánica cuántica, por ejemplo, que es la de que nosotros no tenemos un sistema determinista, sino que tenemos unas probabilidades de existencia de algo. Entonces, digamos que al final la inspiración te venía de las cosas más rebuscadas que supieras en tu consciente, que a lo mejor hacía años que no veías, y de repente te daba la bombilla. Pero el depender de ideas felices, el tener que solucionar problemas que no se han solucionado, utilizar redes neuronales o un sistema, por ejemplo, los sensores de peso, de una forma que no están hechos para funcionar, que es, por ejemplo, a muchas lecturas por segundo, los sensores de peso que hemos encontrado, usarlos de esa manera nos ha costado mucho. Todo eso, al final, son cosas que muy poca gente ha resuelto, que no hay nada de literatura al respecto, o muy poca literatura al respecto, y al final era depender de ver quién había dormido menos ese fin de semana y el que tenía ya la idea feliz de decir: "esto se va a hacer así". Ha habido una suma de esas ideas felices que ha llegado a donde estamos ahora.

El mayor problema que tenemos es a nivel del hardware de las básculas. El sensor de peso, digamos, que da lecturas diferentes a diferentes temperaturas; la humedad no la lleva muy bien. Entonces, lo hemos resuelto y, al final, la temperatura va a ser un parámetro más en la aproximación del peso. En cuanto a la imagen, no presenta ningún problema. El único problema que puede llegar a tener es la geometría diferente de las distintas formas de mostrar el producto, pero también, como veréis, luego tenemos maquillaje o productos colgados. Entonces, el tema de la diferente geometría lo hemos resuelto también. En cuanto al hielo, la descongelación es muy lenta, afortunadamente, así que no modifica lo suficiente como para que de repente se produzca un salto y salten todas las cámaras, porque no es tan rápida. Luego, en cuanto a la diferencia de peso, si no hay una buena diferencia de peso, que hemos medido que no cambia tanto, al final lo que da la otra parte es la imagen.

Además, tenemos la idea de montar un cerebro por detrás, utilizando ya sea aprendizaje por refuerzo, o algunos sujetos de manera supervisada, aunque aún no lo tenemos muy claro, en el que se adapte el sistema a quién eres tú y cuáles son tus acciones. De tal forma que, aparte de que una persona media en un comercio se comporta de una manera, también vamos a poder saber cómo te comportas tú cuando vas a la compra. Vamos a poder predecir qué acciones vas a realizar en la compra y vamos a poder predecir todo eso de tal forma que, al final, la pequeña parte que el sistema no pueda alcanzar la vamos a superar conociéndote, quién eres tú y cómo se comporta un cliente.

Nosotros ahora, con el sistema que tenemos, es lo suficientemente rápido. Si bien es cierto que tendría que ser más rápido, estamos teniendo problemas en cuanto a colapso del puerto serial y cosas en las que, desgraciadamente, ninguno es experto. Por lo tanto, tiene que entrar gente capacitada en esa parte. Pero ahora la rapidez que tenemos nos permite, y la robustez que tenemos nos permite enseñarte, ya sea en el futuro en la aplicación, o ahora en una pantalla de televisión que tenemos encima de la demostración, tu lista en tiempo real. Así, en el momento en el que cojas algo, si quieres comprobar que todo va bien, lo puedas verificar. Si ves que no está en su sitio o que algo va mal, puedes ir a una de las personas que esté trabajando en la tienda e informar de un error, o desde la aplicación puedes informar del propio fallo. Al final, eso nos da mucha más libertad que, por ejemplo, competidores como Amazon Go, donde tienes que esperar unos 15 minutos, 5, 10 o 20, a que te salga la lista de la compra. Nosotros, ahora mismo, con el sistema que tenemos trabajando, podrás ver tu lista mucho antes de salir de la tienda. Evidentemente, al salir de la tienda ya lo tendrás, pero mientras estás cogiendo productos también.

Lo primero que hice fue empaparme de toda la documentación que puede existir sobre el tema, ya sea de universidades que han intentado clasificar productos, seguir a personas por una tienda, un poco de todo. Luego, encontrar empresas o personas que hayan hecho proyectos similares, intentar leer las patentes, saber qué información se puede obtener sobre el tema. Una vez que tienes la idea sobre todo de lo que no se puede hacer, a veces es más útil que saber lo que se puede hacer. Entonces, si tú tienes conocimientos, afortunadamente, mi formación es en ingeniería, electrónica y telecomunicaciones. Así que yo conocía la parte de sensores, hasta dónde podía llegar, cómo se pueden usar, a qué velocidad se pueden usar ciertos sensores, cómo hay que acondicionarlos para ciertas tareas. Cualquier tipo de información que creas que sea útil, por ejemplo, estudiar un poco de computación cuántica, a lo mejor lo haces como hobby, pero al final lo acabarás usando. Seguramente, en algún proyecto en el que estés, si tienes la mente abierta y no rechazas ningún tipo de dominio que hayas tocado, porque seguramente podrá entrar.

Cuanta más información absorbas, cuanto más reciente mantengas ese conocimiento y te actualices, si puede ser, evidentemente, cada día es imposible, porque la cantidad de papers que salen es abrumadora. A veces, yo también estoy saturado, pero sí que dedicarle una mañana, un sábado, por ejemplo, o un domingo por la mañana, a echar un ojo a lo que se ha publicado, hay páginas web como Papers with Code, ArXiv, que te hacen el trabajo por ti, te dan la recopilación de cuál es el top, lo que más se está hablando de Deep Learning, y eso te va a servir. Por ejemplo, hace poco, creo que fue Microsoft, sacó una red neuronal que resolvía sopas de letras, o creo que era crucigramas. Yo lo leí, no porque me interesara la resolución de sopas de letras, sino por cómo descartaba de las posibles combinaciones las que no tenían las letras exactamente donde tocaban. Eso nos parece muy útil, por ejemplo, para decir: "tienes esta lista de la compra, aquí hay cosas, ¿cómo descarto las cosas que no pueden ser porque son físicamente imposibles, o las que no se asemejan en peso, o las que no se asemejan en imagen?" Toda esa información la vas a encontrar incluso debajo de las piedras, al final. O sea, no descartes absolutamente nada.