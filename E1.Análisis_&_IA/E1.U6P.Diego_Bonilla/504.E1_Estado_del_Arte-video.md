---
title: Estado del Arte | Diego Bonilla
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #practica #Estado del Arte #Diego Bonilla
lang: es-AR
---
# Estado del Arte
![[504.E1_Estado_del_Arte.mp4]]
[Estado del Arte](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla)
[Estado del Arte (PDF)](505.E1_DeepLearningStateoftheArt-230120-153018.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1HBSP9yyfvaOqG3FebpSP7VrSFEuR7kUv?usp=sharing)

Bienvenidos a la sesión de "State of the Art". En esta sesión, veremos de manera general cuáles son las técnicas de "State of the Art" de las redes neuronales en la actualidad. Algunas de ellas tienen unas pocas semanas de antigüedad al momento en que se está grabando esta sesión, que es en diciembre de 2022, y otras son muy conocidas. Si ustedes han realizado algún tipo de tratamiento de imágenes con redes neuronales, seguramente les sonarán. Partiremos de las más conocidas hacia algunas más novedosas que quizás no sean tan familiares. Iremos viendo esto en orden de precisión, comenzando con las más antiguas y las que menos precisión tienen hoy en día, y luego iremos subiendo en esa escala.

Es importante notar que muchas de estas técnicas tienen más precisión que la anterior, aunque a veces solo sea por unas pocas décimas. No por ello son una revolución, pero sí es cierto que la arquitectura cambia lo suficiente como para considerarlas redes completamente diferentes o arquitecturas muy distintas a las anteriores. Por último, saltaremos a Google Colab, donde tengo preparadas algunas prácticas para poder ejecutar estos modelos de forma sencilla utilizando la librería de PyTorch. La verdad es que facilita bastante el uso de estos modelos, incluso los preentrenados. Simplemente cargaremos los modelos y ejecutaremos unas cuantas imágenes de internet para ver su precisión, comprobar cuál puede funcionar mejor, cuál es más robusto, y que, si luego deciden usar alguno de estos modelos internamente, tengan más claro cómo utilizarlos y que esta sesión les sirva también de referencia.

Lo primero que vamos a necesitar para comparar cualquier tipo de modelo es una base. En este caso, la base es un dataset llamado ImageNet, que quizás les suene, ya que es muy popular. Es la base más conocida hoy en día y contiene más de 14 millones de imágenes que han sido anotadas a mano. Esto significa que ha habido personas anotando durante muchas horas esas imágenes. Algunas de ellas contienen la clase a la que pertenece la imagen y otras también incluyen la posición que ocupa esa clase en la imagen. Es decir, no solo se indica que hay, por ejemplo, una persona, sino que también se especifican las coordenadas de la imagen que ocupa esa persona.

En cuanto a categorías, hay más de 20,000. Estas categorías son muy diversas, incluyendo vehículos, animales, razas dentro de esos animales, comida, objetos cotidianos, entre otros. Al final, son 20,000 categorías. Lo más común es que los modelos que un equipo de científicos desarrolle, o que ya hayan desarrollado, para medir la precisión o cómo se comparan con el resto, utilicen mil imágenes, es decir, las mil imágenes de mil categorías de este dataset en lugar de las 20,000, y midan la precisión de su modelo en esas mil imágenes. Hay un benchmark, una tabla donde cada modelo tiene su precisión, lo que facilita la comparación entre ellos.

Aquí podemos ver una de estas tablas, donde se indica la precisión en la tabla de la derecha. En este caso, como podemos observar, se hace con mil clases. Esta es la más reciente que he encontrado; no están todos los modelos que vamos a ver aquí, desgraciadamente, pero sí están la mayoría y hay otros que no están. De izquierda a derecha, normalmente se puede ver el nombre del modelo que se está tratando. Cada tipo de modelo tiene diferentes variaciones, de más pequeño a más grande, en este caso, y el número de parámetros en millones. Luego, los FLOPS son una unidad de computación que necesitas para ejecutar esa red neuronal, expresada en gigaflops. Finalmente, se muestra el número de imágenes por segundo que puede procesar y, por último, el "top 1", que significa que efectivamente la clase que sale con más precisión de tu modelo es la clase en la que está etiquetada esa imagen. Por lo tanto, eso sería un acierto, y el número de aciertos, el porcentaje de aciertos, se refleja en la tabla de la izquierda.

Toda esta tabla se divide en dos tipos de arquitecturas principalmente: las convolucionales y los vision transformers. Es cierto que todas estas se aplican bastante a visión, pero la arquitectura transformers también se aplica en la actualidad en temas de reconocimiento de texto. Las convolucionales también se aplican a temas de audio u otros tipos de datos. Tratarlo con imágenes es una forma de universalizarlo todo, pero no está limitado solo a imágenes; la mayoría de estas arquitecturas son muy versátiles.

En la tabla, podemos ver cómo a lo largo de los años ha ido aumentando la precisión en este dataset. Efectivamente, como se puede entender, cuanto más avanza la investigación en estos campos y cuanto más avanza la capacidad de cómputo de los ordenadores, que son más accesibles que nunca, también va aumentando la precisión de los modelos. Esto es directamente proporcional. Hoy en día, el último modelo que ha salido, al momento de grabar esto, es el FocalNet, que veremos al final, con una precisión que ronda el 83.5% a 84%, lo que lo convierte en el "State of the Art" en la actualidad.

Para empezar, como hemos visto en el índice, una red neuronal que en su día fue una revolución se llama ResNet. La ResNet tiene ese nombre porque "Net" se refiere a "network" y "Res" hace referencia a las conexiones especiales que incorpora, llamadas conexiones residuales. Las conexiones residuales permiten que, en una red neuronal normal, que como ya saben está formada por capas conectadas una detrás de otra, se salten algunas conexiones. Esto facilita que la información fluya más fácilmente a través de las capas y mitiga muchos problemas que tiene una red neuronal, como por ejemplo, hacer que sea más fácil de entrenar y que se pueda aumentar mucho de tamaño sin que haya problemas en el entrenamiento. Esto la hace mucho más robusta en general. Efectivamente, una vez que se introdujo, fue una revolución, ya que aumentó considerablemente la precisión en el reconocimiento de imágenes.

Aquí se refleja en la imagen de ResNet 50, que también veremos luego en la práctica. Cada dos capas hay una conexión residual. Es importante aclarar que las conexiones residuales, una vez que se conectan, pueden sumar las activaciones o concatenarlas. Lo más común es que se sumen las activaciones. La ResNet en general ocuparía esta parte de la gráfica que hemos visto, siendo de las primeras, ya que se sigue utilizando bastante. Es una de las más antiguas en esta tabla y tiene una precisión alrededor del 80% en ImageNet.

Ahora pasaremos a un salto tanto en arquitectura como en historia hacia los Vision Transformers, que fueron creados después de observar la precisión de los transformers, que inicialmente fueron diseñados para el procesamiento de texto. Al ver la precisión que tienen con el texto, se intentó aplicar esta técnica a imágenes y se obtuvieron muy buenos resultados. Así se creó un Vision Transformer, que es la versión de imagen del transformer. A partir de aquí, el modelo es exactamente igual que los transformers aplicados a texto.

Esto nos lleva a la idea de que, tal y como funciona nuestro cerebro, la información se trata de forma muy similar, aunque provenga de diferentes dominios. Si tu cerebro tiene suficiente capacidad de cómputo, se puede adaptar a cualquier tipo de datos que reciba, siempre que sean representativos de algún tipo de información. En el caso del procesamiento de texto, es más directo separar, por ejemplo, una frase en palabras, aunque en realidad se separan en entidades más básicas llamadas tokens. Sin embargo, en imágenes no había una forma clara de hacerlo hasta que se les ocurrió, en el paper de Vision Transformer, separarlas en parches de 16 por 16 píxeles. De esta forma, se trata la imagen como diferentes palabras que forman una frase completa, que sería la imagen entera, si se quiere entender de esa manera. Esto reduce considerablemente la carga computacional en comparación con pasarle un píxel como un token de la imagen, lo cual se ha intentado y resultó ser inviable, sobre todo porque el transformer, y ahora vamos a explicar un poco lo que significa, es una arquitectura que intenta prestar atención a cada parche extraído, observando cómo se relaciona con los demás parches.

Esto crea una atención global donde cada parte de la imagen presta atención a la parte que le interesa, generando un mapa muy global de las características. Esto es mucho más potente, por así decirlo, que las convolucionales, que en principio procesan por partes de la imagen. El obtener ese mapa global no se llega a hacer o se realiza en etapas muy finales de la red neuronal. Comparando los Vision Transformers con las convolucionales, las convolucionales son mucho más fáciles de optimizar, pero una vez que optimizas los Vision Transformers, que requieren mucho más tiempo de entrenamiento y más datos, sí es cierto que han demostrado tener mucha más precisión y robustez que las convolucionales.

También es importante mencionar que tanto los Vision Transformers como las redes convolucionales tienen una capacidad llamada "inductive bias". Esto significa que cada red, tal y como está formada, tiene una capacidad intrínseca. Se ha demostrado que, por ejemplo, los Vision Transformers prestan más atención a la forma de los objetos, mientras que las convolucionales prestan más atención a la textura de esos objetos. Esto es algo a tener en cuenta. Además, es cierto que las convolucionales tienen más variaciones que los Vision Transformers, por lo que, en general, los Vision Transformers son más robustos a las distorsiones de imágenes y a las permutaciones que pueden tener los parches de las imágenes. Sin embargo, a pesar de la universalización de los beneficios de una arquitectura sobre la otra, al final depende de cada tarea aplicar una arquitectura u otra.

Ahora vamos a ver los Swin Transformers. Una vez que hemos visto los Vision Transformers, son muy fáciles de entender. Lo que hacen es que, en la parte que hemos visto de los Vision Transformers, se separa la imagen en parches de 16 por 16, y en ningún momento la atención se computa entre los parches de la imagen de forma global. En el Swin Transformer, por otro lado, esos parches se dividen en otros pequeños parches. Así, la atención primero se computa a nivel local dentro de estos parches y luego se acumulan para calcularla a nivel global. Esto hace que la arquitectura sea más densa y que la atención sea más fina, por lo que se atiende a parches mucho más pequeños que en la arquitectura de Vision Transformer que acabamos de ver.

No solo eso, sino que además tiene otra complejidad computacional diferente a la de Vision Transformer que la hace más eficiente. Gracias a esto, se utiliza tanto en clasificación de imágenes como en detección de objetos, que consiste en dibujar un cuadro alrededor de los diferentes objetos que forman una imagen. También se utiliza para segmentación, que es a nivel de píxel, donde se determina a qué clase pertenece cada píxel. Por ejemplo, en una imagen donde aparece una persona y un coche, se sabe qué píxeles forman parte de la persona y cuáles forman parte del coche. Esto se puede observar fácilmente si han hecho alguna videollamada últimamente, ya que lo que segmenta a la persona del fondo de pantalla utilizaría esta arquitectura. En la tarea de segmentación, se identifica qué píxeles forman parte de la persona, no solo de la piel, sino también del pelo o la ropa, y el resto se identifica como que no forma parte de la persona.

En la tabla que hemos visto anteriormente, podemos destacar la parte donde están estos transformers. Vemos que tenemos el Tiny, el Small y el Base. Como hemos mencionado, cada uno de estos tiene un incremento de casi el doble de parámetros que el anterior, por lo que su complejidad de ejecución aumenta considerablemente. La velocidad a la que se procesan las imágenes disminuye bastante y, por último, también es cierto que, cuanto más grande es la arquitectura, en un orden más normal y lógico, también aumenta la precisión general. Es verdad que, como hemos visto, parece poco, porque al final ResNet es una arquitectura más simple, entre comillas, pero tiene un par de puntos solo por debajo. Sin embargo, ese par de puntos significa mucho en estas tareas. Por lo tanto, aunque parezca poco a nivel de números, es un buen salto.

Luego vamos a ver las ResNeXt, cuyo nombre evidentemente se relaciona con las ResNet que hemos visto anteriormente. Esta es una nueva generación, en este caso, es una arquitectura un poco curiosa porque no cambia nada de ninguna capa en general de las convolucionales que hemos visto antes. Siguen siendo redes convolucionales; lo único que hace diferente es crear unos stacks de bloques que tienen la misma topología. Por lo tanto, la misma topología, como hiperparámetros de los filtros que se aplican, es compartida. Así, de una capa residual, que es de las que hemos visto anteriormente, donde simplemente pasamos la red normal, tenemos una conexión que se salta a todas las conexiones y se suma a lo que se procesa por otra parte. En este caso, se divide mucho más, incluso en 32 pads, como podemos ver, con la misma topología, y luego se suman cada uno de ellos. Además, tenemos otra conexión residual que se suma a todos. Esto hace que la precisión que puede llegar a tener sea más robusta.

En la tabla que hemos visto, que es diferente a las que hemos visto anteriormente, podemos observar que es una arquitectura bastante reciente al momento en que se está procesando esta clase. Podemos ver que tiene unos parámetros que no son muchos y unos FLOPS que tampoco son demasiados, pero sin embargo, tiene una precisión muy alta. Según esta tabla, no sería mejor que el Swin Transformer, pero en general se ha demostrado en otras que sí lo supera. Sin embargo, no consigo encontrar estas tablas; seguramente en el paper de ResNeXt estarán reflejadas. Además, se ha demostrado que ResNeXt es mucho más robusta que el Swin Transformer, según explican en su paper. Esto también supone un buen salto desde la ResNet anterior sin cambiar la arquitectura. La verdad es que facilita mucho a las personas que han trabajado más con convolucionales que con transformers, que tienen menos tiempo de antigüedad. Esto también nos facilita el salto, ya que no es necesario cambiar a transformers y, simplemente utilizando las capas que conocemos de toda la vida, podemos obtener resultados similares a los transformers.

Por último, y más como curiosidad que como algo que vayamos a explicar en profundidad, aunque tampoco es muy complicado, vamos a hablar de las Focal Nets. Digo como curiosidad porque, al momento en que se está grabando esta clase, que recuerdo que es diciembre de 2022, tiene unas pocas semanas de antigüedad. Es una red desarrollada por Microsoft que cambia el mecanismo de atención de los transformers por uno basado en convolucionales. Como pueden ver, hay una pequeña guerra entre transformers y convolucionales, donde los transformers superan fácilmente a las convolucionales, pero luego, si se aplican esas mismas técnicas a las convolucionales, vemos que vuelven a resurgir. Al final, no hay mucha diferencia entre ellas, y menos si se copia un poco la arquitectura, simplemente cambiando algunos detalles de cómo se perciben los datos de las imágenes.

En este caso, el self-attention, sin entrar mucho en detalle, crea matrices de key, query y value, y aplica una atención entre el query y el key. Utiliza la función de atención, que luego se multiplica por el value para crear el output. En este caso, lo que hacen es una arquitectura un poco similar; si pueden ver una similitud entre los gráficos, la agregación de contexto se hace mediante redes convolucionales que utilizan kernels cada vez más grandes. Aunque parezca complicado de entender, son unas pocas líneas de código para implementar esta convolucional. Luego, también se concatena esto con el query para obtener el output, como se hace de manera similar en el self-attention.

Para comprobar esto, evidentemente, la tabla la he sacado del propio paper, donde se puede ver que FocalNet está resaltado en negrita. Tiene diferentes arquitecturas, como hemos visto, para saber cuáles son más potentes. Simplemente, fíjense en el número de parámetros: 28 millones de parámetros es la más pequeña y llega hasta 88 millones de parámetros, casi 89 millones, que sería la más grande. Efectivamente, la más grande es la que más precisión tiene y, hoy en día, es el "State of the Art" en clasificación de imágenes y reconocimiento de imágenes. Como ya hemos comentado, el ser capaz de reconocer imágenes también indica que esta red se podrá adaptar a otro tipo de datos.

Además, resulta que la FocalNet, al igual que el Swin Transformer, tiene ese "State of the Art" en segmentación de píxeles, no solo en clasificación de una imagen entera. Así que, si quieren un resumen de lo que hemos visto, el FocalNet es la red más utilizada. Si bien es cierto que no hay ninguna implementación fácil, está en GitHub; la pueden buscar fácilmente buscando por el nombre de FocalNet. Está desarrollada por Microsoft y es de código abierto. Como ya comenté, el modelo es muy fácil de entender porque son unas pocas líneas, pero no la veremos en la práctica porque es tan reciente que no tiene una implementación fácil, como las que yo estaba buscando para esta práctica.

El "State of the Art" hoy en día, como otra conclusión, es del 83.9% en ImageNet 1K. Como ya hemos comentado, vamos a pasar ahora a Google Colab, donde veremos cómo cargar unas cuantas imágenes utilizando la librería Pillow para cargar las imágenes como un objeto de Python. Utilizaremos la librería de PyTorch, que tiene internamente una sublibrería llamada TorchVision, dedicada a temas de visión. Usaremos los modelos que han sido entrenados con ImageNet 1K, lo que significa que simplemente cargaremos los pesos de los modelos, que se descargarán automáticamente en una sola línea. No vamos a tener que programar nada y, automáticamente, le podemos dar una imagen. Si en esa imagen hay alguna de las mil clases de las que hemos comentado, nos mostrará la salida junto con la precisión, o mejor dicho, la confianza que tiene en esa clase.

Muchas gracias por el momento por la parte teórica. Ahora nos vemos en la práctica.

Hola de nuevo, estamos ahora en Google Colab, donde vamos a ejecutar lo que hemos comentado. Una vez más, vamos a seguirlo de arriba abajo, sin saltar ninguna celda, ya que están todas en orden, e intentaremos ejecutarlas todas juntas, así les iré explicando un poco lo que hace cada línea por si acaso no está bien comentado o no se entiende. Vamos a importar los modelos automáticamente. Google Colab, por defecto, viene con la librería TorchVision instalada. Por lo tanto, simplemente desde TorchVision vamos a importar los modelos y vamos a imprimir en pantalla qué modelos vienen incluidos con esta librería.

Vemos que no son pocos. Estos son modelos que vienen de código abierto; pueden acceder al código en la página oficial en el repositorio de PyTorch. Están ordenados alfabéticamente, así que no se fíen de este orden en cuanto a precisión histórica. Es cierto que AlexNet, en este caso, creo que es la más antigua que hay aquí, o DenseNet, pero aquí vemos las ResNet, que también hemos visto, y los Swin Transformers, Vision Transformers. Aquí están los Vision Transformers, pero hay muchísimos más que no he mencionado, porque evidentemente estaríamos un curso entero para verlas todas. Sin embargo, están aquí para probarlas y todas ellas están preentrenadas.

Vamos a limpiar este output. En este caso, he decidido simplemente tomar cuatro clases, un poco básicas, en este caso de un gato y un perro. Podemos ver, si quieren, el enlace a las imágenes: este gato de aquí y este perro de aquí. También he cogido, si no me equivoco, una mangosta; he tenido que buscar porque no me acordaba. Una mangosta es un animal que, la verdad, no conocía mucho, pero es una clase un poco más rara. Luego, un fagot, que es un objeto poco conocido. Estas son clases que efectivamente están dentro del 1K, las clases que hemos visto.

Pueden ver que cada una es muy diferente a la anterior. Son animales muy conocidos, un animal un poco menos conocido y un objeto poco conocido. Vamos a descargar estas imágenes. El hecho de descargarlas es para que no tengan que conectarse a ninguna base de datos ni a ningún drive ni nada de eso. Simplemente las descargamos automáticamente y nos las dejará en nuestro sistema de archivos de la ejecución de este Colab. Utilizando la función `wget`, podemos especificar el nombre que queramos y simplemente el enlace a la imagen. Con éxito, nos han descargado todas.

También nos vamos a descargar esto de aquí. Este es un documento que contiene las mil imágenes y el nombre de las mil clases que hemos visto. Podemos ver que tiene muchas clases muy diferentes. La verdad es que no sé lo que son todas, pero algunas son un poco más raras. Sin embargo, no todas son animales; también hay muchos objetos dentro de estas clases. Ya las pueden ver ustedes si quieren, pero no son muy variadas. Al final, son mil clases y hay de todo. Por ejemplo, aquí hay un cañón, que sería un objeto, o una rueda de coche. Todo esto es capaz de reconocerlo.

Descargamos todo esto porque, al final, el modelo lo que nos va a devolver es simplemente un índice. Por ejemplo, 8. Entonces, claro, si el índice nos devuelve un número, tendremos que venir a esta lista y buscar en plan 1, 2, 3, 4, 5, 6, 7 y 8 para ver que es un gallo. Así que, si cargamos todo el nombre de estas clases, es mucho más fácil.

Evidentemente, lo que vamos a hacer no sirve para evaluar estos modelos, porque lo único que hemos hecho es cargar cuatro clases y ya está. No es una buena evaluación de un modelo simplemente con cuatro clases, pero es una demostración de cómo utilizar estos modelos, cómo cargar imágenes en estos modelos y cargar los pesos preentrenados utilizando esta librería. Verán que es extremadamente fácil.

Me he descargado de aquí el archivo que hemos visto antes con las clases y lo que voy a hacer es abrirlo. Utilizo la función `open` que viene con Python y simplemente lo que voy a hacer es abrirlo y, a la vez que lo abro, también lo voy a separar por líneas utilizando la función `readlines`. Esto nos devuelve una lista con las diferentes líneas, así que simplemente voy a guardar en una lista que se llama `labels`. Por lo tanto, `labels` tendrá una lista con todas estas imágenes. De hecho, vamos a echarle un ojo. Vemos que todas las clases que hemos visto anteriormente están aquí, las mil clases. De hecho, si vemos el `length of labels`, esto nos va a devolver el tamaño que tiene. Vemos que son mil clases, como en otras prácticas que hemos hecho.

Vamos a importar la librería de Matplotlib para dibujar las imágenes que vayamos viendo. Entonces, me creo una función de `show_image` para mostrar las imágenes en la resolución que yo quiera. Vamos a utilizar la librería Pillow, que ya hemos importado anteriormente. No hace falta importarla de nuevo, porque viene con Colab. Vamos a subirla hacia arriba, donde ya la hemos importado. Dentro de la librería, utilizaremos la clase `Image`. En TorchVision, vamos a importar `transforms`, que veremos qué significa eso, y luego, por supuesto, PyTorch.

Lo que vamos a hacer en todas las imágenes es cargarlas a partir de las que hemos descargado anteriormente y convertirlas en formato RGB, por si acaso. Las guardaremos simplemente en variables y las mostraremos por pantalla, cosa que ya hemos hecho antes, pero simplemente para ver que se han cargado todas efectivamente.

Aquí es donde vamos a crear el flujo de preprocesamiento de las imágenes. Esto es porque las imágenes que les vamos a dar a los modelos tienen un tamaño fijo. En este caso, el tamaño universal que se utiliza es de 224 por 224. Luego, las convertiremos en tensores para poder dárselas a PyTorch y las normalizaremos. Esto son datos de normalización estándar de media y desviación estándar de la librería de ImageNet.

Ahora vamos a crear una función que sea para clasificar una imagen. Como vamos a clasificar muchas imágenes con muchos modelos, no queremos tener que escribir todo esto todo el rato. Así que crearemos una función que simplemente le llamaremos y ya está. En esta función, le daremos la imagen y el modelo. Lo que significa esta función es que sacará las labels de una imagen utilizando un modelo. El modelo lo pasamos a modo evaluación. Esto significa que, internamente, PyTorch lo que hará es decirle que no vamos a entrenar el modelo, simplemente lo vamos a evaluar. Entonces, se hacen algunas modificaciones en las capas necesarias y no se pasa ningún gradiente, o digamos que no se almacena ningún gradiente.

Preprocesaremos la imagen con el flujo que hemos visto anteriormente y, entonces, ya la tendremos en tensor. Con esto, la convertiremos en un batch de una imagen y la pasaremos al modelo. El modelo te devolverá un vector de mil dimensiones, que corresponde a las probabilidades de que esa imagen pertenezca a cada clase. Como queremos la clase con más probabilidades, cogeremos el índice de la clase con más probabilidades y lo convertiremos en porcentaje utilizando la función softmax, que tampoco vamos a detallar, pero que hace que todas las probabilidades sumen uno.

Por lo tanto, esto nos dará la clase con más probabilidades y la mostraremos por pantalla, junto con el nombre que tenga dentro de la lista que hemos sacado de nombres de clases, junto con el porcentaje que tiene o que ha sacado el modelo de seguridad de que esa imagen pertenece a esa clase. Simplemente vamos a crear esta función y la metemos en memoria. Vamos a probar primero con ResNet. Dentro de ResNet, ya hemos visto que hay diferentes versiones, como ResNet 50, ResNet 101, y nosotros vamos a probar de momento simplemente con la ResNet 101 como un test.

Es muy importante ver que le estamos dando el flag de `pretrained` como `True`. Eso significa que va a descargarse los pesos de ImageNet 1K y los va a meter dentro de la red neuronal. También se puede poner como `False` si, por ejemplo, queremos entrenar la red nosotros desde cero. Una vez entrenada, podemos quitar algunas capas y hacer las nuestras para realizar transfer learning. Todo esto es muy fácil con la librería de PyTorch. Ahora nos va a descargar los pesos del modelo y ya tenemos el modelo cargado en memoria.

Lo que vamos a hacer es utilizar la función que hemos creado antes para imprimir y pasarle las diferentes imágenes, el modelo que queremos usar, y que nos imprima lo que cree. La primera es un gato. Fíjense que no solo nos dice que es un gato, sino que además te dice la raza de gato que es, que en este caso es un gato tigre. Con el perro, lo mismo; te dice que es un golden retriever con cierta porcentaje de seguridad. Aquí, efectivamente, te dice que es una mangosta, creo que se llamaba, con un 99.87% de certeza. Luego, el fagot, que es un objeto en inglés, tiene una mala traducción, pero también nos lo ha sacado con muy buena probabilidad.

Ahora vamos a ver muy rápidamente el resto de modelos. Aquí ven que cambia un poco la forma de escribir. Aquí era `pretrained` igual a `True`, aquí es `weights` igual a `True`. También ven que aquí los pesos ocupan 171 megas, mientras que aquí ocupan 1.14 gigas. Por lo tanto, ya es una demostración de que este modelo va a ser mucho más grande que la ResNet. Vamos a ver si tiene más precisión, pero tampoco, como lo he dicho, aunque falle en algunas de estas imágenes, no es una medida de la robustez de estos modelos.

Aquí, por ejemplo, saca otra raza de perro. Con el gato es lo mismo. Los últimos dos modelos bajan un poco la seguridad que tienen de que son esa clase, pero también las categoriza correctamente. Si quieren, luego pueden buscar en internet la imagen del perro y la raza que pone aquí para ver cuál es la que creen que se parece más.

Por último, el Vision Transformer. Bueno, por último, perdón, luego van los Swin Transformers, que son mucho más pequeños. Aquí también entiendan que estamos utilizando el `vt32`, que es un modelo medio grande, y luego el `swin transformer tiny`, que es el más pequeño. Aquí, los weights ya tienen que ser especificados, que quiere decir que son de ImageNet 1K y versión 1. Por lo tanto, son 108 megas, que es más pequeño incluso que la ResNet. Por lo tanto, también vamos a ver que va a tardar muchísimo menos que la ResNet.

Tampoco es que aquí estemos midiendo tiempos, pero parece que tiene mucho que ver con el Vision Transformer. Ha sacado la misma clase con unas probabilidades bastante parecidas, siendo una red muy diferente. Pero bueno, eso estamos viendo cómo clasificar estas imágenes. Ahora sí, por último, vamos a cargar el ComNet, que estaría un poco entre ResNet y Vision Transformer en cuanto a tamaño del modelo. Vamos a clasificar estas imágenes. Parece que la mayoría están de acuerdo en que es esta raza de perro, pero no todas. En general, han clasificado, aunque sean diferentes razas de perro, que es un perro. Tienen mucha seguridad en eso.

Con el gato no lo ha dudado en ningún momento, pero también ha detectado que el objeto que existe en esa imagen es un gato. Luego, algunos que son menos comunes también ha sido capaz de entenderlo. Por último, el FocalNet, como hemos comentado, no está la librería aún metida en esta librería, pero sí que en el repositorio oficial de Microsoft está todo el código abierto. Pueden descargar los pesos preentrenados también con ImageNet y para diferentes otras tareas.

Esto sería un poco la práctica de esta sesión. Hemos visto cómo coger una imagen, reprocesarla para cargarla en un modelo preentrenado y obtener las salidas de ese modelo, convirtiéndolas a porcentajes de seguridad de la red y sacando la clase con el porcentaje más alto, todo en muy pocas líneas de código.