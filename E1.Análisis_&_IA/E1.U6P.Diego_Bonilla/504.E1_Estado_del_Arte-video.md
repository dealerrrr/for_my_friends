---
title: Estado del Arte | Diego Bonilla
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla
Tags/Keywords: #Estado del Arte #Diego Bonilla
lang: es-AR
---
# titulo
![[504.E1_Estado_del_Arte.mp4]]
[Estado del Arte](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla)
[Estado del Arte (PDF)](505.E1_DeepLearningStateoftheArt-230120-153018.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1HBSP9yyfvaOqG3FebpSP7VrSFEuR7kUv?usp=sharing)

Bienvenidos a la sesión de State of the Art. En esta sesión lo que veremos es un poco por encima cuáles son las técnicas de State of the Art de las redes neuronales de hoy en día. Algunas de ellas tienen unas pocas semanas de antigüedad a la hora que se está grabando esta sesión, que es diciembre de 2022, y otras son muy conocidas y si vosotros habéis hecho algún tipo de tratamiento de imágenes con redes neuronales pues seguro que sonarán. Entonces partiremos de esas a algunas más novedosas que a lo mejor no suenan tanto. Iremos un poco viéndolo por orden de precisión. Empezaremos con las más antiguas y las que menos precisión tienen hoy en día el primero y luego pues iremos subiendo en esa escala. Notar que muchas de ellas pues tienen más precisión que la anterior a lo mejor en unas pocas décimas, no por ello son una revolución por así decirlo, pero sí que es verdad que la arquitectura cambia lo suficiente como para considerarlas una red completamente diferente o una arquitectura muy diferente a la anterior. Y por último saltaremos al Google Colab donde tengo preparadas algunas prácticas para poder ejecutar estos modelos de forma fácil utilizando la librería de PyTorch. La verdad es que facilita bastante el uso de estos modelos, pre-entrenados incluso, entonces simplemente cargaremos los modelos y ejecutaremos unas cuantas imágenes de internet para ver su precisión, para un poco comprobar a ver cuál puede funcionar mejor, cuál es más robusto y todo eso y que luego pues si lo vais a usar, vais a usar alguno de estos modelos internamente, bueno por vuestra parte digamos que ya lo tengáis más claro de cómo usarlo y que ésta os sirva también de referencia. Lo primero que vamos a necesitar para comparar cualquier tipo de cosa, vamos a necesitar digamos una base, en este caso la base es una dataset que se llama ImageNet, a lo mejor os suena, es muy popular, es la base más popular hoy en día, contiene más de 14 millones de imágenes que han sido anotadas a mano, digamos que ha habido personas anotando durante muchas horas esas imágenes y algunas de ellas contienen la clase que contiene la imagen y otras pues también contiene digamos la posición que contiene esa clase, o sea ya no solo es que aquí hay por ejemplo una persona sino que coordenadas de la imagen ocupa esa persona. En cuanto a categorías hay más de 20.000, las categorías son muy muy diversas tanto vehículos, animales, razas dentro de esos animales, comida, objetos cotidianos, de todo, o sea son 20.000 categorías al final. Digamos que esta dataset lo más normal es que los modelos que digamos que si un equipo de científicos quieren sacar un nuevo modelo o han sacado un nuevo modelo y quieren medir la precisión o cómo se compara con el resto pues entonces lo que van a hacer es coger mil imágenes, o sea las mil imágenes de mil categorías de esta dataset en vez de las 20.000 y medir cómo la precisión que tiene su modelo en esas mil imágenes y digamos que hay un benchmark, una tabla donde cada modelo tiene su precisión entonces pues es más fácil compararlos entre ellos. Aquí podemos ver una de estas tablas donde aquí pone la precisión en la tabla de la derecha del todo. En este caso como podemos ver lo hace con mil clases. Esta es la más reciente que he encontrado, no están todos los modelos que vamos a ver aquí desgraciadamente pero bueno están hechos de la mayoría y hay otros que no están. Un poco de izquierda a derecha podemos ver que normalmente se puede ver el modelo del modelo que se está tratando, el nombre del modelo. Normalmente cada tipo de modelo tiene diferentes variaciones de más pequeño a más grande en este caso y el número de parámetros en millones de parámetros y luego los flops es una unidad de computador digamos que que necesitas de cómputo para ejecutar esa red neuronal en gigaflops en este caso y luego pues las imágenes por segundo que saldría y luego por último el top 1 que significa que efectivamente la clase que sale con más precisión de tu modelo es la clase en la que está etiquetada esa imagen entonces por lo tanto pues digamos que eso sería un acierto y el número de aciertos el porcentaje de aciertos sale reflejado en la tabla de la izquierda de la derecha un poco toda esta tabla se divide en dos tipos de arquitecturas principalmente que son convolucionales y vision transformers en el caso es verdad que todas estas están se aplican bastante a visión pero digamos que la arquitectura transformers pues también es ahora mismo lo que más se aplica en temas de texto reconocimiento de texto pero también y las convolucionales también se aplican a temas de audio o otro tipo de datos y simplemente pues digamos que el tratarlo con imágenes es una forma de universalizarlo todo pero no está limitado solo a imágenes son muy versátiles la mayoría de estas arquitecturas en la tabla de aquí podemos ver como como él a lo largo de los años ha ido aumentando esta esta precisión en esta dataset efectivamente pues como se puede entender cuanto más avanza la investigación de estos campos y cuanto más avanza la la la capacidad de cómputo de los ordenadores y son más universalmente digamos que compras un ordenador potente es más universalizado que nunca pues también va aumentando la precisión de los modelos digamos que es directamente proporcional de momento y hoy en día pues nos situamos pues ahora mismo el último modelo que ha salido a día de que se está grabando esto es el focal net que lo veremos el último de todos de una precisión que ronda el 83,5 33 84 casi vale que sería el street of the art hoy en día vamos a ver para empezar como hemos visto en el índice una red neuronal que en su día fue una revolución que se llama resnet la resnet tiene ese nombre ya que net efectivamente pues evidentemente desde network y res es el tipo de conexiones especiales que va a que mete esta esta red que se llaman residuales las conexiones residuales digamos que si en una red neuronal normal que como ya sabéis esta forma por capas conectadas una detrás de otra pues seguiría un flujo que seguiría estas estas flechas de color un poco más oscuro y luego la residual lo que hace saltarse conexiones esto hace que la información fluya más fácil a través de la de las capas y mitiga muchos problemas que tiene una red neuronal como por ejemplo convertirla en que sea más fácil de entrenar que la puedas aumentar mucho de tamaño sin que sin que haya problemas en el entrenamiento digamos que la hace mucho más robusta en general y efectivamente una vez que se introdujo pues fue una revolución ya que aumentó mucho la precisión de las del reconocimiento de imágenes está aquí que se refleja en la imagen es resnet 50 que también la veremos luego en el laboratorio bueno la práctica perdón en la que pues cada dos capas hay una una conexión residual aclarar que las conexiones residuales una vez que se conectan digamos que cuando incidiendo se puede tanto sumar las activaciones como concatenar las activaciones lo más normal es que se sumen las activaciones la resnet 50 en la tabla que hemos visto antes ocuparía perdón la resnet en general ocuparía esta esta parte de la de la grafía que hemos visto digamos que es de las primeras porque siguen usándose bastante si son las más antiguas que hay en esta en esta tabla y tiene una precisión alrededor del 80% en imagenet vamos a pasar ahora buen salto en cuanto a tanto a arquitectura como de histórico a los vision transformers que fueron creados después de ver la precisión fueron creados inicialmente los transformers fueron creados para texto para procesamiento de texto y vista la precisión que tienen con el texto lo intentaron pasar a imagen y vieron muy buenos resultados y efectivamente pues se creó una un vision transformer que es la versión de imagen del transformer pero a partir de aquí a partir de esta parte de aquí del modelo es exactamente igual que los transformers de que se aplican a texto entonces digamos que aquí es donde un poco se da la idea de que tal y como funciona nuestro cerebro pues la información se trata de forma muy similar aunque venga de diferentes dominios o digamos que si tu cerebro es suficiente o digamos la capacidad que tengas de cómputo es suficientemente grande digamos que tiene una fuerte inteligencia como quieran como queréis llamarlo sería capaz de adaptarse a cualquier tipo de datos que te entran mientras que sean representativos de algún tipo de información en este caso en digamos que en procesamiento de texto era un poco más directo separarlas por ejemplo una frase por palabras aunque en verdad se separan por unas entidades más más básicas que palabras llamadas tokens pero bueno se separa por palabras es muy fácil digamos el pasarlo a partes pero en imagen no digamos que no había ninguna forma de hacerlo hasta que se les ocurrió en el paper de vision transformer separarlo por parches de 16 por 16 píxeles de esta forma tratar digamos la imagen como diferentes palabras que forman una frase entera que sería la imagen entera si lo queréis entender por esa parte y eso reduce bastante el nivel a la computación de por ejemplo pasarle un píxel como un token de la imagen que se ha intentado y efectivamente era incomputable sobre todo porque el transformer y ahora vamos un poco a lo que significa el transformer es una arquitectura que lo que hace es por cada parche que se extrae digamos que intenta prestar atención al resto de parches intenta ver cómo se parece o qué relación tiene con qué parche entonces digamos que se crea una atención global donde cada parte de la imagen un poco presta atención a la parte que interesa de la imagen entonces esto crea un mapa muy global de las features y digamos que pues es mucho más potente por así decirlo que las convolucionales que en principio pues irían por partes de la imagen entonces el obtener ese mapa global o no se llega a hacer o se hacen etapas muy muy finales de la red neuronal digamos que también por poner un poco los vision transformers comparados con las convolucionales los convolucionales son mucho más fáciles de optimizar pero una vez que optimizas los vision transformers de los cuales pues aparte de que cuesta mucho entrenarlos pues también editas muchos datos pero una vez que cumplas eso sí que es verdad que los vision transformers han demostrado mucha más precisión y mucha más digamos de robustez que las convolucionales también mencionar que los vision transformers y tanto los vision transformers como los convolucionales con las redes convolucionales tiene una digamos una capacidad llamada inductive bias que lo que hace es que cada red tal y como está formada tiene digamos una capacidad intrínseca y está demostrado que pues la por ejemplo los vision transformers prestan más atención a la forma de los objetos y las convolucionales prestan más atención a la textura de esos objetos entonces digamos que también tenerlo en cuenta y sí que es verdad también que las convolucionales tienen más de más vallas de estos que las que los vision transformers entonces por lo tanto los vision transformers por defecto ya son más robustos a las distorsiones de imágenes y a las permutaciones que pueden tener estas los parches de las imágenes y digamos que en verdad tampoco por mucho que haya una universalización de estos digamos de los beneficios de una con respecto de otra al final depende de cada tarea aplicar arquitectura o la otra. Vamos a ver ahora los swing transformers, los swing transformers una vez visto los vision transformers son muy fáciles de entender lo que hacen es digamos que la parte que hemos visto de los vision transformers es separar la imagen en parches de 16 por 16 y digamos que en ningún momento siempre la atención se computa con los digamos entre los parches de la imagen de forma global en el swing transformer por otro lado de esos parches se vuelve a dividir en otros pequeños parches entonces digamos que la atención primero se computa a nivel local dentro de estos parches y luego estos parches se van acumulando y luego se va calculando a nivel global esto hace que la arquitectura sea más densa que la atención sea más fina entonces por tanto se atiende a parches mucho más pequeños que anteriormente con una arquitectura de vision transformer que acabamos de ver pero no sólo no sólo eso sino que además tiene otra complejidad computacional diferente la de vision transformer que la hace más eficiente. Gracias a esto se utiliza tanto en clasificación de imagen pero también se utiliza en detección de objetos que sería un poco dibujar una cajita alrededor de los objetos que diferentes objetos que forman una imagen y también se utiliza para segmentación que sería ya a nivel de píxel decir a qué clase forma cada píxel entonces pues digamos de una imagen en la que te aparece una persona y un coche saber qué píxeles forman parte de la persona y qué píxeles forman del coche y esto se ve muy fácilmente si habéis hecho alguna llamada vídeo llamada últimamente pues lo que te segmenta a la persona del fondo de pantalla utilizaría este la nuestra arquitectura necesariamente pero si la tarea de segmentación en la que sabe qué píxeles forman parte de la persona ya no sólo de la piel sino el pelo o la ropa y el resto digamos que lo puede sabe que no es no forma parte de la persona entonces luego lo puede cambiar a otra foto en dentro de la tabla que hemos visto anteriormente destacar el papel la parte que donde están estos transformers los transformers vemos que tenemos el tiny el small y el base vale y como hemos visto pues cada uno de estos tiene incremento el doble de parámetros que el anterior casi y entonces por tanto su complejidad de ejecución aumenta mucho la velocidad en la que en la que se procesan las imágenes disminuye bastante y por último también pues efectivamente cuando más grande es la arquitectura por orden un poco más normal y lógico pues también aumenta la precisión general entonces también es verdad que como hemos visto parece poco porque al final en resnet es una arquitectura más simple entre comillas pero tiene un par de puntos sólo por debajo pero ese par de puntos la verdad es que significa mucho en digamos en estas en estas tareas entonces por lo tanto pues aunque parezca poco a nivel de números es un buen salto luego vamos a ver las resnext que su nombre evidentemente se aplica a las resnet que hemos visto anteriormente pero digamos que una nueva generación en este caso es una digamos una arquitectura un poco curiosa porque no cambia nada de la ninguna capa en general de las de las convolucionarias que hemos visto antes son siguen siendo redes convolucionales lo único que hace diferente es un poco hacer unos stacks de bloques que tengan misma topología entonces por tanto digamos misma topología como como hiperparámetros de digamos que los filtros que se aplican y todo eso son compartidos entonces por tanto de una capa residual que de las que hemos visto anteriormente que pueden ser que pueden ser esto de aquí vale donde simplemente pasamos la rendering al normal pero luego tenemos una conexión que se salta a todas las conexiones y se suma a lo que se procesa por otra parte pero en este caso digamos que se divide mucho más incluso en 32 pads como podemos ver con misma topología y luego se suman cada uno de ellos y luego además tenemos otra conexión residual que se suma a todos esto pues digamos que la presión que puede llegar a tener lo hace más robusto y como podemos ver en esta tabla que es diferente a las que hemos visto anteriormente porque efectivamente no estaba en las que en las que hemos visto también es verdad es una arquitectura muy bastante reciente a la hora que se está procesando esta clase y podemos ver que tiene unos parámetros que no son muchos la verdad y unos flops que tampoco son demasiados pero sin embargo tiene una precisión muy alta en este caso según esta tabla no sería mejor que la swing transformer pero en general está demostrado ya en algunas otras que sí que sí que supera los swing transformers pero no consigo encontrar estas tablas seguramente en el paper de la resnext si queréis buscarlos estará reflejado la resnext además es mucho más robusta que el swing transformer en principio según explican en su paper y obviamente pues supone también un buen salto desde la resnext anterior sin cambiar la arquitectura y la verdad es que también facilita mucho a la gente que ya hemos trabajado con mucho más con convolucionales que con los transformers que tienen menos tiempo de antigüedad pues también nos facilita mucho el salto a decir que a lo mejor no hace falta cambiarse a transformers y simplemente utilizando las capas que conocemos de toda la vida pues podemos digamos el superar o hacer tener resultados similares a los transformers por último y más por una curiosidad que por otra que digamos como algo que vayamos a explicar muy en profundidad aunque tampoco es muy complicado sería explicar las focal nets digo como curiosidad porque a la hora que se está grabando esta clase que recuerdo que es diciembre del 2022 pues tiene unas pocas semanas de antigüedad es una red desarrollada por microsoft digamos que lo que hace es cambiar el mecanismo de atención de las de los transformers por uno basado en convolucionales digamos que ya estáis viendo que hay una pequeña guerra entre transformers y convolucionales donde las transformers pues fácilmente superan a las convolucionales pero luego a lo mejor si se aplican esas mismas técnicas a las convolucionales pues vemos que vuelven a resurgir las convolucionales al final no hay mucha diferencia entre ellas y menos y si un poco se copia en la arquitectura simplemente pues cambia algunos detalles de cómo se perciben los datos de las imágenes y ya está en este caso self attention sin entrar mucho en detalle lo que crean son matrices de key query y value y aplica una atención entre el query y el key utilizan la función de atención que luego se multiplica por el value para crear el output en este caso lo que hacen es digamos una arquitectura un poco similar si podéis ver un poco una similitud entre los gráficos pero digamos que la la agregación de contexto se hace mediante unas redes convolucionales que se hacen que cada vez van cogiendo un kernel más más grande y esto aunque parezca complicado de entender son unas pocas líneas de código para hacer esta convolucional no es para nada algo complicado y luego pues también se concatena esto con el query para sacar el output como se hace un poco similarmente en la en el self attention esto simplemente para para comprobar esto pues evidentemente la tabla está la he sacado del propio paper donde se puede ver que focal net está resaltado en negrita tiene diferentes arquitecturas como hemos visto para saber cuáles son más potentes simplemente fijaos en el número de parámetros 28 millones de parámetros es la más pequeña y llega hasta 88 millones de parámetros casi 89 millones de parámetros sería la más grande y efectivamente pues también la más grande es la que más precisión tiene que hoy en día es el state of the art en clasificación de imágenes reconocimiento de imágenes pero como ya hemos comentado el ser capaz de reconocer imágenes también explica un poco y digamos que también da índice de que esta red también se va a poder clasificar otro tipo de o digamos adaptarse a otro tipo de datos también resulta que la focal net igual que igual que el swing transformer tiene ese state of the art en segmentación de píxeles ya no sólo en clasificación de una imagen entera así que si queréis un poco resumen de lo que hemos visto ahora mismo el focal net es la red más usada si bien es cierto que no hay ninguna la implementación está en github la podéis buscar súper fácil buscando por el nombre de focal net está hecha por microsoft y es de código abierto y es pues como ya comentaba el modelo es muy fácil de entender porque son unas pocas líneas pero no la veremos en el laboratorio porque en la práctica porque porque es tan reciente que no tiene una implementación fácil digamos como las que yo estaba buscando en esta práctica y el state of the art hoy en día también como otra conclusión es del 83,9 en image net 1k como ya hemos comentado vamos a ir ahora a pasar al collab donde veremos cómo cargar unas cuantas imágenes utilizando la librería pillow para cargar las imágenes como un objeto de python y utilizaremos la librería de torch de pytorch una librería que tiene internamente que se llama torch vision que está dedicado para temas de pues como indicas un nombre de imágenes de visión y usaremos los modelos que han sido entrenados con image net 1k eso significa que vamos a simplemente cargando los modelos pesos nos lo descargará automáticamente y en una en una línea solo no vamos a tener que programar nada y automáticamente le podemos dar una imagen y si en esa imagen hay alguno de las mil clases de las que de las que hemos comentado pues o sea de las que está entrenado pues nos pondrá la salida junto con la precisión que tiene o la confianza mejor dicho que cree que tiene esa clase vale pues muchas gracias de momento por la teoría la teórica está y ahora nos vemos en la en la práctica hola otra vez estamos ahora en él en el collab donde vamos a ejecutar lo que hemos comentado una vez más vamos a seguirlo de arriba abajo vale no haber que saltar ninguna celda están todas por orden y lo vamos a intentar ejecutar todos juntos y así os voy explicando un poco lo que hace cada línea por si acaso no estaba bien comentado si no se entiende vamos a importar los modelos automáticamente el google collab por defecto viene con la librería torch vision instalada eso es por lo tanto simplemente del torch vision vamos a importar los modelos y vamos a imprimir por pantalla qué modelos vienen incluidos con con esta librería vale vemos que no son pocos estos son modelos que vienen de código abierto podéis acceder al código en la página oficial en el repositorio de de pytorch y digamos que parten están ordenados alfabéticamente en este caso al parecer entonces no nos fíéis tampoco de esta de este orden de mayor a menor precisión y en de forma histórica sí que es verdad que alex net en este caso creo que es la más antigua que hay aquí o dense net pero aquí vemos las con next que las hemos visto ahora hace poco algunas más conocidas como por ejemplo la resnet que también las hemos visto swing transformer vision transformer aquí están vision transformers pero bueno también hay muchísimas más que no he mencionado porque evidentemente pues estaríamos un curso entero para verlas todas pero pero están aquí para probar todas ellas y todas ellas están pre entrenadas vale vamos a limpiar este output en este caso he decidido simplemente coger cuatro clases una la dos un poco básicas en este caso de un gato y de un perro podemos ver si queréis el link a las imágenes este gato de aquí y este perro de aquí vale muy bien bastante de digamos modelos de sus clases pero también he cogido si no me equivoco esto era un esto es una mangosta he tenido que buscar porque no me acuerdaba una mangosta que es bueno yo no la verdad es que la conocía pero no me sonaba mucho pero bueno como una clase un poco más rara y luego un fagot en este caso y estas son clases efectivamente que están dentro del 1k este las clases que hemos visto entonces podéis ver que pues cada una es muy diferente el anterior entonces son animales muy conocidos un animal un poco menos conocido y un objeto la verdad es que poco conocido entonces nos vamos a descargar para poder bueno primero nos descargamos estas imágenes vale el hecho de descargarlas es para que no os tengáis que conectar a ningún a ninguna base de datos ni a ningún drive ni nada de eso simplemente la descargamos automáticamente nos lo dejará en nuestro file system de la ejecución de este colab vale utilizando la función web le podemos decir un poco el nombre que queramos y simplemente el link a la imagen con éxito nos lo han descargado todas y también nos vamos a descargar esto de aquí esto aquí es un documento que contiene las mil imágenes a la clase el nombre de las mil clases que hemos visto vale pues eso podemos ver que tiene muchas clases muy diferentes de ellas yo la verdad es que no sé lo que son pero bueno alguna vez así que son un poco más raras pero sí que no nos suenan pero nosotros digamos que tampoco es que sean todas una de animales también hay bueno ahora tampoco me voy a poner a buscarlas pero digamos que también hay muchos objetos dentro de las dentro de las de estas clases vale ya las podéis ver vosotros si queréis pero no son muy variadas al final son mil clases y claro hay de todo unidad por ejemplo aquí hay un canon un cañón que sería una un objeto o una rueda de coche todo esto es capaz de reconocerlo entonces simplemente porque las cargamos porque al final el modelo lo que nos va a devolver simplemente es un índice por ejemplo 8 entonces claro si el índice pues nos tenemos que venir a para poderlo entender nosotros nos tendremos que venir a esta lista de aquí y buscar en plan 1 2 3 4 5 6 7 y 8 y ver que es un gallo entonces digamos que que si cargamos todo el nombre de estas clases es mucho más fácil luego evidentemente pues el el mirar a ver si ha sido correcta la identificación o no esto tampoco sirve lo que vamos a hacer no sirve tampoco para evaluar estos modelos porque lo único que hemos hecho es cargar cuatro clases y ya está pues tampoco es una buena evaluación de un modelo simplemente con cuatro clases pero simplemente que sea como demostración de cómo utilizar estos modelos como cargar imágenes en estos modelos y cargar los pesos pre-entrenados utilizando esta librería y ver que es extremadamente fácil bueno me he descargado de aquí la la el el archivo que hemos visto antes con las clases y lo que voy a hacer es simplemente abrirlo utilizo la función de open que viene con él con python y simplemente lo que voy a hacer es abrirlo y a la vez que lo abro también lo voy a separar por líneas utilizando la función de readlines esto nos devuelve pues es una lista con las diferentes líneas entonces simplemente nos voy a guardar en una lista que se llama labels entonces por lo tanto el labels va a tener una lista con todas estas imágenes vale de hecho vamos a echarle un ojo vale y vemos que todas las clases que hemos visto anteriormente están aquí las mil clases de hecho mira podemos son muchas clases si vemos el length of labels esto nos va a devolver el tamaño que tiene vale vemos que son mil clases como en otras prácticas que hemos hecho vamos a importar vamos a utilizar la librería de matplotlib para dibujar las imágenes que vayamos viendo entonces me creo yo mi función de show image para mostrar las imágenes en la resolución que yo quiera y entonces vamos a utilizar la librería pillow que le hemos importado anteriormente bueno no hace falta importarla perdón porque viene con bueno está importada aquí de hecho vamos a subirla hacia arriba donde aquí ya vamos a importar la librería pillow en este caso vamos a utilizar dentro de librería la clase de image en torch vision vamos a importar transforms que nos vamos a ver qué significa eso y luego pues pytorch per se en image lo que vamos a hacer en todas va a ser cargarlas a partir de las que nos hemos descargado anteriormente y convertirlas en formato rgb por si acaso y la vamos a guardar simplemente en variables y las mostraremos por pantalla cosa que ya hemos hecho antes pero simplemente para ver que se han cargado todas efectivamente a los que hemos visto antes vamos a limpiar esto aquí es donde vamos a crear el flujo de pre procesamiento de las imágenes esto es porque las imágenes que les vamos a dar o sea que le tenemos que dar los modelos son diferentes o sea tienen un tamaño fijo en este caso el tamaño universal que se utiliza es de 224 por 224 luego las convertiremos en tensores para poderse las dar a pytorch y luego las normalizaremos esto es digamos que son datos de normalización estándar de media y de desviación estándar de la de la librería de imagenet entonces ahora nos vamos a crear una función que sea para clasificar una imagen para simplemente como vamos a clasificar muchas imágenes con muchos modelos pues que no tener que escribir todo esto todo el rato pues nos queremos una función para simplemente llamarle y ya está en esta función le vamos a dar la imagen y le vamos a dar el modelo simplemente para que lo que significa esta función es para digamos que sacarlas las labels de una imagen utilizando un modelo el modelo lo pasamos a modo evaluación esto digamos que internamente pytorch lo que va a hacer es decirle que no vamos a entrenar el modelo simplemente lo vamos a evaluar entonces hacemos algunas modificaciones en las capas necesarias y no se pasa ningún gradiente o digamos que no deja almacenado ningún gradiente preprocesaremos la imagen con la con el flujo que hemos visto anteriormente una y entonces ya la tendremos en tensor con esto la convertiremos en un patch en un batch perdón de una imagen y la pasaremos al modelo el modelo te va a devolver un vector de mil dimensiones vale por cada según las probabilidades de que tiene esa imagen de una parte en cada clase entonces obviamente como queremos la que más probabilidades tenga la clase que más probabilidades tenga entonces cogeremos el índice de la de la clase con más probabilidades y lo que vamos a hacer es convertirlo en porcentaje utilizando la función softmax que tampoco vamos a entrar en detalle lo que hace pero que hace bueno digamos que todas las probabilidades sumen uno entonces por tanto te sacará la te convertirá la que más probabilidades o sea la que más energía tenga a probabilidades y las mostraremos por pantalla según la clase el nombre que tenga dentro de la lista que hemos sacado de nombres de clases junto con el porcentaje que tiene o que ha sacado el modelo de seguridad de que está seguro de que en qué porcentaje de probabilidades una parte de esa clase entonces simplemente vamos a crear esta función y la metemos en memoria y vamos a probar primero con resnet dentro de resnet ya hemos visto que había diferente estaba resnet 50, resnet 101, resnet más grandes y nosotros vamos a probar de momento simplemente con la resnet 101 como un test y ahora es muy importante ver que le estamos dando el flag de pretrain como true, eso significa que va a descargarse los pesos de image net 1k y los va a meter dentro de la red neuronal, también se puede poner como false si por ejemplo queremos entrenar la red nosotros desde cero o una vez entrenada o sea cargada entrenada podemos quitar algunas capas y hacer las nuestras para pues digamos hacer transfer learning todo esto es muy fácil con la librería está de PyTorch, ahora nos va a descargar los pesos del modelo y ya tenemos el modelo cargado en memoria entonces lo que vamos a hacer es utilizar la función que hemos creado antes para imprimir para pasarle las diferentes imágenes el modelo que queremos usar y que nos imprima a ver cada una lo que cree que la primera es gato vale y fijaos ahora que no sólo nos dice que es gato sino que encima te dice en la la raza de gato que es que en este caso es un gato tigre con el con el perro lo mismo te dice que es un golden retriever con cierta porcentaje de seguridad aquí pues efectivamente te dice que es una mangosta creo que se llamaba con un 99 como 87 por ciento y luego pues el basún que es el fagot en inglés que tiene muy mala traducción efectivamente pero también nos lo ha sacado con muy buena probabilidad ahora vamos a ver muy rápidamente el resto de modelos aquí veis que cambia un poco el la forma de escribir aquí era pretrain igual a true aquí es weights igual a true también veis que aquí los los pesos son ocupan 171 megas aquí ocupan 1,14 gigas entonces por lo tanto ya es una demostración de que este modelo va a ser mucho más grande que la resnet y vamos a ver si tiene más precisión pero bueno tampoco como lo he dicho aunque falle en algunas de estas imágenes no es un modelo digamos una medida de la robustez de este de estos modelos aquí por ejemplo saca otra otra raza de perro del gato es lo mismo y los últimos dos digamos que baja un poco la seguridad que tiene de que son esa clase pero digamos que también las categoriza correctamente si queréis luego vosotros con la imagen del perro y la raza que pone aquí podéis buscar en internet a ver cuál es la que creéis que se parece más por último sin transformer bueno por último perdón luego van las estas por penúltimo sin transformer mucho más pequeño aquí también entended que aquí estamos utilizando el vt 32 que es un modelo medio grande y luego el sin transformer tiny que es el más pequeño que aquí las weights ya tienen que ser especificadas que quiere es el image net 1k y versión 1 por lo tanto son 108 megas que es más pequeño incluso que el resnet entonces por lo tanto también vamos a ver que va a tardar muchísimo menos que el resnet tampoco es que aquí estamos midiendo tiempos pero iba a sacar una pues aquí parece que tienen mucho que ver con el visión transformer ha sacado la misma clase con unas probabilidades bastante parecidas siendo una red muy diferente pero bueno eso estamos viendo cómo clasificar estas imágenes la ahora sí por último vamos a cargar el el com next que estaría un poco entre resnet y y visión transformer en cuanto a también a tamaño del modelo y vamos a clasificar estas imágenes parece que están la mayoría están de acuerdo a que es esta raza de perro pero no todas en general han clasificado pues obviamente aunque sea diferentes razas de perro el que es un perro tiene súper seguro lo que quiere jugársela más hacia la raza con el gato no lo ha dudado en ningún momento pero bueno también ha detectado efectivamente que el objeto que existe en esa imagen es un gato y luego pues algunos que son menos comunes también ha sido capaz de entenderlo por último focal net como hemos comentado no está la librería aún metida en la en perdón no está el modelo metido en esta librería pero sí que pues en el repositorio oficial de microsoft está todo el código abierto os lo podéis descargar los pesos pre-entrenados también con imagenet y para diferentes otras tareas incluso nada esto sería un poco la práctica de esta de esta sesión la hemos visto cómo coger una imagen reprocesarla para para cargar para hacerle un paso en un modelo pre-entrenado y sacarlos las salidas de ese modelo convertirlas a porcentajes de seguridad de la red y sacar el porcentaje más alto la clase con porcentaje más alto pues en muy pocas líneas de código.