---
title: Diego Bonilla | Reconocimiento Óptico de Caracteres
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #practica #Reconocimiento Óptico de Caracteres #OCR #Diego Bonilla
lang: es-AR
---
# Reconocimiento Óptico de Caracteres
![[500.E1_Reconocimiento_Óptico_de_Caracteres.mp4]]
[Reconocimiento Óptico de Caracteres](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres)
[Reconocimiento optico de caracteres (PDF)](501.E1.Reconocimiento_optico_de_caracteres-230120-152820.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/14kHTyVrfLQa2g2znTjS2Yiqkqy54O0Ci?usp=sharing)

Hola y bienvenidos a la práctica de reconocimiento óptico de caracteres. La idea que tenía yo para esta práctica era ver un poco lo que significa el reconocimiento óptico de caracteres o OCR, tanto casos un poquito también de poner en contexto qué significa esta tecnología que mover casos más en profundidad de... en qué tipo de industria se usa, un poco también cómo funciona internamente y luego saltar directamente a un caso práctico de una librería muy usada, actualmente si no me equivoco es la que más se usa, que es el Tesseract. También veremos qué funcionalidades tiene, qué tipo de herramientas dispone al usuario. Y luego veremos casos prácticos. En los casos prácticos, yo actualmente me encuentro en un proyecto con un banco español, en mi empresa Cognizant, donde estamos haciendo un caso en el que se utiliza el OCR como la principal columna donde se asienta el proyecto. Tanto mis compañeros como yo estamos usándolo en el día a día, y puedo daros de primera mano un caso práctico que se está usando hoy en día. Y luego, por último, veremos una práctica en la que nos iremos a Google Colab. Yo he creado unos cuantos celdas en Python y veremos cómo se ejecutan y las diferencias que tiene el OCR con diferentes imágenes, cómo funcionan. En este caso, como ya hemos comentado, el OCR de la lección ha sido Tesseract por lo mucho que se utiliza hoy en día y lo fácil que es usar. Bueno, para empezar y poner también en contexto, ¿qué es el reconocimiento óptico de caracteres? Pues es una tecnología que convierte, evidentemente, texto que procede de documentos que son digitales, que no tienen texto extraíble per se, como podría ser un Word o un PDF, pues los extrae el texto. En este caso, Deep Learning es la metodología más usada. No siempre ha sido Deep Learning, pero actualmente utilizan sistemas inteligentes para detectar el texto y extraerlo. De una imagen, puede ser también un documento escaneado, con un escáner, puede ser también un PDF que no tenga texto extraíble. Los usos principales... del reconocimiento óptico de caracteres es crear flujos de trabajo automatizados digitalizando documentos de diferentes unidades comerciales. Puede ser tanto. Pues podemos ver aquí un ejemplo donde se utiliza uno CR para identificar la matrícula de un coche pero también podemos ver que no sólo se queda. en la matrícula, sino que encima es capaz de detectar las entidades, tanto del estado en el que fue una parte donde está este vehículo, la matrícula, alguna información extra, la marca... Entonces digamos que también en el OCR no se queda solo en saber dónde está el texto y ser capaz de reconocerlo, sino además también hay un fuerte reconocimiento de entidades, también de tipo de reconstrucción de documentos, saber... solo el texto en el documento que papel forma ese texto, entonces por lo tanto luego todo esto se puede meter en un flujo de inteligencia artificial o en un flujo de información que da muchísima información. También tener en cuenta que todo este reconocimiento de entidades se hace de una forma anónima, no tiene que estar nadie detrás etiquetando estos documentos, entonces por lo tanto ya no solo es quitarse muchísimo trabajo y automatizarlo, sino también se vuelve más seguro ya que se quita el factor humano en este caso. Entonces, todo lo que viene a ser el escaneo automático de DNIs o pasaportes o documentos de extractos bancarios se convierte en algo completamente anónimo ya que no hay nadie detrás en este caso. También, pues eso, lo he dicho, el reconocimiento óptico de caracteres también puede ir apoyado con otros sistemas, como el reconocimiento de entidades, como estamos viendo en las imágenes que reconoce tanto el texto como qué parte forma, si es el título, si es el precio, si es una tabla. Reconstrucción. que sería el caso de poder reconstruir el orden lógico de los documentos o las partes de un documento y por último clasificación que sería por ejemplo este documento es una matrícula, este documento es un DNI, este documento es un pasaporte, todo lo que engloba esa parte. Más específicamente, un documento. un motor que se encarga de hacer este tipo de trabajo es el Tesseract, actualmente es el más usado ya que es súper fácil de usar y además es de código abierto y es gratis. Tesseract es de HP, fue creado en HP pero luego fue esponsoreado patrocinado por Google, como hemos dicho es open source. Actualmente soporta 116 idiomas y 37 alfabetos diferentes. Y ya no solo detecta el texto, sino que encima es capaz de devolver la jerarquía del texto. Esto significa que te puede decir que, por ejemplo, si un texto depende de otro anterior, si está dentro de una tabla, si uno es el título y el resto es algo que lo sigue. Esto es capaz de detectarlo. También es capaz, como detecta varios idiomas, pues no solo es capaz de leerlo de izquierda a derecha, sino también de derecha a izquierda y en diferentes formas de texto. Ya sea, por ejemplo, en texto tipo columnas, si es un texto que forma un círculo o si son tablas y, bueno, muchas diferentes otras parámetros que se le puede dar al Tesseract para que detecte tipos especiales. En este caso el Tessera desgraciadamente tiene un problema que es que está entrenado, bueno, digamos que es muy fuerte en documentos, como podemos ver en la imagen, pero no es capaz de detectar texto salvaje, que se le llama. Que es texto, por ejemplo, de hacer una foto a un... a un coche y que te identifique el texto de la matrícula le va a costar mucho. Necesita un documento ya recortado, rotado y un documento que se parezca mucho a un PDF para detectar el texto muy fiablemente. Entonces, por lo tanto, incluso los propios de Tesseract recomiendan aplicar un preprocesamiento a la imagen antes de pasarla por el motor Tesseract. En este caso puede ser mejora de contraste para que la diferencia entre el fondo del documento y las letras quede muy resaltada y se diferencie muy bien. También puede ser una corrección de perspectiva. Si por ejemplo hemos hecho una foto del documento, que en las cuatro esquinas del documento se agranden y se deshaga esa perspectiva y que quede como si lo hubiéramos escaneado. Algunas aplicaciones de móvil lo hacen automáticamente, pero el Tesseract si no se hace ese preprocesamiento no va a ser capaz de detectar casi nada de texto o le va a costar mucho. Y en el mismo caso va a pasar en la rotación. Si le damos un documento que está rotado... tampoco va a ser capaz de detectar ya que presupone que el texto está completamente recto una vez que le damos la imagen. Esto es un poco lo que hemos visto, lo que explica también otro preprocesamiento que se hace, reducción de ruido, si también es una técnica bastante fácil de utilizar. realizar es básicamente reducir el ruido de la imagen. Se puede hacer tipos de filtro que reducen el ruido de la imagen sin tocar mucho o intentando no quitar muchas partes de texto. Y esto es, en la imagen que vemos a continuación, es un flujo de, digamos, de procesar una imagen con un OCR. La imagen sería la entrada. y digamos que primero haces unos cuantos pre-procesamientos a la imagen que hemos comentado, pues en un filtro, puede pasar el quitar la rotación, convertir a blanco y negro o mejorar el contraste, quitar el ruido. y luego internamente, en este caso del tesseract, hace toda esta parte automáticamente. Simplemente tenemos que llamar el tesseract a partir de aquí. Entonces internamente el tesseract lee las líneas, detecta cuántas líneas hay, a partir de las líneas las palabras, utilizando los espacios, y luego los caracteres que tiene cada palabra, y ahí ya se detecta cada carácter, se hace un posprocesamiento, utilizando conocimiento interno de gramática. y te devuelve el texto. Luego el texto, también es posible que haya que hacer una etapa de posprocesamiento del texto porque evidentemente no se puede adaptar a todos los casos que existen y a lo mejor tienes que adaptarlo luego para cada caso y cada tarea que quieras realizar la salida a lo que estés tratando o a tu caso específicamente. Estos son algunos parámetros que le podemos dar al tesseract, como lo que hemos comentado, que puede detectar el texto en muchas formas, tanto digamos que si es de izquierda a derecha o si por ejemplo va por columnas, si es una parte de un círculo o por tablas, digamos que esto es lo que el Page Segmentation Mode, o PSM, es un parámetro que le podemos dar. Podemos decirle que sea una columna sola. que digamos que es un bloque uniforme de texto alineado verticalmente, que simplemente es una palabra, que simplemente sea un texto, una línea de texto que forma un círculo, que simplemente sea un carácter. Y luego el 11 y el 12 es un poco, digamos, en cuenta todo el texto posible sin que forme parte de ninguna jerarquía, o sea, ninguna segmentación de página. Eso, todo esto son más opciones que nos da el Tesseract. para poder adecuarnos a nuestro tipo de problema. Como he comentado antes, en mi empresa actual, en Cognizant, estamos llevando a cabo un proyecto muy grande de inteligencia artificial pura, de lectura, escaneo y reconocimiento de documentos para una empresa de banca española. Y... Y como hemos comentado, ahora mismo es el proyecto más grande de Inteligencia Artificial Sur de Europa y utilizamos las salidas del OCR para sacar información de cada documento, cómo aparecen las imágenes, evidentemente las imágenes están sacadas de internet, no son del proyecto actual. Podemos también unir... diferentes documentos que vienen de diferentes partes y unirlos a un mismo usuario o a una misma empresa. Podemos luego dirigir cada documento a donde va porque sabemos lo que contiene y sabemos exactamente cuáles son los pasos siguientes para ese documento. Y luego también es algo que se puede escalar, ya no solo, digamos, a la clase que estamos viendo ahora, también somos capaces de escalarlo a otro tipo de tareas. todo esto partiendo de un sistema que detecte los caracteres, o sea, que detecte el texto de los documentos y la jerarquía también de los documentos de manera eficaz. Efectivamente, pues es algo que interesa mucho actualmente, específicamente, pero no sólo limitado a las bancas porque lo que hemos dicho es algo que se puede automatizar muy fácilmente. que se quita muchísimo gasto humano de recursos y luego encima al quitar esa parte humana pues también se reduce mucho la latencia y también se aumenta mucho la seguridad. En este caso pues ya hemos visto que todo el reconocimiento de caracteres y tal son algoritmos entonces por lo tanto es completamente anónimo y no digamos que no se va a quedar con ninguna información que a lo mejor un humano sí que podría tener. Y el resto de la sesión vamos a ver el collab. Aquí tenéis el link que coloquiqueis cuando vayamos todos. En esta práctica lo que he preparado es, digamos, un par de formas de leer el texto de un documento. tanto si el documento es un PDF como si es una imagen. Veremos que si es un PDF a lo mejor tiene el texto extraíble. Entonces accederemos a la meta información del PDF con el VREA que se llama PDFminer. Entonces podremos leer el PDF y extraer el texto. Lo bueno de la librería es que si hay texto extraíble a lo mejor también es capaz de extraer las imágenes, por ejemplo. Entonces nos daría mucha información. Estaría también junto al texto te devuelve su posición, te devolvería también obviamente el texto y demás información. Lo dicho, es muy útil porque hay muchas veces que a lo mejor no necesitamos un OCR si los documentos que nos dan son en PDF. Sin embargo también hay que tener en cuenta que no todos los... PDFs tienen texto extraíble. Entonces, para el resto de casos, tanto imágenes como PDFs sin caso extraíble, cogeremos e instalaremos el Tesseract en Colab y veremos rápidamente unos cuantos ejemplos de casos en los que funciona muy bien, casos en los que no funciona tan bien, cómo corregir esos casos, a lo mejor aplica alguna técnica de mejora. y ver como afecta eso a la salida del modelo. Nos encontramos aquí en el collab, poco os lo he preparado ya por partes, está ya separado por los diferentes ejercicios que vamos a hacer, así que nada, todas las celdas se van a ejecutar desde arriba hacia abajo, entonces por lo tanto esto ya lo he ejecutado yo pero vosotros tendréis que instalar las dependencias, seguramente os pondrá un mensaje que tendréis que leer. hacer un restart de la rutina del collab así que le dais y ya estaría. Aquí es donde vamos a importar todas las librerías, vale, si no estáis relacionados con Python no pasa nada, simplemente todas las funciones que vayamos a usar las tenemos que tener dentro del programa y si no pues no vas a ver dónde buscarlo. todas las imágenes y los archivos y los pdfs que usemos en esta demo no van a estar en el colab ni os vais a tener que meter en ningún filesystem simplemente utilizando la función de wget podemos acceder a cualquier documento que esté en la web y descargarnoslo para meterlo automáticamente en el filesystem que viene con él en mi caso ya he ejecutado algunas de estas celdas este por ejemplo es el pdf si no recuerdo mal el de atención vale entonces nos lo descargamos vale que parezca todo correcto y aquí es donde como es un pdf que bueno podéis ver que yo me puedo meter en el pdf y puedo seleccionar el texto entonces por qué no intentar acceder a la meta información que tiene el propio pds y extraer la información que tiene ese pdf porque a lo mejor no tengo que tirar un OCR si puedo acceder directamente a esa información. Entonces aquí comentándolo línea por línea cargaremos el archivo, en este caso utilizaremos la función open de Python y le daremos el flag de read bytes para que lo lea como un buffer de bytes. y utilizaremos la librería pypdf2. Entonces le daremos el objeto que ha leído The Bytes, y se lo daremos a la función de FileReader. En este caso ya nos devolverá un objeto de la librería. Entonces en ese objeto podemos acceder a sus atributos, como por ejemplo el número de páginas. pero también podemos acceder al documento, a la información que tenga ese documento. Entonces en este caso lo vamos a guardar en una variable que se llama información, info, y luego lo mostraremos por pantalla. Entonces de tal forma que el número de páginas va a ser el número de páginas, el título va a ser el título, el autor el autor, ¿vale? Y así un poco iremos a ver qué información podemos extraer directamente del PDF y lo comprobaremos con el propio PDF a ver si es correcto. así que vamos a ejecutarlo. Ha sido muy rápido obviamente. Vemos que el número de páginas son 11 y podemos ver aquí que efectivamente tiene 11 páginas el documento. El título o title, attention is all you need, efectivamente así es el título. El autor, pues varios autores que podemos ver que están aquí todos referenciados. El subject, esto ya no lo pone en el PDF en sí, sino que es de la información que tiene el documento. El creador vemos que no está definido y el producer tampoco. Entonces creo que automáticamente se pone la librería que hemos usado. Pero bueno, el resto de cosas, aquí hay información muy útil, tanto el autor como el título del documento, como el número de páginas. Yo personalmente, la información que utilizo en el proyecto que estamos ahora, sobre todo, es el número de páginas para poder iterar sobre ellas. Antes de comentar que tanto hay librerías como pdf2 que te extran la meta de información del texto, también hay otras que te convierten los pdfs a imágenes para luego poder procesar los tugs. Por otra parte, si no hay suficiente texto extraíble en los documentos o no hay directamente. Aquí voy a comprobar antes que se esté grabando todo. Un momentito, vale, disculpad. Aquí es donde vamos a utilizar justo la variable de número de páginas, que como hemos visto nos devuelve un número entero, en este caso 11, y vamos a hacer un bucle for que significa para cada página vamos a iterar sobre ellas y leeremos el contenido. Disculpad el corte, he visto un pequeño fallo que había en el código y lo he corregido. Fallos del director. Entonces una vez creado el objeto de la página utilizando el objeto que nos ha devuelto antes la función y la función getPage le damos como argumento el número de la página. y nos devolverá un objeto que contiene información de esa página. En este caso nos interesa el texto entonces llamaremos sobre ese objeto a la función extractText. En mi caso vamos a imprimir, he decidido imprimir un salto de línea y unas cuantas barras para separar las páginas, que nos imprima también el número de la página y luego el contenido de la página y luego pues cerraremos el pdf cuando lo hayamos dejado de leer. Vamos a ejecutarlo. vale pues podéis ver que se ejecuta efectivamente todo entonces vamos a ir al principio vale pues tampoco vamos a leerlo aquí pero bueno página 0 vale empieza a contar desde 0 entonces acabará en la 10 pues eso attention is all you need los diferentes autores el abstract vale todo esto pues evidentemente se corresponde a esto de aquí y digamos que ya tenemos un texto que es modificable por un ordenador. Yo puedo a partir de aquí buscar por ejemplo cada, por palabras clave, como haremos a continuación y o estar diferente información que antes pues evidentemente no podía. Vamos a limpiar este output. Ahora vamos a ir con imágenes. En este caso evidentemente una imagen no contiene texto extraíble ni leíble por una máquina a priori. Entonces vamos a decodificar ese texto, vamos a leerlo utilizando Tesseract para poder acceder a luego ya a modificar ese texto o a ver qué tal lo ha hecho. Esto es una pequeña función que utiliza la librería Matplotlib. para mostrar imágenes por pantalla. No voy a ir muy en profundidad evidentemente de qué significa esto y todo eso pero bueno básicamente como estamos utilizando OpenCV que es una librería de computer vision para leer las imágenes como matrices de números pues digamos que OpenCV lo lee en formato BGR que es blue green and red pero luego para mostrarlo por imagen lo muestra en RGB por lo tanto pues hay que cambiarle los colores y esto es lo que significa. No mostramos el axis porque en este caso estamos mostrando imágenes y lo mostramos a la pantalla. Una vez ejecutada la función esta, para que la meta en memoria vamos a descargarnos la primera imagen que es esta de aquí. Es una imagen bastante, digamos, se lee todo bastante claro, no es un escaneo ni nada, es una imagen completamente digital. no tiene muy buena resolución pero bueno es más que suficiente entonces esto digamos que es un caso perfecto. Aquí cargaremos las imágenes, utilizaremos la función de OpenCV para leer la imagen, la mostraremos por pantalla y luego le daremos esta configuración al tesseract. La configuración, si lo recordáis de lo explicado en el pdf, es el page cementation 11 que significa que el pdf está saca el texto que pueda sin preocuparte de cómo esté repartido por la imagen, por así decirlo, y luego le vamos a especificar que el idioma es el inglés. Aquí ya podéis hacer modificaciones de qué pasaría si le quito el idioma inglés, si funciona peor o mejor. No siempre funciona mejor, no siempre funciona peor, es un poco dependiendo de cada caso. Y luego, lo que sí que es obvio es que si queréis detectar caracteres especiales que tenga un idioma, siempre es bueno decírselo porque si no va a ser muy difícil o directamente no os lo detectará, esos caracteres. y directamente llevamos a la función, es una función super fácil, de tesseract que le damos la imagen y es imageToString y como indica la función pues va a pasar de una imagen a un string de palabras o un texto ejecutamos, nos muestra la imagen y aquí nos muestra el texto que he extraído y pues lo podéis comprobar vosotros pero vamos está perfectamente extraído no se ha equivocado en ninguna parte efectivamente es un texto muy fácil y nos ha extraído sin mucha dificultad el texto, pero también no solo devuelve el texto, hay otra función que se llama imageToData que le volvemos a dar la imagen y la configuración nuestra como entrada y vamos a ver que devuelve este tipo de función. Aquí vemos que devuelve como un data frame en el que nos devuelve información de nivel, número de página, número de bloque, parNum, número de línea, número de párrafo, número de línea, número de palabra, left, top, width, height, const, text. Esto es una... un data frame o un diccionario que nos va a devolver mucha información por cada palabra. ¿Vale? Veis que la palabra this, que es la primera de todas, pues te dice que un poquito de izquierda, de derecha a izquierda la confianza es del 96%, o sea que está muy seguro de que pone esa imagen. Vamos a abrir todo lo que hay aquí porque esto ya son cosas internas de párrafos o de caracteres especiales. La siguiente información nos va a dar la posición que tiene ese texto en la imagen, los píxeles en los que se encuentra ese texto. Y luego todo esto nos va a decir que fue una parte de un nivel número 5, el número de página número 1, el bloque 1. el párrafo 1 y digamos que diferente información sobre jerarquía que contiene más de información sobre el texto que es en plan pues digamos que no sólo ha detectado this sino que te dice exactamente dónde está localizada y qué jerarquía forma con respecto al resto del texto. Vamos a también a limpiar un poco este output. Vamos a probar ahora con imagen de peor calidad. Vamos a ver qué imagen es. Ya nos lo he imprimido aquí por pantalla. Es una imagen que podemos ver que también es texto por texto de digital perdón, digital no, texto de documento pero la calidad es mucho. Pues lo primero es que está un poco rotado luego además no hay mucho contraste, no era como la imagen esta en la que el negro y el blanco se diferencia muchísimo, aquí hay unos cuantos grises en medio y luego además pues la calidad también es muy baja, entonces vamos a ver si es capaz de detectar. Bueno, aquí no lo he mencionado pero volvemos a llamar con la misma configuración a la misma función que antes, nada nuevo, imprimir el texto. Vemos que sí que es capaz de detectar las primeras líneas. pero luego la palabra dog se lo ha comido. Pues digamos que hacer un debut de esto es un poco complicado porque evidentemente todo esto es muy alto nivel pero por lo que sea no ha detectado la palabra dog en este caso si era una abstracción pues casi perfecta pero vemos que ha fallado en dos caracteres. Si también podemos ver la posición y la jerarquía que ha estudiado el texto y podemos ver también que si antes esta jerarquía era la misma osea digamos nos devolvía caracteres ups disculpad voy a cargar la imagen esta un segundo la confianza que nos daba del texto 95, 96, 96 96% osea digamos que una precision muy alta vemos que ahora esa nueva confianza se convierte pues también hay muchos 96 pero en la palabra D por ejemplo ya ha bajado a 80 y directamente el DOG la O y la G y el punto nos los ha cogido Entonces podemos ver también que es bastante sensible a cualquier deformación de la imagen. Vamos a probar ahora con la imagen manuscrita, que para nosotros es muy fácil leerla, pero vamos a ver si el TSRack es capaz de detectar texto manuscrito. Vale y vemos que falla bastante. Esto es porque digamos que todo lo que se vaya fuera de, como hemos dicho en el PDF, en la presentación anterior, todo lo que se salga de documentos, por así decirlo, de uso, no es capaz de detectar tanto ya cosas rotadas, de peor calidad, manuscrito, otro tipo de... de letra que no sea la típica de documento entonces pues todo eso no es capaz de detectarlo. Ha hecho lo que ha intentado su mejor pero no ha sido capaz. Vale ahora vamos a hacer un pequeño ejercicio por último ya para acabar de extracción de entidades en la que vamos a leer esta página, vamos a imprimirla. que es un documento legal, está extraído de internet, digamos que no significa nada, está en formato imagen, entonces por lo tanto no podemos extraer ningún texto a priori. pero digamos que yo quiero tener un sistema que automatice esto, que los usuarios me puedan enviar estos documentos en foto y yo pueda acceder al nombre, a yo que sé, por ejemplo, el número de teléfono y a dónde reside. Entonces vamos a crear un pequeño flujo de imagen en el que hagamos exactamente esto. ¿Cómo haré ese flujo? Pues cuando extraiga el texto a la imagen podremos ver que tanto la palabra resident name y fake person en este caso están muy juntos, están digamos seguidos en la extracción del texto. Entonces si yo busco la palabra clave resident name pues me dirigirá automáticamente al nombre de esa persona. Entonces lo primero como hemos dicho vamos a convertirlo a texto. Pues aquí nos lo ha convertido todo a texto. el texto que ha detectado no es muy bueno en este caso porque digamos que no tiene muy buena calidad el documento para empezar pero bueno lo que es necesario sí que lo ha extraído bien. Entonces vamos a hacer ahora unas cuantas funciones de Python. Voy a explicarlas un poco por encima. Esto lo que nos va a hacer es quitarnos caracteres especiales como saltos de línea o saltos de diferentes caracteres del propio string de Python. Luego lo separaremos por saltos de línea. Disculpad, el stream no quitaría saltos de línea, efectivamente. Porque aquí lo separaremos el texto por saltos de línea. Es decir, cada una de las entidades, cada una de las líneas va a formar parte de un índice diferente en una lista. Luego, cada uno de los índices de esa lista lo vamos a separar por espacios. por estos espacios de aquí. De tal forma que ya vamos a tener una lista que contiene todas las palabras del documento. Esto simplemente es para convertirlo a la raíz de palabras. Ahora vamos a filtrar, vamos a quitar todas las palabras que no tengan texto porque por lo que sea a lo mejor ha habido dos espacios seguidos entonces no hay ningún texto en esos espacios. vamos a ver cómo se limpia el texto. Como hemos dicho ya tenemos un array con todas las palabras, un poco limpio. Entonces yo quiero extraer, pues como hemos comentado antes que está el nombre, el número de teléfono, la ciudad y el código postal. Entonces podemos ver que en la imagen pues tenemos que buscar recién el nombre, tenemos que buscar teléfono número. y tenemos que buscar también el zip que estaría aquí. Entonces vamos a hacer eso mismo, buscarlo y la función que tiene para buscar es index, la función que tiene Python. Entonces yo buscaré nombre y me quedaré con qué índice de este array contiene la palabra nombre y lo mismo con el resto. Entonces ahora lo que voy a hacer es sabiendo que el nombre me espero dos palabras, que con el teléfono me espero a lo mejor dos palabras también, con la ciudad de residencia también me espero unas cuantas palabras predefinidas y el código postal es solo una palabra, pues entonces puedo acceder a esa información dentro de la lista. En este caso pues el idre también está un poco hecho a drede para que funcione bien porque lo he comprobado que funcionaba bien. pero no has extraído correctamente esa información de forma automática. O sea, digamos que yo solo le he dado una imagen de entrada, completamente que no tiene nada de texto extraíble, y tendría que estar una persona detrás accediendo a esa información y leyéndola. Pero se le ha dado al tesseract y con un poquito de programación básica en Python he sido capaz de extraer las entidades que yo quiero. ¿Vale? Esto es un ejemplo muy básico. Efectivamente, no es la mejor forma de hacerlo. pero digamos que es un ejemplo suficientemente que demuestra un poco la capacidad y la facilidad que tienen estos algoritmos de muy pocas líneas poder hacer un sistema automático automatizado de extracción de esas entidades. Esto sería el laboratorio y con esto concluye la práctica de OCR. cualquier duda que tengáis o cualquier sugerencia que tengáis me podéis escribir como hemos comentado a tanto al linkedin como a github y tanto también dentro del linkedin está el correo electrónico si os viene mejor y eso.