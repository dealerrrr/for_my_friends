---
title: Diego Bonilla | Generar Imágenes con Stable Diffusion
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #practica #Generar Imágenes #Stable Diffusion #Diego Bonilla
lang: es-AR
---
# Generar Imágenes con Stable Diffusion
![[502.E1_Generar_Imágenes_con_Stable_Diffusion.mp4]]
[Generar Imágenes con Stable Diffusion](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion)
[Generar Imágenes con Stable Diffusion (PDF)](503.E1_StableDiffusion-230120-152902.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1KwcbPDiRL_RArlVdWvopIZ-_nEVICn1K?usp=sharing)

Hola y bienvenidos a un nuevo curso de W3. En este curso vamos a dar Stable Diffusion y diferentes modelos también de traducción de texto a imagen o de generación de imágenes a partir de texto. La idea que estaba pensada para esta clase era empezar con un poco la trayectoria que han seguido estos modelos, ver un poco sus inicios, cómo han empezado, las primeras redes que hacían este tipo de tarea y luego también pasar directamente a dónde está el state of the art ahora mismo, los modelos actuales, qué tipo de imágenes pueden generar, para qué se pueden usar. Como ya sabéis esto también interesa tanto a nivel de investigación, a nivel de deep learning y inteligencia artificial. Evidentemente es un tema extremadamente interesante y con mucho futuro y que está teniendo muchísimo desarrollo en estos últimos, sobre todo en estos últimos meses y en estos últimos dos años. Entonces veremos un poco por qué existe ese interés y luego también cómo se puede usar y qué significa para el arte sobre todo, porque claro ahora como se está poniendo de moda, por ejemplo, la generación de arte digital de NFTs, de arte digital y de compra y venta de estas piezas de arte, pues también un poco este tipo de modelos ayudarían o cambiarían un poco ese tipo de mercados, de tal forma que cualquiera que tenga conexión a internet y la mayoría de veces gratis o pagando muy poco dinero puede generar imágenes distinguibles de imágenes reales o de piezas de arte generadas por cualquier autor en cualquier estilo a lo largo de la historia, básicamente con este tipo de herramientas. Entonces por lo tanto son bastante útiles de conocer y evidentemente van a ser muy prácticas y muy usadas en el futuro. Por último vamos a ver una práctica, unos pequeños ejemplos hechos por mí para vosotros en Collab. Está todo bastante preparado, simplemente iremos ejecutando e iremos explicando cómo generar imágenes de forma gratis desde Collab con muy buena calidad. Eso lo veremos al final. Para empezar, como ya hemos comentado, vamos a ver un poco de dónde viene este tipo de modelos. La verdad es que son relativamente recientes. Por ejemplo, uno muy famoso fue Stack Gun, que fue del 2017 y era un hito, por así decirlo, para este tipo de modelos porque hasta ahora no se han generado imágenes de tanta calidad. Evidentemente para hoy en día no es muy buena calidad, pero bueno para aquel entonces la verdad es que era una pasada y como no sé si se puede ver por la imagen, pero digamos que tiene dos etapas el modelo. La primera etapa generaba un pequeño boceto muy borroso con mucho ruido del texto que se intentaba generar y luego una segunda etapa lo que hacía era con el boceto y con el texto intentar realizar una especie como de super resolución o de mejorar la resolución y la calidad de la imagen teniendo en cuenta también lo que se quiere generar. Digamos que pensando desde un punto de vista estadística es un problema que se llama de one to many porque digamos que un mismo texto puede generar una cantidad casi infinita o infinita de imágenes. Por lo tanto es el modelo el que no hay una respuesta correcta. Sí que existen incorrectas o mejor dicho lejanas a la idea que se quería generar, pero digamos que no existe solo una respuesta correcta. Esto hace que al entrenarlo digamos que sea muy complicado y muy complejo. En este caso se entrenó esta red neuronal la stack GAN en la metodología GAN que es una forma de generar imágenes en la que se contrasta la generación con un discriminador que te dice si lo que se ha generado es realista o no, por así decirlo. Intenta saber si lo que se ha generado es falso o verdadero y con la salida de este discriminador también hay un bucle en la que se se entrena al propio generador para que aprenda a engañar al discriminador. Entonces al final este bucle y este juego lo que hace es generar imágenes con mucha calidad y con no demasiados datos ya que es casi imposible o es muy difícil o teóricamente no se podría hacer overfitting de este tipo de modelos ya que no ven los datos originales. Esto sería en 2017. Vamos a dar un salto de cuatro años al 2021. En enero del 2021 la compañía que empezó un poco esta esta nueva moda de generación de imágenes a partir de texto fue OpenAI. OpenAI en enero del año pasado del 2021 sacó su primera versión de Dall-E que era un modelo que utilizaba un tipo de generación de imágenes que se llama VQ-VAE que es Vector Quantisized Variational Autoencoder que digamos que lo que utiliza es una representación discreta de los datos en lugar de continua como por ejemplo pueden ser las GANs o otro tipo de modelos más usados para este tipo de generación de imágenes. Esto lo que hace es que tenga muchísima más representación, ese espacio embedido y además que de capacidad a muchas más variedades diferentes. Entonces por tanto gracias a esta nueva representación digamos que era fue más entre comillas fácil o fue un poco el desencadenante de que la calidad de las imágenes fuera mucho mejor y en este caso se demuestra en la imagen de la izquierda es la del 2021 y la de la derecha se correspondería a su variante más actual en abril del 2022 creo que es un poquito anterior pero bueno el paper revisado la última revisión que se hizo fue en abril del 2022 en el que OpenAI sacó su segunda versión y bueno la calidad el salto de calidad fue increíble no solo tampoco de calidad sino también de tamaño y de entendimiento del texto y un poco lo que cambió de un modelo al siguiente es que el siguiente no utiliza la metodología que hemos hablado de los VQBAE sino que utiliza un proceso de generación de imágenes en este caso o de datos en general que se llama modelo de difusión. El modelo de difusión digamos que es una técnica en la que a una imagen conocida se le añade ruido poco a poco paso por paso en el que en cada paso se le va añadiendo más ruido y digamos que en cada paso que se realiza lo que lo que se crea es una renderonal que vea un poco cómo se ha añadido el ruido y que aprenda a deshacer ese ruido todo esto por pasos entonces al final pues por ejemplo tenemos 700 pasos en los que se ha añadido 700 veces un poquito de ruido qué pasa que al final después de añadir 700 veces ruido pues te queda una imagen muchísimo ruido tanto ruido que digamos que es indistinguible de una imagen simplemente de ruido que digamos que la imagen original queda completamente ocultada y eclipsada por el ruido pero como hemos tenido una renderonal que ha aprendido o le hemos enseñado a deshacer cada paso lo que vamos a hacer es empezar desde el paso 700 y deshacer y que cree de esta forma hasta que deshaga todo el ruido vaya quitando el ruido en estos 700 pasos entonces nos queda al final una imagen sin nada de ruido como las que vemos en la pantalla esto es lo que nos permite dado que después de todos esos 700 pasos te queda ruido básicamente porque la imagen ya ha sido completamente destruida pues lo que vamos a hacer es simplemente coger ruido aleatorio y pedir que deshaga los pasos entonces este proceso a partir de ruido aleatorio puede es capaz de generar imágenes que no existen en la en nuestra digamos en nuestra base de datos que no ha sido entrenado con ellas por así decirlo entonces lo que hacemos es al generar al realizar este proceso inverso de difusión que se llama el quitar ruido lo que hacemos es condicionarlo con contexto entonces si simplemente le ponemos una condición textual a este proceso de difusión inverso lo que hacemos es condicionar que digamos que genera me o quita el ruido de esta imagen sabiendo que en esta imagen hay pues por ejemplo lo que puede ser eso cerro en un campo en el estilo de moned por ejemplo entonces por lo tanto es capaz de generar datos con muchísima más calidad y con digamos que también el proceso de generación de imágenes mucho más estable que utilizando por ejemplo las guns como hemos visto en la diapositiva anterior y esto permite también con data sets mucho más grandes como por ejemplo la que fue entrenada Dall-E que son billones de imágenes pues también el tener información de mucha cantidad de diferentes objetos artistas animales situaciones y todo un poco sin embargo menos conocido en mayo del 2022 google por ejemplo también ha sacado un par de modelos de este estilo google lo que hizo es generar utilizar otro tipo de arquitectura que no vamos a entrar tampoco en él es un poco parecida a la anterior que hemos visto de uvecubae combinada con guns pero era muchísimo más robusto en algunos problemas que tenía open y el Dall-E de open y tiene un gran problema por así decirlo que es que no sabe escribir vale como se puede ver en la imagen de la izquierda estos serían textos generados por open y por Dall-E la que podéis usar todos pagando una pequeña cuota en su en su api digamos que si le pones un cartel que por ejemplo que genere un cartel en el que se escribe en el que se puede leer deep learning pues ya veis que genera un poco de todo menos lo que le estamos pidiendo mientras que por ejemplo si a esta a la imagen de google le pedimos que genere una una tienda en la que se puede leer en el que tiene un letrado de que ponga texto image pues es bastante elegible de forma humana mientras que en esta parte de aquí sería también la generada por Dall-E en la que no ha conseguido leer escribir el texto y hay veces que pues parece como una especie de combinación rara de las letras o que como que intenta hacerlo pero no lo consigue y esto es un poco de la potencia del modelo no permite no tiene tanta entendimiento de geometría sobre todo le cuesta bastante y es algo que también le cuesta a la mayoría modelos a la imagen también que es la geometría del entendimiento de una escena a nivel geométrico es algo extremadamente complicado que obviamente lo sabemos desde nacimiento pero digamos que al decir por ejemplo quiero una foto en la que haya 35 pelotas de ping-pong pues a este tipo de redes le cuesta muchísimo porque claro tiene que entender pues que por ejemplo que una pelota de ping-pong donde empieza donde acaba nivel de píxel o nivel de concepto que se vean todas por la imagen que no haya ningún tipo de contaminación entre conceptos entonces claro a partir de creo que eran de 7 o de 8 objetos se lía si le pedimos por ejemplo a partir de 10 pelotas de tenis o lo que sea pues en principio se va a empezar a liar pero bueno en texto pasa un poco algo parecido en el que digamos que en texto tienes que entender el orden de lo que se está escribiendo aparte de también entender que luego un humano lo pueda leer y es algo que OpenAI, Dall-E perdón, no lo entendía y por ejemplo el de imagen en este caso sí que se el imagen de google sí que lo consigue llegar a entender luego google sacó otra versión de un modelo de texto-imagen estos ya como os digo no están públicamente accesibles y no parece que los vayan a lanzar en ningún momento son modelos mucho más grandes que Dall-E pero aparte de eso también pues digamos que la calidad de las imágenes en principio es mejor y luego bueno ya podéis ver por las imágenes que ya no sólo conceptos un poco más abstractos como una cobra hecha de sushi o lo que sea que a lo mejor también Dall-E también lo podría llegar a generar o por ejemplo un wombat tomándose un un cóctel con camiseta hawaiana y todo eso también Dall-E lo podría llegar a generar pero en el texto como vemos en la en la de la derecha del todo el texto es de muy buena calidad y cumple con todas las los requerimientos de diseño y de sobre todo de estilo que se le pide entonces pues la verdad es que es un buen paso hacia esa dirección de entendimiento de la escena sobre todo se ve al final el texto el que escriba algo es una digamos que es una excusa para saber si o determinar si un modelo es capaz de entender cosas muy complicadas como por ejemplo en la escritura entonces pues digamos que si es capaz de escribir texto que nosotros podamos leer es una muy buena señal una cosa a tener en cuenta también que pues también ha sido bastante hablado a la vez que se comen que se iban saliendo este tipo de papers es que pues evidentemente muchos todo esto este research se está haciendo desde américa principalmente entonces por lo tanto pues los datos también de internet en general están un poquito o sea bueno tienden hacia la cultura americana europea entonces por lo tanto es un poco razonable o sea es razonable el decir que existe un baias una tendencia en este tipo de modelos dado que pues evidentemente hay un punto de referencia en el que en el que se crean estos modelos entonces por lo tanto pues evidentemente siempre que quieras yo que sé por ejemplo medir la palabra exótico pues siempre que pues en cualquier parte que te vayas de la tierra la palabra exótico se va a significar cosas completamente diferentes y en la por ejemplo la imagen que vemos a continuación en esta parte de aquí la palabra exótico pues evidentemente tiene una definición muy europea y muy digamos americana y una persona exótica pues tampoco te va a salir una persona como como las que estamos acostumbrados a ver y sale pues un punto de vista muy diferente al de por ejemplo cualquier otra persona de otro país también estas son este por ejemplo el punto de vista de una persona exótica pues puede ser una de las menos perjudiciales pero sí que pues otras sí que fomentan los estereotipos que tenemos en este tipo de países y lo cual es un poco evidente y lo que estaba comentando en la diapositiva anterior es que evidentemente el si los datos es garbachín garbachado si le das basura te da basura entonces si los datos que le hemos dado es internet entero y en internet por lo que sea pues está plagado de de este tipo de tendencia americana europea pues evidentemente lo que saca este modelo va a ser ese tipo de cosas que ha aprendido esto puede ser evidentemente perjudicial también evidentemente si a este tipo de modelos en vez de simplemente decirle quiero un terrorista pues yo que sé quiero un terrorista rubio con ojos azules pues evidentemente te va a sacar un terrorista rubio con ojos azules entonces también esto es muy fácil mitigable cualquiera cualquier persona lo puede evitar pero bueno esto simplemente es lo que saca el modelo de Dall-E en este caso con palabras o con conceptos muy muy muy generales hemos hablado mucho de Dall-E que es de pago hemos hablado también de los modelos de google que no son accesibles para el público en general y ahora lo que vamos a ver son modelos que son gratis o también otros modelos que son más baratos en algunos de ellos veremos que que son que tiene una prueba gratuita pero que luego habrá que que si se quiere generar más pues pues también el habrá que pagar un poco después otros son no dejan prueba gratuita y otros son 100% gratuitos de estos vamos a ver un poco el estilo de imágenes que generan los pros y contras de cada uno y un poco pues también para que si en algún momento alguien quiere hacer algún tipo de proyecto sobre esto pues también saber qué puede esperar de ejecutar un mismo texto en varios de estos sistemas el primero mencionar otra vez más Dall-E, Dall-E no tiene prueba gratuita pero pero digamos que puedes generar 115 imágenes por 15 dólares y permite hacer una gran variedad de de tiene una gran variedad de herramientas dentro de su de su página web digamos que la este ejemplo de aquí sería la de simplemente poner texto decir pues en este caso yo que sé un par de ositos de peluche químicos en óleo y digamos que te generaría una unas 4 o 5 imágenes pero también le podemos dar una imagen por aquí como por ejemplo la de vermeer este la de la mujer con él con el pendiente y te genera variaciones de la imagen que le le vayamos a dar esto se puede utilizar posiblemente para logos para retratos para otro tipo de arte para que te genere por ejemplo piezas similares a la que tú quieres o fotografías por ejemplo de un diseño de una habitación y todo eso y no sólo eso sino que también sobre la misma imagen otra herramienta que tiene Dall-E es que podemos estirar la imagen de forma coherente entonces al estirar la imagen esta es el cuadro original vale la pieza que está aquí en también en el recuadro es capaz de extender la imagen en todas las direcciones de forma arbitraria o sea no tiene final simplemente el número de dinero que os queréis gastar en la que digamos que se puede ir generando parches por parche viendo hacia dónde va la imaginación para decirlo entre muchas comillas del Dall-E entonces copiará el estilo copiará que sea una transición buena entre entre la imagen que le vayáis a dar y el resto la imagen que tenga sentido y todo eso lo dicho es una herramienta muy potente la calidad de imágenes que genera es muy buena pero es de pago y el modelo no es libre para que digamos que nosotros no podemos acceder al modelo y ver a ver qué pasa dentro del modelo y tan simplemente podemos acceder a las salidas lo cual evidentemente para la mayoría de personas es más que suficiente pero a la hora de crear en la comunidad por ejemplo no hay ninguna comunidad de esto otro otro modelo disponible es mi journey que es también muy famoso creo que es el bueno en mi opinión es el segundo y por lo que he visto el segundo más famoso después de Dall-E en este caso se utiliza solo por discord en el que tú es un bot de discord en el que bueno también es un canal de disco en el que tú te puedes meter y puedes pedirle que genere imágenes y en la forma gratuita de forma gratuita creo que se pueden generar unas cuantas unas 20 o 30 no recuerdo el número exacto y a partir de ahí son 10 dólares por 200 imágenes bastante más barato que Dall-E y al final pues por imagen son unas unas cuantas fracciones de centímetros por lo tanto pues en verdad por imagen sabiendo lo que costaría de normal con un artista y todo eso pues es bueno es interesante para la gente que lo necesite esto la diferencia con Dall-E es que ha sido entrenado con con artistas famosos de todo tipo tanto artistas clásicos como artistas actuales y artistas sobre todo muy famosos de arte digital y todo eso por eso también tiene una tendencia muy clara a sacar siempre una salida muy artística mientras que Dall-E tiende a sacar algo realista y por ejemplo el cuadro bueno el cuadro la pieza de la izquierda es un poco la una exageración de lo que saca de normal Dall-E mid journey que son como muchas fractales pero muy artistas muy artístico muy este estilo no sé pintura de cristal de este no sé muy bien cómo describirlo pero pero siempre tiende como muy a la fantasía y se le da muy bien las piezas fantásticas mientras que también es capaz como se ve en la pieza de la derecha de generar pues algo que es muy imaginativo por así decirlo y no existe en la vida real y pero de forma más o menos realista mid journey también dentro de la aplicación de discord dentro dentro del disco se le puede pedir que digamos que una vez que genera cuatro imágenes parecidas a lo que tú le dices o sea bueno cuatro imágenes a partir del texto que tú le dices puedes elegir una de ellas por ejemplo y aplicarle un algoritmo de súper resolución por eso es capaz de sacar este tipo de imágenes aparte de que Dall-E sólo puede generar imágenes de 1024 x 1024 píxeles y esto puede generar imágenes de una resolución arbitraria y luego además si se quiere más resolución se le puede aplicar algoritmo de súper resolución varias veces hasta poderlo creo que era multiplicar por 8 la resolución y tener una imagen creo que en 2k se puede llegar a tener luego a partir de ello hay otras otros algoritmos de y otras páginas web que escalan la imagen mucho más entonces por tanto pues digamos que se puede llegar a tener incluso una calidad suficiente como para hacer un póster o cualquier cosa que es ni te pucha calidad por último vamos a ver stable diffusion que es un modelo que es 100% gracis y además está públicamente accesible bueno se me olvidó comentar que mid journey no es público tampoco el acceso al modelo en este caso sí en este caso nos dan el modelo los pesos del modelo el código suficiente como para ejecutar el modelo y ya está nos dicen que que hagamos lo que queramos con eso no está capado de ninguna forma puede generar cualquier tipo de imágenes que le digamos no no tiene ningún filtro ya que lo estamos ejecutando desde local otra cosa es que elijamos ejecutarlo desde utilizando alguna api o utilizando hugging face o lo que sea que en ese caso sí que va a estar capado pero si nosotros somos los que nos descargamos los pesos no tiene ningún tipo de filtro de ninguna forma está entrenado con artistas modernos e imágenes de fotografía de imágenes normales y yo por la experiencia que tengo es muy bueno generando sobre todo piezas realistas de sobre todo imágenes de personas es como ya estáis viendo en las imágenes estas son increíbles y cuesta mucho saber que no han sido hechas por inteligencia artificial de hecho en la mayoría bueno en sobre todo en la de la izquierda a ser que se te diga es muy complicado y luego también pues conceptos como por ejemplo el del medio es evidentemente no es un retrato por así decirlo es un receto imaginativo y la verdad es que también lo defiende muy bien este es el que vamos a usar luego en la práctica entonces tampoco voy a entrar mucho en detalle de cómo funciona evidentemente se llama stable difusión es porque utiliza el modelo o la técnica de difusión que hemos comentado anteriormente que también utilizaba Dall-E y luego lo veremos generaremos unas cuantas imágenes utilizando este modelo de aquí y ya que este botifusión es gratis y está disponible el código lo que ha hecho la gente es crear una comunidad alrededor de esta este algoritmo o esta red entonces por lo tanto la gente friki como como como puede ser cualquier programador y muy interesado en en la inteligencia artificial y que sepa de esto pues coge ese modelo y por ejemplo ya que lo puede utilizar gratis desde donde yo quiera como yo quiera pues lo que voy a hacer es por ejemplo coger el todas las hacer como una especie de red social como es léxica es una red social de gente que le gusta mucho el hacer ingeniería de texto para ver qué tipo de imágenes se puede llegar a generar y en esta red social digamos que la gente sube textos y las imágenes que han llegado a generar con ese texto entonces la cosa es que yo puedo ir ahí y coger una imagen que me gusta muchísimo y decir en plan de a ver qué texto han utilizado para esto y me lo quedo pero esta imagen también me gusta entonces por ejemplo poco que haya ese texto y combinarlo de alguna forma con el que tengo llegar a uno nuevo genera una cosa súper chula y también subirla a esta red social para que otra persona lo pueda usar o también yo personalmente lo he usado para si tengo muy claro el estilo que quiero para una generación de una imagen pero pero por ejemplo no sé muy bien qué ponerle o no sé exactamente a qué artista se puede atribuir este tipo de estilo o no conozco exactamente qué ingeniería hay que aplicar para para escribir el texto exacto para la que salga algo que yo quiera algo muy decente o de muy buena calidad entonces pues me voy a esta red social y como el que pues se mete en instagram estás un rato dando scrolleando buscando exactamente qué es lo que es lo que más se parece a lo que tú quieres generar y digamos que también ayuda mucho pues a inspiración a coger ideas y todo eso luego está dream studio que creo que es la más famosa de todas que es la de la que nos permite utilizar este modelo de forma online y no sólo es la generación de imágenes a partir de texto pero también te permite hacer la edición de imagen dream studio si no recuerdo mal tiene una aplicación para poder utilizar el modelo de forma local es muy recomendable esto usarlo si se tiene una gpu muy potente sino pues también pues evidentemente no o no se va a poder generar imágenes o va a tardar muchísimo tiempo en generar cualquier imagen pero digamos que lo bueno de utilizarlo de utilizarlo en local es que no tenemos el filtro que sí que puede tener dream estudio en su en su en su forma online de esto creo que la aplicación para ser descargada si se quiere usar en local creo que se llama art room y creo que dream estudio no es la compañía es la página web pero la compañía si no me acuerdo se llama stability ai y eso entonces digamos que si se quiere utilizar online sí porque por lo que sea no me apetece descargarme un modelo y usarlo en local porque ya sea porque no tengo tiempo porque no tengo un ordenador suficientemente potente pues utiliza en su modalidad online con dream studio y si por lo que sea sí que me interesa el quitarme el filtro y el que no tenga ningún tipo de límite ni el tener que depender de la conexión de internet nada de eso pues art room nos permite el descargar el modelo y utilizarlo sin ningún de esa forma plant manía por tercera una tercera creación que he visto por ahí que es que plant manía permite crear textos para generar buenas imágenes digamos que tú le escribes un texto como lo podría escribir yo o otra persona que no sabe cómo escribir buenos textos para para para este tipo de de modelos pues yo que sé yo quiero una imagen de una fotografía de un gato entonces digamos que una vez que le das esta esta idea te escribe un texto muchísimo mejor y mucho más capacitado para o que permite a una vez que luego pones en stable difusión el que la imagen que genere tenga muchísima más calidad que si no le dices solo una cosa en general porque una cosa que vemos en estos modelos sobre todo en este tipo en este tipo el difusión es que cuanta más información le des es mucho mejor en cambio es muy poca información o algo muy general las imágenes que genera son de peor calidad entonces por tanto es aplicaciones como esta son muy útiles porque porque saben exactamente qué tipo de qué tipo de texto escribirle de qué en qué formato en qué orden porque son cosas que afectan bastante a la generación final de las imágenes y bueno esta herramienta habrá más seguramente yo sí que conozco alguna más pero bueno esta es la que la que la más famosa y una que yo he usado bastante y he tenido muy buenos resultados y digamos que yo la recomiendo y es una idea al final y por último bueno photoshop sí que ha habido ya implementaciones o ideas o conceptos no no de la no no sé si del propio la prima compañía de adobe de photoshop pero pero para implementar este tipo de herramientas en photoshop que sería una pues evidentemente los que hagáis algo de ya sea arte digital o edición de fotografía de alguna forma pues el poder por ejemplo quitar cualquier cosa de la imagen utilizando este tipo de modelos o con completar una imagen si falta un trozo o por ejemplo simplemente decirle a esta persona quiero que tenga el pelo rubio ahora o esta persona quiero que tenga un traje en vez de camiseta o lo que sea y todo eso lo genere de forma automática en segundos y que quede tan realista como si hubiera sido una foto de verdad pues es algo muy interesante y que pues está a punto de venir y también un poco gracias a y aquí para finalizar esta diapositiva explicar un poco el hecho de que estas ideas han sido gracias a que pues stable diffusion ha sido lanzado al público sin ningún tipo de restricción y claro al final mi journey o Dall-E no tienen este tipo de de ventajas ya que no han lanzado el código y claro el resto de personas que queremos crear algo con esas con esos modelos pues no nos estamos no se nos permite ya que este código cerrado pero en cambio los de código abierto pues sí que al final crea una comunidad explotan evidentemente es mucho más famoso ha convertido a este modelo en algo muy famoso y muy utilizado que la gente ha ido mejorando y eso y al final es en este caso es la comunidad lo que ha hecho el modelo este muy grande y por último simplemente por comentar como curiosidad que digamos que por el auge este de generación de imagen a partir de texto también ha habido otras otras ramas por así decirlo que surgen de esta de esta nueva línea investigación o de esta línea muy avanzada de investigación que que generan otro tipo de datos sintéticos a partir de texto o sea se ha visto un poco también a partir de imágenes pues digamos que también fuera del ámbito del arte y de la creación artística pues que también ha habido pues por ejemplo generación de imágenes de rayos x para aumentar data sets de reconocimiento de tumores o de otro tipo de problemas médicos también se ha utilizado en otro tipo de generación de imágenes pero que también se puede generar otro tipo de datos con metodologías muy similares a las que hemos visto en con imagen pero por ejemplo en audio en vídeo en imágenes sensoriales de tridimensionales de un líder y todos en este caso sólo vamos a verlas porque todo también más interesantes más curiosas pero también más más famosas en audio meta en la antigua facebook creo un modelo hace muy poco hace creo que un mes que se llama audio gen que como su nombre indica es un modelo que genera audio a partir de texto esto está muy bien porque porque claro imaginabas que si por ejemplo yo soy un productor de sonido y yo quiero exactamente quiero meter un sample en alguno de cualquier audio que yo esté creando y pues va a haber una página web en un futuro no muy lejano en el que yo puedo quiero escribir yo que sé una persona sonidos de tacones de alguien caminando sobre parque y yo que sé y un bebé llorando de fondo y que te genera un audio de una longitud que tú quieras sobre eso pero no sólo eso sino que también este tipo de modelos no sé si no recuerdo si era este modelo en particular pero que le podemos dar los primeros tres segundos de audio de una canción y te la continúa hasta el infinito lo cual es una pasada y evidentemente pues a nivel de composición ya no sólo es capaz de hacer lo mismo que hemos visto con imágenes que es prolongar una imagen hasta el infinito y pues además también con audio y evidentemente una combinación de ambas decir en plan vale pues simplemente contexto decirle quiero que me genere es una canción de cuatro minutos y medio que sea folk mezclado con electrónica y que él es en el estribillo entre un piano y que pues en un futuro esto no sé si existe ahora creo que no pero vamos no es nada no es ninguna tontería y es algo que vamos a ver si no el año que viene en el siguiente pero me extrañaría mucho que no lo veamos ya a principios del año que viene bueno por contexto esto se está grabando en en diciembre del 2022 entre otros en la gente que lo esté escuchando en el futuro pues ya me dirá si tengo razón y por último vídeo como también como curiosidad google creo fenaki que es un modelo de generación de vídeo a partir del texto lo cual ya es muy fuerte porque al principio crearon uno que podía generar unos gifts de una duración determinada tú lo escribías por ejemplo pues que sea un oso panda volando en el cielo y te creaba un gift que de unos creo que eran unos tres segundos o cinco segundos de eso pero luego crearon uno que se llama que es este el fenaki que lo que podía seguir un guión de que el texto fuera cambiando y digamos que el vídeo iba cambiando de y siguiendo ese guión de forma adaptativa todos los frames tenían sentido entre ellos y bueno yo quería entrar para enseñaros luego aquí luego podéis estar obviamente toqueteando todo lo que queráis pero un poco por encima pues podéis ver que el texto va cambiando en este ejemplo de aquí por ejemplo te dice un oso de panda fotorrealista que está para un oso panda un osito de peluche que está nadando en el océano en san francisco dice que luego se va debajo del agua luego que ve peces de colores y de repente que es un panda el que está debajo del agua entonces podemos ver cómo está por encima se sumerge aparece un pececillo por ahí y de repente se convierte en un panda pero podéis ver que la transición es perfecta o sea es extremadamente bueno lo mismo con otros como puede seguir en plan de que primero esté buceando en el océano que luego pase a la tierra a andar por la playa y luego que aparezca por último un campfire una hoguera mientras que la cámara se aleja de él hace un plano alejado y cómo sigue exactamente el guión y con esto también han por ejemplo han creado un vídeo de dos minutos que es el vídeo más largo de la historia y siguiendo todo este guión de aquí que podría ser por ejemplo la introducción a una película las imágenes como podéis ver no son extremadamente buenas así que se ven muchos efectos por ahí pero bueno yendo un poco a la diapositiva de las primeras que hemos visto ya habéis visto que Dall-E desde la primera versión hasta hasta la actual tardó un año menos de un año entonces por lo tanto como pues no tampoco me parece una locura pensar que todo esto dentro de menos de un año podremos darle el guión de una película y que pueda generar una película entera en hd o incluso en 2k en el que pues eso digamos que sólo falte poner la voz de los actores o la música y ya está esto es un poco las cosas curiosas y ahora sí si queréis nos vemos en él en el collab y ejecutamos unas cuantas ejemplos para poder ver las imágenes que podemos llegar a generar utilizando este google diffusion hola de nuevo aquí estamos en el collab aseguraos cuando entréis en el google collab que cogéis una una una sesión con gpu vale yo ya me he conectado de antemano lo de la gpu es importante ya que no es limitante pero sí que nos va a dejar nos va a permitir el el digamos el generar imágenes más rápidas y de mejor calidad que siempre vende utilizando la cpu una vez que nos instalemos las librerías vamos a loguearnos con nuestra cuenta de hugging face vale esto es importante porque vamos a utilizar la librería de stable diffusion de diffuser que es una que va a cargar un modelo de hugging face entonces por lo tanto vamos a tener que tener acceso a la API de hugging face que es súper fácil simplemente os tenéis que loguear ahora veremos cómo hacerlo vale simplemente vamos a ejecutar de momento las celdas de en orden y aquí nos va a pedir que escribamos el token vale cuando va a ser muy fácil para crear el token yo lo voy a tener aquí yo tengo mi cuenta de hugging face hecha simplemente clicando una vez que tengáis la cuenta hecha clicando en el link este que se aparece os va a salir directamente la la clave de la del access token yo simplemente con copiarlo lo tenéis aquí le dais a login y dicen que login sucesful vale y una vez creado el login sucesful vamos a cargar el modelo directamente desde hugging face aquí muy importante ver o una curiosidad es ver que estamos utilizando el modelo de fp16 o de float 16 normalmente el modelo es de float, float es un tipo de variable en python que representa números con decimales y en este caso pues utilizamos una precisión de 16 bits lo normal es utilizar el modelo de digamos que el modelo de base es de 32 bits pero no para quitarle un poquito de bueno quitarle bastante carga de computación a google quitando muy poca un poquito de precisión a la generación de imágenes pues nos va a permitir generar imágenes muy buenas con requisitos de software un poco más ligeros vamos a cargar la el modelo va a tardar un ratillo mientras que se cargan todos los pesos de la nube son 16 archivos vale un poco por hablaros de estos archivos pues es tanto el modelo de reconocimiento de texto el modelo de generación de imágenes y luego también hay un modelo que que es de detección de imágenes que no sean not safe for work de NSA for double esto lo que nos va a detectar son pues lo que hablábamos antes de cuando utilicemos stable diffusion a partir de otra compañía en este caso hagan face pues están obligados también a tener un filtro de digamos de cosas que generaría la gente si no le dieras ningún tipo de filtro y en este caso es un modelo que detecta si en algún caso generamos una imagen que tenga algún tipo de contenido explícito de cualquier cual de cualquier forma pues pues nos va nos va a sacar una imagen en vale disculpad que había habido un problema con la versión de la librería de difusers ya está solucionado vosotros vais a tener el repositorio perdón el collab como como toca yo tenía un pequeño fallo probad si si no si por lo que sea no os funcionara os pasa el mismo error que a mí simplemente ejecutando esta línea aquí lo que va a hacer es instalar una nueva versión de estas librerías entre es por tanto en principio el error a mí me ha desaparecido al menos lo dicho hemos cargado el modelo a partir de la librería de difusas que se conecta a hagan face introduciendo nuestras credenciales de hagan face y nos hemos descargado ya la pipeline que contiene una serie de modelos ya en el orden que toca entonces lo único que vamos a hacer es meter la pipeline en la gpu para que como hemos dicho tarde menos tiempo en realizar estos cálculos entonces por tanto esto simplemente diciéndole que la pipeline entera la pasea acuda vale que es un tipo de la gpu vamos entonces por lo tanto ahora simplemente escribiendo en un texto en el que queramos en este caso el clásico de este tipo de modelos es escribir este para empezar y simplemente lo vamos a llamar a la pipeline para que dado el texto y un número de pasos la altura y el ancho de la imagen y un número de aquí vale ahora vamos a ver lo que lo que significa que te saque la primera imagen y luego la mostraremos por imagen por perdón por pantalla lo que hace esto de aquí el número de steps es el número de cantidad de digamos de pasos que se hace para quitarle el ruido cuanto más pasos mejor calidad pero también va a tardar mucho más tiempo aunque ponerle más de ciento y pico es demasiado 50 es la verdad es que genera imágenes de muy buena calidad y tarda bastante poco entonces por tanto eso luego el tamaño 512 por 512 es un buen tamaño para una imagen y luego el guidance scale es la digamos cuánto quieres que se parezca la imagen al texto que tú le has dado si es muy alto por ejemplo 14 o 15 te va a generar imágenes muy raras si es si se pone alto tipo 12 va a tener menos capacidad de imaginación por así decirlo y evidentemente también va a pasar lo mismo por el otro lado pero al revés y digamos que 7 con 5 es la verdad es un buen número de decirle escríbeme lo que te he dicho pero también con un pelín de decir en plan de también imagínate el resto de cosas si le damos por ejemplo para esta imagen vamos a ver que va a tardar 50 que es el número de pasos que le hemos dicho va bastante rápido dentro de la generación como ya hemos visto como tiene que quitarle el ruido de forma paulatina es lo que tiene que tiene que ir poco a poco en este caso bueno bueno aparece el astronauta y el caballo pero no ha sido una buena vamos a intentarlo otra vez así genera una imagen de con un poquito más de sentido de esto como como hemos dicho cada vez que lo ejecutamos lo que hace es coger ruido aleatorio pues cada generación va a ser completamente diferente está es mucho mejor y luego a partir de ahora pues si se quiere por ejemplo decir en el estilo de moned vamos a ver si consigue por ejemplo coger el estilo de moned bueno es mucho estilo de moned pero bueno seguramente si le damos otra vez a esto generamos un texto mejor es capaz de entenderlo esto también lo que hemos comentado también hay un buen trabajo de saber exactamente qué tipo de texto preguntar esta imagen bueno es un poquito van gogh pero bueno no está mal pero eso que es muy importante saber también cómo escribirle este texto y para que genere exactamente lo que queramos pero bueno esto simplemente es una pequeña práctica un pequeña prueba una explicación de cómo se puede utilizar de forma gratuita completamente gratuita este tipo de modelos y a partir de ahí hacer cualquier proyecto que os apetezca cualquier proyecto la final la imaginación también en este caso ahora sí que no hay ninguna excusa que la imaginación es el límite ya que esto pues más fácil de implementar y más chulo no no puede ser bueno muchas gracias por asistir a esta sesión y nos vemos en otra.