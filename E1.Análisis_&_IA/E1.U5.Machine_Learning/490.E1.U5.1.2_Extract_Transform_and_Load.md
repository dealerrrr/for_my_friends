---
title: ETL | Extract, Transform and Load
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/texts/41479121-u5-1-2-extract-transform-and-load
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U5 #Machine Learning #ETL #Extract, Transform and Load
lang: es-AR
---
# Extract, Transform and Load (ETL)
Es, quizás, el proceso más pesado y largo para un científico de datos; los Algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial.

Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente en unas condiciones diferentes. Por tanto, nuestra primera misión, como científicos de datos, es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. 
- El principal problema que solemos encontrar son los **valores nulos,** situación bastante común, ya que hay diferentes variables (columnas). Los valores nulos, muchas veces, vienen marcados por un NAN o un simple interrogante.
- **Lo primero que tenemos que hacer siempre es visualizar los datos,** porque si no visualizamos los datos, nunca vamos a entender la historia.
- Cuando dibujamos las gráficas de las diferentes variables vamos a observar que distribución tienen los datos. Vamos a observar también, cuántos valores nulos o diferentes incongruencias tenemos. 
- **Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo** y, simplemente, implica ir comprobando, columna a columna, el tipo de datos que tenemos para ver qué transformación le tenemos que aplicar para trabajar de forma correcta con nuestro algoritmo.

## Privacidad
Para que podamos trabajar de forma correcta con nuestros algoritmos, la Privacidad es un tema bastante recurrente.

En el campo del machine learning, los datos se tratan siempre de forma anonimizada, es decir, para nosotros no tiene valor el nombre de una persona; se trata de trabajar con ID's.

Pero es cierto que si necesitamos practicar y entrar en este mundo tenemos repositorios como Google Dataset, donde podemos descargar Dataset de todo tipo de casos de uso. 

Esto nos será muy útil para poder entrenar a nuestro Algoritmo y ampliar nuestro conocimiento y destreza.

## El proceso de desarrollo de un algoritmo. 
### 1 | Data Collection 
Esta parte es muy importante, pues, normalmente, quien mejor captura los datos es quien mejores algoritmos desarrolla.

Podemos encontrar muchos escenarios para realizar esta descarga, como hacerlo desde una API o, incluso, buscarnos unos datos, por un lado, y otros por otro para después fusionarlos. 

Aquí nos podemos encontrar en múltiples escenarios.

### 2 | Business Analysis
Para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema.

Ejemplo:
Nos piden que desarrollemos un algoritmo predictivo de ventas.
1. Briefing con la empresa, expectativas y casos de uso para comprender todos los Script Holders y variables que afectan al negocio para elaborar una primera hipótesis.
2. A través de esa hipótesis averiguaremos qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. 

Esta parte es fundamental. De hecho, es una parte en la cual muchos científicos de datos se pueden perder por pensar demasiado en la programación. 

Hay dos partes muy bien diferenciadas
- Parte 1: Táctica, estrategia y lógica de entendimiento de negocio. 
- Parte 2: Programación y Desarrollo.

### 3 | Programación y desarrollo
Una vez hemos hecho esta primera parte de Business Analysis de nuestro cliente, pasaríamos a analizar los datos que hemos capturado con la Data Collection.

Seguimos con el caso de la empresa.

Ahora analizaremos:
- Si los datos son internos o externos.
- Si los hemos descargado de una API o desde una base de Datos. 
- Si no nos lo van a proporcionar de forma aislada en un CSV.

Una vez hemos coleccionado estos datos, pasamos a preparar los ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, a limpiarlos y evaluamos la calidad del dato. Porque muchas veces nos va a pasar que a lo mejor partimos de 10.000 filas y cuando hacemos el proceso de ETL nos podemos quedar en 4000.

Esto es muy común cuando la calidad del dato es anómala, por lo que tenemos que pensar que el algoritmo trabaja de una forma muy matemática y muy fina, por tanto, si nuestra "gasolina" encuentra algún error, lo va a reproducir en el resultado. Esta parte es crucial y nos puede llevar muchísimo tiempo.

### 4 | Escoger un modelo
Como decíamos, esto funciona como cajas de herramientas y, a medida que vamos avanzando en conocimiento, de un golpe de vista, ya podemos saber qué tipo de modelo puede encajar en nuestro problema. 

Una vez hecho esto, pasamos a entrenar el modelo. Luego explicaremos que significa un entrenamiento de un algoritmo.

### 5 | Evaluación final
Nuestro modelo coge unos datos, aprende unos históricos, realiza unas predicciones y contrastamos los resultados con datos de test, y lo haremos así porque no vamos a utilizar todos los datos.

Nosotros vamos a lanzar predicciones que vamos a contrastar con la realidad. 

También es común iterar con el Parámetro Tuning.

Imaginemos que hemos llegado a una predicción de una precisión del 80%. pero no es suficiente para nuestro caso de uso. Este valor puede ser bueno para una predicción de ventas.

Pero en el caso de predecir una posible enfermedad, no podemos errar un 20%, por lo que estableceremos nuestros umbrales e iteraremos, pasando al Parámetro Tuning, que no es otra cosa que optimizar los parámetros del algoritmo para mejorar la precisión.

### 6 | Deploy o Puesta en Producción
Imaginemos que somos un banco y queremos saber a qué potenciales clientes les podemos ofrecer un crédito.

¿En qué se basan?: En el historial de ingresos, gastos, impagos, cobros, etc. Imaginad si tenemos que hacer una revisión manual de una BBDD de millones de personas, fila por fila, ID por ID… Sería una labor muy tediosa.

¿Cómo se entrenaría un algoritmo de machine learning para hacer esta predicción?

Descargaremos un Dataset con un histórico de 10 años con unas características y al lado tendríamos una etiqueta supervisada, es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no, si se le debe dar un préstamo o no, ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo.

A partir de aquí, podemos crear un Algoritmo Predictivo de forma que, cuando entre un usuario nuevo y este algoritmo esté en funcionamiento, hará una predicción si a este usuario se le puede o no ofrecer un préstamo.

¿Por qué? Basándonos en su historial.  

Aquí estamos hablando de que en él dataset contenía una columna, donde si teníamos el histórico de un "sí" o un "no", de impago o de pago.

**Esto es supervisado, significa que tenemos una etiqueta donde colocamos el resultado de antemano en el pasado. Vamos a profundizar sobre esto en la siguiente sección.**