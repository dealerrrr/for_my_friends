---
title: Machine Learning | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866144-u5-1-1-machine-learning-jose-peris
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U5 #Machine Learning #José Peris
lang: es-AR
---
# Machine Learning
![[487.E1_Machine_Learning.mp4]]
[Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866144-u5-1-1-machine-learning-jose-peris)

Big Data es la gasolina de nuestros algoritmos. Cuando hablamos de Big Data, tenemos que hablar de las 4 Vs. ¿Y por qué 4 Vs? Volumen, variedad, velocidad y veracidad. Muy a menudo en los medios, sobre todo escritos, vemos esta palabra también utilizada de forma errónea en lo que al Machine Learning se refiere. Por ejemplo, si estamos viendo una publicación sobre las el Big Data del clásico, de un partido de fútbol de Madrid contra Barça. ¿Qué ocurre? Que aquí tenemos un histórico, hay una estadística de muchos años de enfrentamientos de muchos jugadores y muchos eventos y acciones que han pasado. Estábamos comentando que los datos cuentan historias. Cuando se plantean este contexto, el concepto de Big Data, estamos hablando de visualización de datos, de KPIs, de Key Points Indicators. Realmente es una representación visual de datos. de estos indicadores. Pero cuando hablamos de Big Data en Machine Learning, estamos hablando de nuestra gasolina. Estamos hablando de poner los datos a trabajar. ¿Qué significa volumen? Volumen significa que necesitamos cantidades inmensas de datos. Para que os hagáis una regla muy básica, a más datos, más precisión o más accuracy, que es nuestro objetivo. Pero también necesitamos variedad. ¿Por qué? Porque necesitamos datos heterogéneos. Es mucho mejor tener datos heterogéneos que homogéneos. ¿Por qué? Porque necesitamos que nuestro algoritmo vea muchísimas casuísticas. Es como el ejemplo del padre que le enseña al niño lo que es un vaso. Es mejor que vea mil modelos de vasos que vea diez modelos de vasos. ¿Por qué? Porque cuando realmente salga a la calle y tenga que detectar lo que es un vaso, a más ejemplos vistos, más capacidad de reconocimiento. ¿Qué es la velocidad? Bueno, cuando hablamos de velocidad... básicamente nos referimos a la ingesta de los datos. En este caso haría referencia a la capacidad también que tenemos hoy de trabajar con datos en streaming. También añadir aquí que es importante el saber que los algoritmos se entrenan pero se pueden constantemente ir reentrenando. A más datos que entran se van reentrenando y van mejorando su precisión. Ya que no referimos con veracidad pues bueno estamos hablando de la calidad del dato. Estamos hablando de que estos datos hay que procesarse, hay que transformarse, lo primero que hacemos siempre es visualizarlos para ver que no hayan incongruencias. ¿Por qué? Porque pueden haber pasado eventos específicos en una fecha referente a cualquier caso de uso que realmente estemos hablando de un caso aislado. Y esto puede realizar una distorsión en el planteamiento de nuestro algoritmo. Por tanto, simplemente hace referencia. al proceso de ETL. Para comprender mejor este ejemplo quiero que hagamos el caso de Teachable Machine de Google donde vais a poder comprobar vosotros en casa qué son las cuatro V's. Para ello lo que me gustaría es que presionaseis en el enlace que tenéis aquí en la plataforma y que entrenéis un detector de objetos. Aquí lo que veréis es que vamos a entrenar un algoritmo de visión artificial. Lo que va a hacer es clasificarnos diversas clases las que nosotros determinemos. Podéis coger cualquier objeto que tengáis en casa, asignarlo a una clase, podéis coger una botella, podéis coger un lápiz y simplemente lo que tenemos que hacer es configurarlo y al presionar en la cámara lo que veréis es que os genera en la videocámara, la webcam os genera una cantidad infinita de imágenes. Lo que tenéis que hacer es mover el objeto en muchas posiciones. para comprobar el efecto de variedad. Seguidamente, simplemente tenéis que seguir los pasos, entrenar el modelo y comprobar su performance. Siguiendo con el concepto de Big Data, la traducción que tiene de una forma física o de una forma más tangible sería el dataset. ¿Qué es un dataset? Un dataset es un conjunto de datos. Para que os hagáis una idea, a día de hoy el Big Data puede llegarnos en forma de dataset de diversas formas. Una muy simple y común sería un archivo.txt que simplemente deberemos transformarlo para llevarlo a nuestro terreno. Pero el tipo de archivo dominante es el CSV. También podemos tener ficheros Excel o incluso archivos PDF. Cabe destacar también que el sonido también son datos, aunque debo remarcar aquí que el sonido como tal no existe para el Machine Learning, se convierte en un espectrograma, igual pasa a ser una imagen. Y podemos tener también vídeos e imágenes. ¿Qué es un dataset? Un dataset es nuestro conjunto de datos. Cuando estamos hablando de dataset, vamos a diferenciar entre dos grandes grupos como serían los datos tabulares, el primer grupo que es el más común o cuando estamos hablando del tema de visión artificial que serían imágenes o vídeo. Vamos a centrarnos en el primer grupo de los datos tabulares y vamos a ver al final lo que vamos a ver de una forma muy simple, es una tabla, imaginaos una tabla de Excel con filas y con columnas. Normalmente, hablamos de un dataset de calidad, por ejemplo, una buena cifra sería tener, por ejemplo, 5.000 o 10.000 filas hacia adelante y de 8 a 10 columnas, que estas serían nuestras variables. Ahí estaría nuestro target, nuestro objetivo a predecir. Es cierto que se pueden llegar a hacer pruebas de concepto con 1.000 filas, utilizando Machine Learning, pero no es lo recomendable. Si queremos hablar de buen tamaño, pues estaremos hablando de 100,000 filas para garantizar una buena calidad. Y esto nos permite utilizar todo tipo de algoritmos, tanto machine learning como deep learning. Cuando abrimos un data set de datos tabulares, nos podemos encontrar diferentes tipos de datos. Por un lado, tenemos los datos double, que serían simplemente números con fracción, por ejemplo, 1.7854. o nos podemos encontrar tipos de data como íntegra, que en este caso no habrían partes fraccionales, por ejemplo un 7. También tenemos las strings que hacen referencia a categoría, por ejemplo imaginemos que estamos hablando de una variable donde simplemente añadimos si un paciente padece algún tipo de enfermedad y tenemos cinco opciones, por ejemplo, una de ellas es diabetes. Esto sería... una string, estamos hablando de una categoría, no estamos hablando de números, estamos hablando de palabras, por decirlo de alguna forma simple. Por otro lado también podemos encontrar el timestamp, como su nombre indica es simplemente una fecha, 24 del 7, 1997. Simplemente tenemos que tener en cuenta que este tipo de... hay que también enseñarle a nuestro sistema que al ser marcado como timestamp le estamos diciendo que no es un número corriente. sino que está haciendo una relación a un evento temporal sucesivo. O por ejemplo podemos tener un diccionario. Un caso muy común del diccionario sería cuando vamos al supermercado y hacemos nuestra bolsa de la compra, ahí tenemos un diccionario de artículos, de palabras, con los objetos, con los ítems que hemos comprado. Otro tipo de datos serían imágenes. JPEG o JPEG o simplemente documentos donde encontraríamos por ejemplo, pues, reviews, por ejemplo, documentos de texto donde encontraríamos, pues, Great Food, Excellent Service. Esto sería un poco el abanico de datos, de tipología de datos que podemos encontrar. Nuestra primera misión es, en el momento que abrimos un dataset, es identificar los diferentes tipos y dependiendo del objetivo que queramos veremos qué tratamiento hay que aplicar a cada tipo de datos. ¿Qué es ETL? ETL en inglés significa Struct, Transform and Load. La ETL quizás es el proceso más pesado y más largo para un científico de datos porque como os decía estábamos hablando que los algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial. Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente, en unas condiciones diferentes. Por tanto, nuestra primera misión como científico de datos es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. Normalmente, el principal problema que solemos encontrar y el que primero vamos a buscar es si hay valores nulos. Esto es bastante común, que haya diferentes variables, recordad que las variables son las columnas donde encontremos valores nulos. y muchas veces vienen marcados por NAN o un simple interrogante. Lo primero que tenemos que hacer siempre es visualizar los datos, porque si no visualizamos los datos nunca vamos a entender la historia. Cuando pintamos las gráficas de las diferentes variables, vamos a observar qué distribución tienen los datos, vamos a también a observar cuántos valores nulos tenemos y vamos a observar diferentes incongruencias. Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo y simplemente implica ir comprobando columna a columna el tipo de dato que tenemos y ver qué transformación le tenemos que aplicar. para que podamos trabajar de forma correcta con nuestros algoritmos. También comentaros que a día de hoy, cuando empezamos en este mundo, ya sabéis que el tema de los datos y la privacidad es un tema bastante recurrente. Lo primero que quiero comentar es que en el campo de Machine Learning los datos se tratan siempre de forma anonimizada, es decir, para nosotros no tiene valor el nombre de una persona, se trabaja con IDs. Pero, ¿es cierto? que si queremos practicar y entrar en este mundo, tenemos repositorios como Kaggle o como Google Dataset, donde podemos descargar datasets de todo tipo de casos de uso. Esto nos será muy útil para poder entrenar nuestros algoritmos y ampliar nuestro conocimiento y nuestra destreza. El desarrollo de un algoritmo sigue unos pasos pautados. que vamos a narrar a continuación. El primer paso se llama Data Collection. Aquí se trata simplemente de descargar los datos, de capturarlos. Esta parte es muy importante. Normalmente el que mejor captura el dato es el que mejor algoritmos desarrolla. Por tanto, la primera parte va a ser siempre descargarlo. Nos podemos encontrar en muchos escenarios. Muchas veces, pues a lo mejor toca descargarlos desde una API. O sucede que a lo mejor tenemos que incluso nosotros buscarnos unos datos por un lado y otros por otro y después fusionarlos. Aquí nos podemos encontrar en múltiples escenarios. Vamos a hablar del proceso de desarrollo de un algoritmo. El primer punto indispensable es el business analysis, es decir, para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema, es decir, imaginaros que viene una empresa a pedirnos que desarrollemos un algoritmo predictivo de ventas, por ejemplo. El primer punto sería hacer una entrevista con la empresa para obtener un briefing y comprender el caso de uso a la que se trata. perfección. Debemos entender todos los skate holders, todas las variables que están afectando, cómo funciona ese negocio y hacer de forma intuitiva una primera hipótesis de qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. Esta parte es fundamental. De hecho es una parte en la cual muchos científicos de datos quizás se pierden demasiado en la programación. Aquí una parte que es de táctica y estrategia. de lógica, de entendimiento de negocio y otra parte que es de programación y desarrollo. Una vez hemos hecho esta primera parte de Business Analysis, pasaríamos al Data Collection o la captura del dato. Aquí simplemente analizaremos, seguimos con el caso de esta empresa, si los datos son internos, son externos, si nos vamos a descargar de una API, desde una base de datos o si nos lo van a proporcionar de forma aislada en un CSV. Una vez hemos coleccionado estos datos, los hemos capturado, pasaríamos a prepararlos, a Data Preparation o también ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, pasamos a limpiarnos y evaluamos la calidad del dato, porque muchas veces nos va a pasar que a lo mejor tenemos 10.000 filas, pero cuando hacemos el proceso de ETL nos podemos quedar en 4.000 filas. Esto es muy común, ¿por qué? porque la calidad del dato es anómala, con lo cual hay que pensar que el algoritmo trabaja de una forma muy matemática y muy fina, por tanto si nuestra gasolina contiene algún error la va a reproducir en el resultado, por tanto esta parte es crucial y nos puede llevar muchísimo tiempo. El siguiente paso sería escoger un modelo, como os decía estos son cajas de herramientas y a medida que vamos avanzando en conocimiento de un golpe de vista ya prácticamente sabemos qué tipo de modelo puede encajar en nuestro en nuestro problema, de ahí pasaríamos a entrenar el modelo, luego explicaremos qué significa un entrenamiento de un algoritmo y después pasaríamos a la evaluación, la evaluación al final, esto se trata básicamente de que nuestro modelo coge unos datos, aprende unos datos históricos, hace unas predicciones y contrastamos los resultados. ¿Cómo los contrastamos? Con datos de test. ¿Por qué? Porque no utilizamos todos los datos. Nosotros vamos a lanzar predicciones que las vamos a contrastar con la realidad. Esto es la evaluación. Aquí normalmente también lo que se hace es iterar con el parameter tuning, imaginaros que hemos llegado a una curacy del 80% y no es suficiente para nuestro caso de uso, porque un 80% puede ser muy positivo para ciertos casos de uso, imaginaros para una predicción de ventas pues ya es un valor interesante, pero si estamos prediciendo si una persona tiene cáncer o no, no podemos fallar un 20%. es donde aquí estableceremos nuestros umbrales, si el 80% es suficiente iteraremos y pasaremos al parámetro tuning donde aquí lo que vamos a hacer es optimizar los parámetros del algoritmo para aumentar la precisión y finalmente la última parte serían las predicciones o el deploy o puesta en producción. Imaginemos que somos un banco y esto es un caso real, porque a día de hoy sucede. Un banco quiere saber a cuántos clientes les quiere dar un crédito o les puede dar un crédito. ¿En qué se basa un banco para darte un crédito? Pues va a estudiar todo tu historial de ingresos, gastos, de impagos, de cobros, etc. Imaginaros una base de datos de millones de personas. Si tenemos que hacer un estudio manual, fila a fila, ID a ID, esto sería una labor muy tediosa. ¿Cómo se entrenaría? un algoritmo de machine learning para hacer una predicción de si a esta persona debemos darle el crédito o no. En este caso lo que haríamos sería ir a la base de datos, descargaríamos un dataset donde tendríamos un histórico de 10 años atrás de aquellos usuarios con unas características y al lado tendríamos una etiqueta supervisada, es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no. Es decir, si se le debe dar un préstamo o no. ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo. Entonces, a partir de aquí, podríamos hacer un algoritmo predictivo de forma que en el momento que entre un usuario nuevo y el algoritmo esté funcionando haría una predicción si a este usuario hay que darle crédito o no. ¿Por qué? Porque en base a su historial, de alguna forma. Entonces, aquí estamos hablando de que en el dataset contenía una columna donde si teníamos el histórico de un sí o un no, o de impago o de pago. Esto es supervisado. Supervisado significa que tenemos etiqueta, que conocemos el resultado de antemano, en el pasado. Cuando hablamos de lenguaje de aprendizaje no supervisado, estamos hablando de que los datos nos vienen desordenados. O sea, imaginaos fichas de diferentes formas, ¿vale? Triángulos, cuadrados, círculos, y nos llevan todas las fichas desordenadas. Nosotros, cuando es aprendizaje no supervisado, lo que vamos a hacer es ordenarlas, clasificarlas. ordenarlas, clusterización, esto significa poner los triángulos con los triángulos, los círculos con los círculos y los cuadrados con los cuadrados. Por tanto quiero que diferenciemos aquí no supervisado datos desordenados. Está supervisado, tenemos una etiqueta, sabemos qué ha pasado, sabemos el resultado, por tanto esto nos define el tipo de algoritmos que vamos a utilizar. Normalmente, aprendizaje no supervisado, los algoritmos más comunes serían los de clustering, donde lo que se hace es, en el momento que un cliente entra a tu página web, y esto está funcionando en muchos e-commerce, a día de hoy hay un algoritmo que en base a tus características y a tu patrón de uso, como experiencia de usuario, ya te clasifican con un tipo de cliente o un tipo de buyer persona. Imaginaros que se podría incluso clasificar como un cliente arriesgado, o un cliente conservador, o un cliente dudoso. Otro ejemplo muy común del aprendizaje no supervisado son los sistemas recomendadores, algo que vemos todos los días cuando estamos haciendo una compra en Amazon o cuando estamos en Netflix. Al final siempre tenemos recomendaciones. Estamos hablando en este caso de aprendizaje no supervisado. En el caso de aprendizaje supervisado vamos a diferenciar entre dos grandes grupos, como es clasificación y regresión, con esto retomamos la idea de que los algoritmos de IA de una forma muy sencilla y muy simplificada lo que hacen es predecir, esto sería regresión, predecir un precio, clasificación, sería predecir si va a suceder algo o no y clustering que es agrupar. Y un tercer bloque. que sería un poco más técnico y de nicho, sería el Reinforcement Learning. Reinforcement Learning es una técnica que se utiliza sobre todo en el ámbito de los videojuegos. Es la típica afirmación de estoy jugando contra la máquina o la máquina me ha vencido. Al final, este tipo de aprendizaje se basa en un sistema de castigo o recompensa. Así que es cierto que está cogiendo bastante forma y se está utilizando ahora un poco más incluso en el trading. Ya existen modelos de reinforcement learning que se están utilizando para el tema del trading, porque al final por este castigo y recompensa aprende a hacer trading en short. Esto sería el concepto de los market makers, pero aún son modelos experimentales y falta por demostrar si pueden sustituir o reemplazar de forma eficiente en este campo a la programación tradicional.