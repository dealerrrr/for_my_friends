---
title: Deep Learning | Rafa López
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez
Tags/Keywords: #Deep Learning #Rafa López
lang: es-AR
---
# Deep Learning
![[494.E1_Deep_Learning.mp4]]
[Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez)

Para definir lo que es la inteligencia artificial, primero hay que describir un poco lo que es la inteligencia que nosotros conocemos, que nos asociamos a nosotros como seres humanos. A mí me gusta definir la inteligencia normal, la natural o la que nosotros poseemos, como una combinación de diferentes aptitudes, como puede ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. Y para, digamos, de alguna manera... para entender la inteligencia artificial, tenemos que saber que, a día de hoy, las técnicas que nosotros llamamos inteligencia artificial, son un subconjunto de tareas que son muy buenas en la tarea del aprendizaje, es decir, a partir del conocimiento que le aportan los datos, aprender a resolver tareas. Por tanto, la inteligencia artificial, desde un punto de vista algorítmico-tecnológico, no es una inteligencia general como la que tenemos nosotros, que es capaz de resolver todas estas tareas que he mencionado, o tiene todas estas características que he mencionado previamente, sino es una serie de algoritmos que consiguen aprender a resolver tareas a partir de datos de entrada, y generalmente una gran cantidad de estos datos. Y de hecho, estas técnicas que aprenden a partir de datos, se engloban en un campo que se llama Machine Learning. el cual es justo eso, diferentes tipos de algoritmos. de sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos de entrada. Ya pueden ser estos supervisados, es decir, con la respuesta asignada ya al problema que buscamos, o no supervisados, es decir, datos que no tienen a priori la respuesta al problema que buscamos, pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos. Y, concretamente, lo que ha permitido que la inteligencia artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas. del Machine Learning que se llaman Deep Learning. Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos. Y, por tanto, cuando, o en la mayoría de ocasiones, que estamos hablando de inteligencia artificial en las innovaciones que hemos tenido en los últimos años, estamos hablando, en realidad, de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas a partir de grandes bases de datos. que generalmente están supervisadas o etiquetadas en realidad, tanto los algoritmos de Machine Learning como los de Deep Learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos pero concretamente, los algoritmos de Deep Learning no es que sean diferentes, sino que son una subclase del Machine Learning simplemente, estas subclases están basadas en redes neuronales de las cuales hay diferentes tipos, que posteriormente veremos y este tipo de algoritmos basados en redes neuronales artificiales que al final simplemente son una... inspiración o una imitación algorítmica o una simplificación incluso algorítmica de las redes neuronales que funcionan en nuestro cerebro, aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática. Aunque ahora mismo estamos muy habituados ya a escuchar hablar de inteligencia artificial y parece un término relativamente nuevo, que empezó a popularizarse a partir del año 2012 más o menos de forma muy pronunciada, en realidad las primeras redes neuronales surgieron en la década de los 50-60 más o menos. En ese momento, no estaba la tecnología suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. Por dos motivos principales, estas técnicas requieren de alto poder de computación y además requieren de una gran cantidad de datos. Por tanto, estas dos factores, tanto la capacidad de computación como... La cantidad de datos ha crecido de forma exponencial desde los años 60, porque la digitalización se ha ido expandiendo en todos los dominios. Y, por tanto, en 2012 se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponibles era suficiente para que estas redes neuronales funcionasen. Y a partir de entonces es cuando hemos podido sacar su máximo potencial. Pero... desde su surgimiento hasta la actualidad, han habido diferentes intentos, o diferentes hitos, mejor dicho, en el campo de la Inteligencia Artificial, en el que han ido, progresivamente, superando a los seres humanos en diversas tareas. Por ejemplo, en el año 1996, Deep Blue de IBM derrotó a Garry Kasparov, al ajedrez, que era el campeón mundial en ese momento, y, por lo tanto, en esa tarea concreta, la Inteligencia Artificial ya nos lleva muchos años superando. Pero podemos ver también que han habido hitos muy recientes, como por ejemplo, en el año 2016, una inteligencia artificial, concretamente desarrollada por DeepMind, que es una compañía que fue adquirida por Google, derrotó al campeón mundial del Go, que es un juego muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, mucho más complejo que el ajedrez en términos de árboles de posibilidades. Además, en el año 2019, el algoritmo de OpenAI derrota al mejor equipo del mundo en el videojuego Dota 2, que es un videojuego que requiere de coordinación de diferentes jugadores y de mucha intuición, por lo tanto, demostró otro hito. E incluso en el 2020, otro algoritmo también de DeepMind, la compañía que fue desarrollada por Google, resolvió un problema que hasta entonces no había podido resolverse que era el predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo, va a ser extremadamente prometedor para hacer nuevos avances en el campo. Y, en general, a partir de estos hitos y esta evolución, hemos visto cómo, progresivamente, las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, sino que han transicionado a ser compañías basadas en los datos y, por lo tanto, en la inteligencia artificial. Ahora las compañías más grandes del mundo son Google, Facebook, Apple, etc., compañías puramente basadas en datos, software e inteligencia artificial. y se prevé que esta evolución exponencial del valor que la inteligencia artificial va a aportar al mercado y a los consumidores, va a seguir creciendo. Y, por tanto, este campo es de extremada relevancia su estudio para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías. El Deep Learning es un campo que ha crecido mucho en los últimos años, se han desarrollado muchas nuevas tecnologías, y cada vez es un campo que, de hecho, es incluso vertiginoso el ritmo que está cogiendo, porque cada vez se dedican más recursos y, por tanto, más novedades aparecen de forma más rápida. Por tanto, es un poco difícil mantenerse en la ola de estar a la última en todo lo que va surgiendo. Pero los tipos principales de redes normales que podemos encontrar, o los que están más establecidos, y que ya han demostrado resultados muy significativos en los últimos años, son las redes neuronales totalmente conectadas, que serían las redes neuronales originales, las que se basan en neuronas individuales que se conectan a través de distintas capas. Después tendríamos las redes neuronales recurrentes, que se desarrollaron específicamente para la evaluación o para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros. Luego tendríamos las redes neuronales convolucionales, las cuales están específicamente diseñadas para trabajar con datos que estén ordenados en dos o tres dimensiones. También tendríamos una, digamos, subtipología de entrenar redes neuronales, que serían las redes neuronales generativas adversarias. Estas redes, principalmente, convolucionales, tienen la característica de que al final sirven para generar datos de forma artificial que son muy realistas y finalmente tenemos una red neuronal que ha surgido en los últimos años, un tipo de redes neuronales que surgió en 2020 que son los transformers, este es un tipo de red neuronal que a través de la ingesta de grandes cantidades de datos y a través de desarrollos de modelos con muchos parámetros son capaces de hacer tareas como generación de texto y de imágenes con un realismo que no tiene precedentes. Por tanto, este tipo de redes son capaces de trabajar tanto con entradas de texto, con entradas de imagen o con cualquier tipo de entrada y, por lo tanto, es capaz de combinar información de ambos mundos, del mundo del lenguaje natural, es decir, el texto, y el mundo de la imagen. Pero es muy bueno trabajando con datos no estructurados. Una neurona es un núcleo de cálculo muy simple. Simplemente es una función que tiene como entrada un número o varios números y le aplica una función de activación específica. que puede ser determinada por el programador para obtener un número diferente al de entrada. Esta neurona, como mencionado, tiene una o diversas entradas. Estas entradas tienen asociada un peso. Por lo tanto, si una neurona tiene diversas entradas, como por ejemplo, si estamos hablando en un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa, como número de habitaciones, metros cuadrados, número de baños, etc. Y esta información... entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada, y cuando combina todos estos números con estos pesos, les aplica una función de activación que convierte en este número en otro, o igual puede ser el mismo, depende de la función de activación que se utilice y el número de entrada. Esto al final lo que introduce en el sistema son no linealidades que lo que permiten es adaptarse a entornos de datos muy complejos. Sabiendo ya lo que es una neurona, lo que podemos ver es cómo se crea una red neuronal. Pues es muy simple, una red neuronal, lo que es un conjunto de neuronas ordenadas por capas, es decir, en la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etc. Así hasta la capa de salida, que es la que nos va a dar la respuesta a nuestro problema, que en este ejemplo que he puesto sería el precio de la casa que queremos predecir. Por lo tanto... lo que vemos es una estructura por capas, y al final lo que vamos a conseguir con esto es que... cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto es lo que nos permite transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para conseguir o obtener, desgranar, unas relaciones de muy alto nivel, muy complejas, que quizá no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa que estemos evaluando en cuestión. Las redes nuevas recurrentes, la principal diferencia es que están generadas o diseñadas para entender o procesar la información. datos que tengan una relación temporal, es decir, por ejemplo, en una frase, las palabras, el orden de las palabras, puede cambiar el significado de una frase, por tanto, las redes neuronales recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. Por lo tanto, estas redes neuronales lo que hacen es procesar secuencialmente las palabras de entrada, pero teniendo como información también, a la hora de analizar cada una de las palabras, cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma. Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate. mejoró de forma drástica en muy poco tiempo. ¿Por qué fue? Porque se pasó de utilizar técnicas más convencionales para la traducción, a utilizar técnicas basadas en redes neuronales. Esto lo que permitió es que los resultados de traducir un texto, del español al inglés, por ejemplo, ahora sean extremadamente robustos y prácticamente... apenas se cometen incoherencias textuales. Y cada vez, según se vayan incrementando los datos y los modelos vayan siendo más grandes, este error y esta, digamos, poco similaridad con lo que diría una persona, se van a ir reduciendo paulatinamente. También tenemos, por ejemplo, el reconocimiento de voz, que no es más que la conversión de audio a texto y posterior interpretación de este texto. Así como el texto escrito, el audio también es una secuencia temporal y por lo tanto las redes neuronas recurrentes pueden utilizarse para transcribir o para convertir este audio en una cadena de texto que sea posteriormente interpretada por otra red recurrente para emitir acciones específicas en los asistentes de voz. Por otra parte tenemos las redes normales convolucionales, las cuales están específicamente diseñadas para analizar datos que están organizados en dos o tres dimensiones, como es el caso de las imágenes. Al final una imagen lo que es es una matriz bidimensional de píxeles ordenados en filas y columnas. Por las redes normales convolucionales lo que nos permite es utilizar una operación que se llama convolución, la cual nos permite aplicar diferentes filtros a las imágenes para poder analizarlas. procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etc., a características de alto nivel. Si, por ejemplo, estamos detectando un sistema de... entrenando un sistema de detección de caras, las características de más alto nivel, por ejemplo, serían los ojos, la nariz, la boca, etc. Es decir, la red neuronal aprende, convolucional concretamente, aprende a ver cómo se combinan todas estas características de bajo nivel, es decir, bordes, contornos, etc., para convertirlas en características de alto nivel, boca, ojos, etc., para seguir procesándolas y subiendo el nivel hasta obtener representaciones de lo que sería una cara. Por lo tanto, al final, las redes neuronales convolucionales están trabajando en comprender la información que tienen estas matrices bidimensionales que son las imágenes. La convolución es una operación matemática que lo que nos permite es, dado un filtro, que no es más que una matriz... generalmente de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante, es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente procesada con una información específica resaltada. Las aplicaciones que podemos ver en redes neuronales convolucionales son variadísimas, es decir, son extremadamente extensas, pero, por ejemplo, una de las más conocidas es el reconocimiento facial. es decir, para empezar, gracias a una renombre convolucional, podemos detectar las caras que hay en una imagen, es decir, dónde están esas caras dentro de la imagen, y posteriormente, hacer una identificación de estas caras con respecto a una base de datos, como por ejemplo, el reconocimiento facial en China, está en la orden del día, y por tanto, todos sus ciudadanos están altamente identificados de forma automática gracias a estos sistemas, por lo tanto, evidentemente, las aplicaciones de la identidad artificial, como cualquier otra herramienta. pueden ser positivas o negativas, depende del análisis y de las consecuencias que tengan, pero desde luego son herramientas extremadamente potentes. De hecho, volviendo al caso de China, se descubrió a un fugitivo entre 60.000 personas en un evento, gracias a este reconocimiento facial. Por tanto, esto nos hace darnos cuenta del potencial que tiene esto, de la precisión a la que estos algoritmos están llegando, ya que en un contexto muy caótico, como es un evento de 60.000 personas, la inteligencia artificial, artificial es capaz, concretamente la basada en redes nuevas convolucionales para detección de rostros, identificar a un fugitivo. Adicionalmente, también es amplísimo el uso de las redes nuevas convolucionales para el análisis y proceso de imágenes médicas, ya que nos permiten, por ejemplo, hacer segmentaciones volumétricas de imágenes en 3D como los tags o las resonancias magnéticas, para obtener volúmenes de regiones de interés, como pueden ser pulmones, hígado, cerebro, etcétera, y además utilizar también estas redes neuronales para extraer información y detectar diferentes patologías, como el COVID, nódulos, y una gran otra variedad de enfermedades pulmonares, o incluso de cualquier otra parte del cuerpo, y estas aplicaciones nos están haciendo más que crecer y crecer, es decir, cada día, cada día, cada día, cada día, cada día, cada día, cada día, o cada año son más numerosas las diferentes empresas y startups que se dedican a este sector. Evidentemente, otra aplicación muy común de las redes neuronales convolucionarias son los coches autónomos. Al final, un coche autónomo es un coche que tiene que replicar la conducción de una persona y para ello es primordial la evaluación de la información del entorno. Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras. y por tanto las cámaras producen imágenes que son el target ideal para ser analizadas con redes neuronales convolucionales que detectan desde objetos en la vía, como pueden ser coches, peatones, ciclistas, etc. a también las líneas del carril, semáforos, entre otras muchas cosas. Por lo tanto también son clave para este sector. He mencionado algunas aplicaciones de las GAN, pero también podemos ver otras como por ejemplo, la alteración de imágenes médicas, esto sería una aplicación que no sería positiva, sino de todo lo contrario, sería negativa, pero para que veamos que las redes neuronales también se pueden utilizar con fines nocivos y por lo tanto hay que estar prevenidos a esta clase de situaciones. En un estudio lo que hicieron fue evaluar cómo alterando imágenes médicas mediante estas GAN, los científicos que realizaron el estudio eran capaces de engañar a los usuarios. radiólogos expertos en observar las imágenes médicas. Por lo tanto, lo que hicieron fue incluir en imágenes de sujetos sanos nódulos pulmonares y en imágenes de sujetos con cáncer el cáncer de la pérdida de la sangre. quitar de forma artificial estos nódulos. Y los resultados que se vieron es que, tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico, de hecho, en más de un 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales. También, otra de las aplicaciones no positivas para la sociedad que podríamos evaluar de las redes neuronales generativas, serían, por ejemplo, los deepfakes, en los que somos capaces de generar, de forma artificial, vídeo y audio de una persona concreta, para hacer creer al resto que esa persona ha dicho unas declaraciones concretas o ha hecho una acción específica, induciendo error a la población sobre lo que esa persona ha hecho y, por lo tanto, dañando su imagen. Y, de hecho, hay decenas de vídeos en... por Internet en las que podemos ver esta clase de situaciones. Las redes neuronales generativas adversarias son un tipo de red neuronal que lo que nos permite es generar datos artificiales. Para resolver esta tarea de forma óptima lo que entrenamos son dos redes neuronales una red neuronal que es la encargada de generar estos datos que son artificiales y luego tenemos otra red neuronal que es la encargada de inferir o de discriminar si estos datos son reales o son falsos por lo tanto, cuando hacemos diversas iteraciones de entrenamiento la red neuronal de generación de imágenes es muy buena creando imágenes que se parecen a la realidad porque intenta engañar a la red discriminadora que es la red que intenta detectar si estas imágenes son reales o son ficticias. Por tanto, entramos en un proceso, un círculo virtuoso, en el que cada vez la imagen generadora genera mejores imágenes o más parecidas a la realidad, y la red discriminadora cada vez es más buena detectando estas imágenes que son falsas. Por lo tanto, entrar en un juego de intentar superarse la una a la otra, consiguiendo que la red generadora, finalmente, que es nuestro objetivo, genere imágenes muy, muy, muy... similares a las imágenes reales. Las GANs al final se pueden utilizar para toda una diversidad de tareas que tengan como objetivo un objetivo. generar información de una distribución de datos específica que antes no existía, como por ejemplo generación de caras, de hecho hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe, es decir, y es extremadamente realista. Por lo tanto, las GAN... lo que nos permiten es generar datos de cualquier distribución con la que la hayamos entrenado y por tanto replicar toda esta información. Incluso también nos puede servir, hay muchísimas aplicaciones de las GAM, pero nos puede servir también para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. Por ejemplo, dada una imagen en blanco y negro, conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permite hacer cualquier tipo de técnica que tenga como salida que debería ser realista, es decir, la generación de datos que son realmente plausibles, como por ejemplo, ya os digo, la coloración de una imagen, la generación de una cara artificial, la generación, por ejemplo, de imágenes que sean realistas para tener más datos con el que entrenar otras redes neuronales, etc. Ahora tenemos los transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que empezó a ser un proceso de transformación. como una revolución en la inteligencia artificial, concretamente en el procesador del lenguaje natural, ya que lo que permitía era superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. Los transformers lo que pueden hacer es obtener, sin un coste computacional disparado, la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, cosas en las que las redes neuronales recurrentes no eran tan óptimas. Y, además, nos permite hacer esta clase de relaciones y de entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en términos de parámetros y, por lo tanto, finalmente, de desempeño final. Uno de los... Ejemplos más disruptivos de los últimos años de Transformers ha sido el GPT-3, que no es más que un generador de texto tan preciso y tan realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido, puede ser, por ejemplo, la comprensión de contenido. a través de un resumen del mismo, la traducción de ese contenido, el diseño incluso de aplicaciones que se basen en esta generación de texto para multitud de aplicaciones finales, la programación automática desde un punto de vista informático, es decir, se han creado herramientas que te permiten, o permiten a los desarrolladores, tener una generación automática del código que ellos quieren realizar, o al menos una sugerencia que después ellos pueden adaptar, haciendo mucho más eficiente el trabajo, toda clase de tareas de generación de contenido online que se os puedan imaginar, porque tenemos una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, podemos hacer que esta herramienta específica escriba sobre multitud de temas. Otra aplicación que ha tenido un gran impacto en los últimos años de Transformers es la generación de imágenes dada un texto descriptivo. Es decir, lo que podemos hacer es generar cualquier tipo de imagen dada una descripción específica. Esto lo que nos permite es, de alguna manera, acelerar o hacer más eficiente el proceso creativo. ya que, dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto que nos puede inspirar. Esto puede servir para, por ejemplo, inspirar a distintos grupos de diseño, incluso se han creado NFTs a partir de este arte hecho por redes neuronales, etc., que al final aportan un valor de mercado muy relevante. Finalmente, yo lo que pondría el foco es que la Inteligencia Artificial, y por lo tanto el Deep Learning, lo que se centra es en automatizar una serie de tareas específicas a partir de datos concretos. Y estamos muy lejos de esta Inteligencia Artificial general, que puede estar en el imaginario colectivo por las películas, que es capaz de tomar decisiones autónomas, de hacer una multitud de tareas. No, lo que tenemos son sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. Aunque la Inteligencia Artificial, hemos comentado que tiene ya muchos años, es decir, se empezó a desarrollar en la década de los 50-60, pero su uso decayó debido a que no teníamos ni los datos, ni el poder de computación, en esta era, o en esta época de desarrollo de Inteligencia Artificial, no se había dado cuenta de que había un problema. Y eso es lo que nos ha dado la oportunidad de ver este video, que es el de Inteligencia Artificial, que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver ya se están desarrollando herramientas tan potentes que están proporcionando un montón de valor de mercado y, por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, sino que, todo lo contrario, una tecnología que está aportando valor de mercado, que cada vez aporta más valor de mercado y, por tanto, es una tecnología que está impactando e impactará nuestras vidas de forma muy pronunciada. Sin embargo... teniendo en cuenta todo lo mencionado y el valor de mercado que ya está produciendo, hay que tener en cuenta que solo estamos al principio, es decir, este último boom, esta última serie de desarrollos en la integración artificial, se han empezado a producir mayoritariamente a partir del 2012, por tanto, solo llevamos una década viendo el potencial que estas técnicas pueden tener, pero el cambio, la adopción y la evolución de estas técnicas está creciendo de forma más exponencial y, por lo tanto, cada vez veremos... aplicaciones que nos sorprenderán más. Incluso ya somos capaces de ver todas estas aplicaciones que son incluso capaces de generar información tan real que son capaces de engañar al ojo humano. Y esto solo es el principio, por tanto, esto es un campo apasionante que, sin duda, seguiremos con detenimiento.