---
title: Análisis Avanzado | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris
Tags/Keywords: #Análisis Avanzado #José Peris
lang: es-AR
---
# Análisis Avanzado
![[478.E1.U3.1_Análisis_Avanzado.mp4]]
[Análisis Avanzado](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris)

Vamos a hablar de varios conceptos claves para el correcto funcionamiento de nuestro proceso de desarrollo de nuestros algoritmos, es decir, son aquellos conceptos que siempre tenemos que revisar antes de comenzar a entrenar nuestro algoritmo o cuando estemos observando que la precisión no es la correcta, siempre tendremos que volver a revisar si cumplimos esta serie de características, para ello vamos a... Vamos a intentar analizar punto por punto donde tenemos que prestar la atención. El primero, como ya hemos mencionado anteriormente, sería el concepto de los missing values. Como hemos dicho, el primer punto sería visualizar nuestro dataset. Aquí al visualizar o pedirle, depende del framework o del lenguaje de programación que estemos utilizando, lo que haremos será consultar las estadísticas de este dataset para que nos tratemos de ello. diga de una forma científica cuántos missing values tenemos. No se trata de ir comprobándolo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de missing values tenemos en nuestras columnas, en nuestras variables. Aquí lo importante, una vez los detectamos, es saber qué estrategia seguimos. ¿Cómo manejamos esta situación? Las estrategias a seguir las podríamos dividir en dos grupos. El primero sería eliminar y el segundo sería reemplazar. Aquí hay que hablar de dos escenarios muy diferentes. Uno es cuando tenemos muchísimos datos, estamos hablando de 300.000 o 400.000 filas, de alguna forma vamos sobrados. Entonces, si tenemos un porcentaje de missing values, digamos que es un poco más. que están afectadas el 20% de las filas, el hecho de que las eliminemos no va a importar demasiado. Pero existe otro escenario en el cual vamos justos de datos. Entonces, imaginaos que tenemos 3,000, 4,000 filas. Si aquí estamos hablando de eliminar filas donde hay missing values, estaríamos teniendo un problema, porque realmente si ya íbamos muy justos, estamos yendo aún más justos, con lo cual esto... puede tener una afectación en la performance de nuestro algoritmo. Por tanto, esta sería la primera medida, el eliminar filas que decidiríamos en base a nuestra cantidad de datos. Otra estrategia sería si vemos que una variable tiene una gran cantidad de missing values, pues realmente lo que está afectando a un 40% de los datos, un 40% de las filas, lo mejor es eliminarla porque... porque no va a haber forma de reemplazar estos valores. Y esto nos da pie a hablar de la segunda estrategia, que sería reemplazar. Esta estrategia la aplicaríamos cuando estamos en el escenario de que tenemos pocos datos. Por tanto, lo que tenemos que hacer es exprimir al máximo estos datos. ¿Reemplazar? Se puede reemplazar por la media, se puede reemplazar por la mediana. se puede empezar con una constante o hay otro tipo de estrategias que lo que harían sería si pongamos que por ejemplo en un dataset un factor muy importante es la edad, una variable muy importante y resulta que tenemos missing values en la variable edad que tenemos muchos y que no sabemos la edad de una cantidad muy grande de usuarios pero ¿qué pasa? que imaginaros que también tenemos su sus estudios, su ocupación de los... Imaginaros que es un dataset, ¿no? Otra vez, no sé, por ejemplo, pongamos de... estudios universitarios y tenemos el que han estudiado o a qué se han dedicado después o tenemos diversas informaciones que nos pueden hacer suponer en base a la ocupación o al nivel de estudios podríamos interpolar o averiguar un poco el rango de edad, no es preciso a lo mejor que tengamos la edad exacta, sino podemos... determinar si esta persona, a lo mejor, está en primaria, o está en secundaria, o en estudios universitarios, o trabajando, por explicarlo, o jubilada. Aquí tendríamos cinco grupos, y aquí lo que estaremos haciendo es crear una columna nueva, donde podemos también el parámetro de edad transformarlo, y de esta forma podríamos salvar el problema de los missing values. En definitiva, yo lo resumiría con una frase que es, comprender el contexto y ser creativo. El segundo concepto clave es cross-validation. Cross-validation puede ser un concepto al principio un poco confuso. A mí me gusta una forma de representarlo, de visualizarlo, como el concepto de mezclar las cartas. Cuando jugamos a un juego de cartas... y atascura una partida, después lo que hacemos es mezclarla. Ahí lo que estamos buscando es heterogeneidad. Imaginaros que hemos terminado una partida y los cuatro ases o tres ases están sobre la mesa, porque seguimos una escala. Si nosotros simplemente las cartas las juntamos, lo que nos pasará es que estarán los ases juntos, los caballos juntos, es decir, habrá una homogeneidad. Nosotros, cuando jugamos a cartas, necesitamos mezclar, heterogeneidad. ¿Y por qué es importante el concepto de cross-validation? Recordad que hemos hablado que nosotros partimos en entrenamiento y en test, 80-20 o 70-30. ¿Pero qué ocurre? Vamos a pensar sobre este concepto de entrenamiento y test. Imaginaros, vamos a poner el foco en lo que es el concepto de la captura del dato. Imaginaros que vamos a intentar hacer un algoritmo predictivo, vamos a hacer un clasificador que lo que queremos es en base de ciertos datos sociológicos y de historial médico vamos a intentar predecir si un paciente ¿va a sufrir coronavirus o no? Vamos a visualizar cómo capturaríamos los datos, ya que hemos pasado por procesos de vacunaciones masivas, imaginamos que cogemos un centro sanitario de una capital y ahí lo que hacemos es medir diversos parámetros médicos de estos usuarios. Imaginaros que hacemos una convocatoria masiva, porque necesitamos personas de todas las edades, todo el rango de edades. Imaginaros que nos llega primero un autobús de un parvulario, llegan todos los niños y se les cogen todos los parámetros médicos y se almacenan en una base de datos del 0 al 50, por ejemplo. Imaginaros que después llega un instituto, y lo mismo a todos los alumnos, después llega gente de una empresa y después llega otro autobús con gente de una residencia de ancianos, por ejemplo. ¿Qué ocurriría aquí? Imaginaros que hacemos 80-20 entrenamiento test. ¿Qué ocurre? El primer autobús que ha venido es el de los niños de preescolar y el último es el de los ancianos. Ahora imaginaros, si partimos el 80-20, probablemente nos vamos a dejar fuera el autobús de los ancianos. ¿Qué ocurriría? Que este algoritmo que lo estamos entrenando con unos datos tiene un sesgo porque no ha visto casos de gente anciana no ha visto un rango de edad, por lo tanto aquí hay un sesgo ¿Qué deberíamos hacer? Mezclar las cartas de forma que en este 80% de entrenamiento tendríamos todos los rangos de edad mezclados y en el rango, en el 20% de test, tendríamos ejemplos de todos los rangos de edad. Este es el concepto de cross-validation y se basa en el concepto de k, que son el número de folds o el número de particiones que hacemos. Es una forma de evitar sesgos porque al final no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos. El siguiente concepto clave es el overfitting. y es un concepto que a priori es bastante abstracto, pero tiene mucho sentido. Como hemos dicho, los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. El overfitting simplemente significa que han aprendido demasiado de esos datos del pasado, que se han ceñido demasiado esos datos. El objetivo central de un algoritmo es poder en sus predicciones generalizar bien No es importante No es importante que realmente tenga una precisión súper elevada Es importante que sepa generalizar bien Es decir, ¿por qué? Para que no haga predicciones de aberraciones o de cosas incoherentes En el caso del overfitting, imaginaos que Un ejemplo visual muy sencillo, imaginaos que vais a haceros un vestido a medida a un sastre. Y este sastre os mide vuestras dimensiones corporales y no deja ninguna holgura, simplemente construye. construye un traje totalmente ceñido a tu cuerpo, porque realmente esas son tus dimensiones. Esto nos haría caminar rígidos, siempre dejamos una holgura, un margen de confianza. Cuando hablemos del overfitting, hay que empezar a pensar en algo rígido, en algo que no generaliza bien, que no nos deja margen, que no nos deja tolerancia. O ahora imaginaros que vais a compraros una cama Y verdad que en una cama nosotros unos van a dormir de lado, otros van a dormir hacia arriba, otros boca abajo. Y siempre, fijaos que una cama es rectangular y nos deja un margen, una tolerancia. Fijaros en la gráfica de la derecha y veremos un ejemplo clásico de overfitting. Nosotros estamos hablando que si tenemos que... Al final los datos son una nube de puntos. ¿Qué ocurre con un punto hasta aquí, otro hasta aquí, otro hasta aquí? ¿Qué ocurre? Cuando un algoritmo aprende con overfitting lo que está ocurriendo es que va prediciendo cada punto aprendido, se ha entrenado de forma demasiado exacta. No nos interesa que se ajuste a cada punto, nos interesa que pase por la media de los puntos, es decir, esto ata un poco y liga con el concepto de media y variancia, es decir, nos interesa que nuestro algoritmo no haga una predicción exacta de cada punto. sino que esté en la media correcta más o menos de todos los puntos. ¿Por qué? Porque esto lo que nos va a permitir es que sepa generalizar bien. Porque si un algoritmo se entrena con overfitting, lo que va a ocurrir es que en el momento que tenga que hacer la predicción de ciertos valores que se salen un poco, de este patrón no va a realizar predicciones muy nefastas, dicho de una forma muy sencilla puede pasar de predicciones del 99% a predicciones del 55%, esto no nos interesa, esto es porque se ha entrenado en overfitting se ha ceñido en el entrenamiento demasiado a los datos históricos, por tanto cuando ve un dato nuevo extraño no sabe cómo comportarse, nos interesa más que el algoritmo tenga una precisión más estable por ejemplo del 85 o 89%, pero que cuando vea casos atípicos sepa generalizar bien. Es decir, cuando hablamos de overfitting, estamos hablando en Machine Learning y en Inteligencia Artificial, el 100% no existe, con lo cual estamos hablando que siempre vamos a asumir un error. Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y queremos que al final permita generalizar de forma coherente porque el error ya es asumido. La pregunta es ¿cómo detectar el overfitting? Pues se detecta cuando en los datos, cuando se entrena el algoritmo y en los resultados que arroja sobre los datos de entrenamiento arroja precisiones muy elevadas, estamos hablando del 99%. por tanto aquí tenemos que prestar atención que nuestro algoritmo se ha ajustado demasiado a los datos, por tanto no va a ser flexible ante los datos nuevos y aquí es donde tenemos que iterar y revisar y aplicar técnicas para poder evitar el overfitting ¿Y qué técnicas nos van a ayudar a evitar este overfitting? Lo primero es cross validation, como hemos mencionado partir en un número de carpetas, mezclar las cartas También podemos aplicar data augmentation, que es una técnica que lo que haremos sería aumentar de forma artificial nuestra cantidad de datos. Y podríamos probar también otras técnicas como el dropout o el early stopping, que al final lo que se interesa es buscar heterogeneidad. El dropout sería dejar fuera. ciertos datos de forma aleatoria, al final lo que vamos a buscar es tener esta heterogeneidad y que los algoritmos aprendan de forma correcta. Otro concepto clave sería el concepto de outlier. En primer lugar hay que mencionar que hay que hacer una buena praxis para detectar un outlier. Para ello lo que tenemos que hacer es visualizar y pintar nuestros datos. Es un data point que tiene un valor muy atípico, es decir, que se desvía muchísimo de la media. Intentemos pensarlo en el mundo real. Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce. de un e-commerce que venga todo tipo de productos digamos similares a los que puede tener Amazon. Nosotros capturaríamos, cogeremos la base de datos y empezaremos a analizar los valores. Y resulta que vemos que, imaginaros, que todos los días entre semana se venden de media De media global, Y nos encontramos que un día, un viernes de un mes concreto, ese día se vendieron hay un por diez. Lo primero que haríamos sería visualizar ese dato, lo detectaríamos y comprobaríamos si ese valor es real o no, porque podría pasar que en lugar de mil, en la base de datos se ha insertado manualmente diez mil. Entonces estaríamos hablando de una anomalía, habría que corregirlo, pero resulta que hacemos las comprobaciones, hablamos con el cliente y resulta que nos dice que sí, que efectivamente ese día se vendieron diez mil unidades. Claro, a nivel de variables diríamos ¿qué pasó ese día? ¿había una promoción? No. Pues imaginaos que el hecho que provocó esto fue que en Amazon se terminaron las existencias ese día de un producto muy demandado. Por tanto, como en Amazon se terminó, los clientes llegaron hacia nosotros. Entonces, este hecho ha sucedido y obviamente es importante. Pero si pensamos en términos de matemáticas y nos imaginamos esa gráfica, y estamos hablando de valores de mil o cercanos a los 1900, 1100, 1200 y, de repente, 10000, este valor sucedió, pero nosotros, recordad que un algoritmo puede ser visto de una forma abstracta, conceptual, como una línea. como una línea recta o curva que tiene una fórmula matemática. ¿Qué ocurre? Que esta línea nos la distorsionaría de una forma enorme, ¿no? Este punto, este valor tan elevado. Entonces, es aquí cuando hay que volver al concepto de generalizar. Como queremos que generalice bien, en este caso es más útil eliminar el outlier porque va a generar una distorsión en todo el conjunto. Es aquí donde es muy importante remarcar el entender la naturaleza del problema que estás abordando. Pero no siempre, no siempre es la mejor estrategia tampoco eliminarlos, es decir, tenemos que entender, como hemos dicho varias veces, la naturaleza del problema. Por ejemplo, en los casos de predicción de los stock markets, en el caso de acciones, una bolsa de valores, casos de cripto, los outliers realmente son muy importantes, es decir, hay que prestarles muchísima atención. De hecho, las métricas que hay que utilizar en estos casos son métricas que están preparadas para prestar especial atención a los outliers. ¿Por qué? Porque en la gráfica, por ejemplo, de un proyecto cripto, de un activo digital cripto, son muy importantes los outliers. picos y valles potentes, que al final están hablando de que ese día ocurrió algo, sucedió algo bonito e importante. Por tanto, respecto a los outliers hay que analizar la naturaleza del problema. para ver la estrategia que seguimos. Otro concepto muy importante es la normalización y estandarización, o también conocido como el feature scaling. Como hemos comentado anteriormente, podemos trabajar con múltiples variables. Pueden ir desde 4 hasta 10, 12 o hasta 40. Aquí es muy importante entender que estas variables se encuentran en diferentes escalas. Por ejemplo, si estamos hablando de productos del real estate o del sector inmobiliario, podríamos hablar que la variable euros puede ir desde 0 hasta 4 millones de euros, o 10 millones de euros, por ejemplo, pero la variable cuartos de baño puede ir desde 0 hasta 5 como máximo. ¿Qué tenemos aquí? Escalas muy diferentes. escalas muy desproporcionadas. Entonces la normalización y estandarización lo que pretende de una forma muy sencilla, más conceptual, es reducir esta escala, manteniendo la proporción. Lo que vamos a hacer al normalizar es todos estos valores, transformarlos para que estén entre el rango de 0 y 1. ¿Y por qué vamos a hacer esto? Porque al final estamos hablando... El algoritmo, para que lo visualicemos conceptualmente, es una curva, o es una recta, es una gráfica, y lo que vamos a hacer es facilitarle las cosas, es decir, que entienda de una forma mucho más ordenada estos valores. Al final, lo que ocurre es que, cuando aplicamos una normalización, lo único que va a suceder... es que en lugar de ver 8 millones de euros vamos a ver 0.9 o en lugar de ver 100 mil euros vamos a ver 0.9. Punto Uno. Este es el cambio que vamos a apreciar nosotros, pero lo que vamos a hacer es que durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. También hay que recordar que el proceso de normalización hay que hacerlo y deshacerlo, porque después a la hora de interpretar resultados hay que recordar que tenemos todos estos valores en esta escala. La estandarización es un proceso similar. Lo único es que se utiliza la estandarización cuando estamos resolviendo problemas que se asemejan la distribución de los datos a la campana de Gauss. De esta forma simplemente colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta menos 1. Podemos aplicar una, podemos aplicar las dos, pero bueno, por lo general es una buena praxis el intentar llevar los datos a la escala correcta cuando observemos muchísimas diferencias de escala o de magnitud. Y por último... Otro concepto muy importante, podríamos denominarlo one hot encoder, es de suma importancia porque hay que recordar que podemos tener variables, hemos visto tipos de datos, podemos tener timestamp, podemos tener íntegra, podemos tener double, que son al final números, pero también podemos tener strings. Imaginaros que, en el ejemplo inmobiliario, estamos hablando de posición de una vivienda o situación de una vivienda. Puede ser centro, puerto, periferia o zona montañosa. Claro, aquí tenemos palabras. Puede coger cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo, lo que necesitan los algoritmos son realmente números, son ceros y unos. Al final todo se basa en esto, con lo cual no nos va a admitir palabras. El concepto de One-Hot Encoding lo que hace simplemente es que una variable, si puede tener cinco valores, por ejemplo, si puede ser playa, montaña, etc., la va a desplegar en cinco. Es decir, playa, montaña, etc. periferia, centro, es decir, la variable ubicación se va a convertir en sus cinco columnas, en cinco posibles valores, y a cada fila le va, si estamos en la playa, tendrá, esta casa está en la playa, en la variable, en la columna, en la columna ubicación barra playa, tendremos un 1, un valor 1, y en el resto, valor 0. Imaginemos que la siguiente casa está en la montaña. En la variable ubicación barra playa tendrá un valor 0, en ubicación barra montaña tendrá un valor 1 y en el resto tendrá un valor 0, es decir, al final lo que hacemos es desplegamos. El punto clave aquí es saber. cuántos valores puede coger nuestra variable, si son 5 se va a transformar una variable en 5 variables. De esta forma le permitiremos al algoritmo poder trabajar de forma óptica ya que muchos de ellos solo aceptan números.