---
title: Tratamiento del Dato
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/texts/41630370-u2-1-tratamiento-del-dato
Tags/Keywords: #Tratamiento de Datos #Tratamiento del Dato #ETL
lang: es-AR
---
# ¿Qué es una ETL?
Dentro del mundo del Data, es un concepto denominado Pipeline, en el cual encontramos diferentes fases.

Una ETL consiste en Extracción, Transformación y Carga (_Extraction, Transform and Load_).

![[477.E1_Tratamiento_del_Dato.png]]

## 1 | Extracción
- La fase de extracción, normalmente, corresponde a una serie de scripts o tareas que normalmente suelen leer en las fases de datos y cargan toda esa información en la memoria. 
- En general, se utiliza Python, pero también SQL. 
- Esos datos son leídos para posteriormente ser transformados. 
- Te encontrarás diferentes tablas en una base de datos y dichas tablas pueden tener valores nulos. 
- Como es necesario unir todas esas tablas, debes corregir todos esos valores nulos y otros problemas antes de hacer una carga en Data Warehouse, ya que una vez dado este paso, no se permiten ediciones.

Al trasladar todos estos datos al Data Warehouse, ya están listos para ser consumidos por especialistas de todo tipo, ya sean analistas, CEOs o CTOs. 

## ETL y ELT | Diferencias
ELT (Extract, Load & Transform) se ha puesto bastante de moda por el bajo precio del almacenaje durante estos últimos años, así como por la potencia de cómputo de los Data Warehouse.
- Lo que se hace es extraer datos, como en la ETL, pero la transformación se realiza dentro del propio Data Warehouse. 
- Se utilizan diferentes herramientas, pero ahora mismo la que está más en auge es DBT, que utiliza el lenguaje SQL para conseguir este tipo de transformaciones. 
- Por último, se carga otra vez en el mismo Data Warehouse.

## Data Warehouse y Base de Datos | Diferencias
Podemos encontrar tablas, tanto en un Data Warehouse, como en una base de datos tradicional. Lo que realmente las diferencia son las operaciones que puedes realizar sobre ellas. 

Normalmente, se usa un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, inserciones y borrados de forma continua. 

Sin embargo, en el Data Warehouse todas estas operaciones no están permitidas:
- **No se permite la edición de filas.** Realmente no se hace una edición, si lo que se hace es añadir fila. 
- **No se permiten los borrados de forma habitual,** aunque en algunas tecnologías sí que se permiten. 
- **El coste por almacenamiento en un Data Warehouse es mucho menor** que en una base de datos tradicional.

## Data Lake
Un data lake permite almacenamiento masivo, pero los datos están totalmente desestructurados. 

Es ideal para guardar información que nos llega en forma de eventos, que pueden tener cualquier estructura interna y pueden ser totalmente desordenados.

## ETL y ELT | Casos de uso
No existe tanta diferencia entre lo que podemos llegar conseguir utilizando cualquiera de estas dos Pipelines. 

Al final siempre estamos cargando los datos a un sitio para que se sean consumidos. 

Lo que hay que valorar es: 
1. Esfuerzo económico 
2. Tiempo de desarrollo 
3. Tiempo de reacción al cambio 

Tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda y todos esos datos llegan a nuestra Pipeline.
- Son extraídos y, aquí viene la gran diferencia, ¿qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente?. 
- Si cargamos estos datos, tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no nos esperamos, que puedan contener un campo que no es correcto o tienen un campo que realmente no está dentro del esquema que nosotros estamos buscando.
- Entonces, una buena medida es transformarlos antes de cargarlos, simplemente para asegurarnos de no tener ese tipo de errores. 
- Pero al mismo tiempo, también queremos almacenarlos en un sitio, ahorrarnos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar. 
- Entonces, es un tema económico y también de potencia de cómputo y de rapidez de la gestión del dato, que diferentes negocios pueden valorar de diferentes maneras. 
- Ahora mismo el modelo ELT es más barato que el ETL.

## El Caso de Internxt
Empezamos utilizando un modelo ETL, pero rápidamente nos dimos cuenta de que tenía mucho sentido migrar a un Pipeline tipo ELT, básicamente porque resultaba mucho más económico para todas las gestiones que queríamos hacer.

La potencia de las nuevas tecnologías como DBT junto a [BigQuery](https://cloud.google.com/bigquery/?utm_source=google&utm_medium=cpc&utm_campaign=emea-be-all-en-dr-bkws-all-all-trial-e-gcp-1011340&utm_content=text-ad-none-any-DEV_c-CRE_574561847454-ADGP_Hybrid%20%7C%20BKWS%20-%20EXA%20%7C%20Txt%20~%20Data%20Analytics%20~%20BigQuery%23v6-KWID_43700072681098079-kwd-12297987241-userloc_2056&utm_term=KW_big%20query-NET_g-PLAC_&gclsrc=ds&gclsrc=ds&gclid=CJrW0POoxPwCFWdCHQkdj-8PxQ) (en este caso que usamos BigQuery) era una combinación que prácticamente toda la industria Data estaba utilizando y la compañía también se estaba moviendo a desarrollar nuevos conectores, documentación que nos podía ayudar a poder distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente el Pipeline que teníamos.

No solo eso, también teníamos un problema: Nuestra extracción de datos estaba bastante acoplada a directamente guardar en BigQuery.

> Entonces, almacenar primero y luego transformar tenía mucho sentido. El primer paso de un Pipeline de data siempre es la extracción.

### Otras fuentes de datos
- La mayor parte de veces, la extracción se genera desde bases de datos, pero no siempre es el caso. También se pueden extraer de acciones que realizan los propios usuarios, como clics, heatmaps, puede ser cualquier cosa. 
- La extracción desde este tipo de fuentes también es importante. 
- Hoy en día se utilizan CDPs para realizarlo y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, ya que, muchas veces, se pierden eventos e información por el camino. Entonces, el challenge es tener un sistema robusto. 

Durante la extracción de datos es importante tener un esquema prefijado, esto nos ayudará a que luego el dato en nuestro Data Warehouse esté reflejado de una manera clara para el resto del equipo. 

Entonces, antes de la ingesta de datos, se establece un protocolo: 
1. Esperar con estos datos, campos y tipos, y, cuando pase por nuestro Pipeline, todo lo que no entre dentro de ese marco, lo podemos descartar o hacer una transformación para que entre dentro de ese marco.
2. En el mundo real pueden suceder un millón de cosas; estamos sujetos a caídas de diferentes sitios o servicios o a posibles ataques. Entonces, tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. 
3. Se suele decir que el 70%-80 % del trabajo de un Científico de Datos es limpiar esos datos, por eso no es una cuestión trivial. 

Cuando llegamos a la visualización del dato, muchas veces, el equipo de data tiene que reportar al equipo directivo o a otro departamento. Es importante tener claras qué herramientas de visualización se van a utilizar y sobre todo, qué implica. 

¿Cuánto tiempo de trabajo implica utilizar esas herramientas? La visualización tampoco es trivial. Muchas empresas tienen un departamento exclusivo para ello y tenemos que optar un poco en detener un equipo para ello o usar una herramienta. 

Vas a tener que hacer consultas de SQL, vas a tener que tener algún experto en usabilidad y que pueda decir: Aquí van estos colores, aquí estos otros, y aquí quiero presentar la información de esta manera o de esta otra manera. Y tienes que tener claro cuáles son los gastos que vas a tener que hacer en los diferentes casos.

Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redis que te permiten hacer más cosas, pero son mucho más costosos y necesitas probablemente un equipo que esté dedicado a ello.