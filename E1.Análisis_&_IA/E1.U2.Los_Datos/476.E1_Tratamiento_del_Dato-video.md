---
title: Tratamiento del Dato | Joan Mora
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U2 #datos #Tratamiento de Datos #Joan Mora
lang: es-AR
---
# Tratamiento del Dato
![[476.E1._Tratamiento_del_Dato.mp4]]
[Tratamiento del Dato](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora)

Dentro del mundo de data se considera un concepto que se llama pipeline, en el cual... hay diferentes fases. Un ETL consiste en extracción, transformación y carga. Extraction, transform and load. Entonces la fase de extracción normalmente corresponde a una serie de scripts o tareas que normalmente suelen leer unas bases de datos y cargan toda esa información en memoria y en memoria lo que se hace es mediante... en general se usa Python para esto pero también SQL, se hacen unas transformaciones de estos datos que se leen. Por ejemplo, puedes tener... diferentes tablas en una base de datos y quieres unirlas, pues todo eso se hace en ese momento. Es el momento de solucionar esos problemas, porque luego se hace una carga dentro de un Data Warehouse en general, donde no se permiten las ediciones. Entonces cuando van todos los datos al Data Warehouse ya están preparadas para ser consumidas por especialistas de todo tipo, sean analistas, sean CEOs, sean CCOs. Ahora mismo se ha puesto bastante de moda el LT, que se llama básicamente... porque el precio de almacenaje ha bajado bastante en los últimos años y también la potencia de cómputo de los data warehouse. Entonces lo que se hace es extraer como se hace en la ETL pero se hace transformación dentro del propio data warehouse, se utilizan diferentes herramientas pero ahora mismo la que está más en auge es DBT que utiliza lenguaje SQL para conseguir este tipo de transformaciones. se carga como siempre otra vez en el mismo Data Warehouse. Aunque puedas ver tablas tanto en un Data Warehouse como en una base de datos relacional tradicional, las operaciones que puedes hacer sobre ellas varían bastante. Por ejemplo, de normal se utiliza un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, tener inserciones o borrados de forma continua. Sin embargo, en el Data Warehouse todas estas operaciones no están permitidas, por ejemplo, en general no se permite la edición de filas. Realmente no se hace una edición, sino que lo que se hace es añadir fila. O, por ejemplo, tampoco se permiten los borrados a la ligera, aunque en algunas tecnologías sí que se permiten. En general, el coste por almacenamiento en un Data Warehouse es mucho menor que en una base de datos tradicional. En general un Data Lake también permite almacenamiento masivo, pero el dato está totalmente desestructurado y es ideal para guardar información que nos llegue en forma de eventos, que pueden tener cualquier estructura interna y pueden estar totalmente desordenados. Muchos se pueden preguntar dónde podemos usar un AppIreland tipo ETL o un AppIreland tipo ELT. Realmente no hay tanta diferencia a nivel de... que se puede conseguir, porque al final siempre estamos cargando los datos a un sitio para que sean consumidos, pero realmente sí que hay que valorar esfuerzo económico y esfuerzo de tiempo de desarrollo y de reacción al cambio. Entonces, por ejemplo, tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda y todos esos datos llegan a nuestra pipeline. son extraídos y luego aquí viene la gran diferencia. ¿Qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente? Si cargamos estos datos, realmente tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no nos esperamos, a lo mejor tienen un campo que realmente no es correcto o tienen un campo que realmente no está dentro del esquema que nosotros estamos buscando. Entonces, una buena medida, transformarlos antes. de cargarlos simplemente por asegurarnos de este tipo de errores pero al mismo tiempo también queremos almacenarlos en un sitio ahorranos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar entonces es un tema de... económico y también de potencia de cómputo y de rapidez de la gestión del dato que diferentes negocios pueden valorar de diferentes maneras. En general ahora mismo el modelo ELT es más barato que el ETL. En Internex empezamos usando el modelo ETL pero rápidamente nos dimos cuenta que tenía mucho sentido migrar a un aparitivo tipo ELT básicamente porque nos costaba mucho más económico todas las gestiones que queríamos hacer y la potencia de las nuevas tecnologías como DBT. junto a BigQuery, en este caso es que usamos BigQuery, era una combinación que prácticamente toda la industria data estaba utilizando y la comunidad también se estaba moviendo a desarrollar nuevos conectores, documentación que nos podía ayudar a poder distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente la pipeline que teníamos. No solo eso, también teníamos un problema de que realmente nuestra extracción de datos estaba bastante acoplada directamente guardar en BigQuery, entonces guardar primero y luego transformar tenía mucho sentido. El primer paso de un aparellante de data siempre es la extracción, la mayor parte de veces se hace desde bases de datos pero no siempre es el caso, muchas veces quieres extraerlo de acciones que toman los propios usuarios, pueden ser clics, pueden ser cualquier un tipo de... Gizmos, cualquier cosa, entonces la extracción desde este tipo de fuentes también es importante. Hoy en día se utilizan CDPs para realizarlo y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, porque muchas veces se pierden eventos, se pierde información por el camino, entonces este challenge de tener un sistema robusto. Durante la extracción de datos. es importante tener un esquema prefijado porque esto nos ayudará a que luego el dato en nuestro Data Warehouse esté de una manera clara para el resto del equipo. Entonces lo que se suele hacer es, antes de la ingesta, establecer un protocolo de, oye, me espero estos datos con estos campos, con estos tipos y cuando pase por nuestra Pipeline todo lo que no entre dentro de ese marco lo podemos descartar o hacer una transformación para que entre dentro de ese marco. ¿Por qué? Porque en el mundo real se... pasan un millón de cosas y estamos sujetos a caídas de diferentes sitios o servicios, estamos sujetos a posibles ataques, entonces tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. Se suele decir que el 70 o 80% del trabajo de un data scientist es limpiar datos y por eso no es una... una cuestión trivial. Cuando llegamos a la visualización del dato, muchas veces el equipo de edita tiene que reportar al equipo directivo o a otro departamento. Y es importante tener claras qué herramientas de visualización se van a utilizar y sobre todo, qué implica, cuánto tiempo de trabajo implica utilizar esas herramientas, porque la visualización no es trivial, muchas empresas tienen un departamento exclusivo para ello tenemos que optar un poco entre tener un equipo para ello o usar una herramienta. Vas a tener que hacer consultas de SQL, vas a tener que tener algún experto en usabilidad y que pueda decir, oye, aquí van estos colores, aquí quiero presentar la información de esta manera o de esta otra manera y tienes que tener claro cuáles son los gastos que vas a cobrar. a tener que hacer en los diferentes casos. Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redax que te permiten hacer más cosas, pero son mucho más costosos y necesitas probablemente un equipo que esté dedicado a ello.