---
title: El futuro del Machine y Deep Learning | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U7 #futuro del machine learning y deep learning #Machine Learning #Deep Learning #José Peris
lang: es-AR
---
# El futuro del Machine y Deep Learning
![[506.E1_El_futuro_del_Machine_y_Deep_Learning.mp4]]
[El futuro del Machine y Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris)

Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no hablar de un concepto que se llama Automachine Learning. Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolla inteligencia artificial. A priori puede parecer una hoja. una amenaza para la humanidad, pero no tiene nada que ver con todo esto. Simplemente, estamos hablando que hay, como dijimos, la razón de ser de este campo de la IA y del Machine Learning, es el eliminar tareas repetitivas de bajo valor. Por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías. De una forma muy rápida, tenemos... plataformas que nos permiten hacer tareas que en otras ocasiones duraban meses. Estamos hablando, debemos ponernos un poco en la piel de un científico de datos, tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. La primera sería con lo que se llaman desarrollo a bajo nivel, estaríamos hablando de código, estaríamos hablando de... estamos hablando de Python, que es el lenguaje oficial, por decirlo de alguna forma, de esta comunidad, aunque están apareciendo otros, como Go, que también pretenden ganar terreno. También tenemos R, que se utiliza más en el ámbito de la bioinformática. En definitiva, Python es el que mayor comunidad tiene. Pero también tenemos otro grupo, que sería de los... y esta es una pregunta muy... común de los estudiantes que es el de los programas de alto nivel. Los programas de alto nivel son aquellos que son user friendly, es decir, son interfaces de usuarios que son amables, que son fáciles de entender. En pocas palabras estamos hablando de picar código versus tener herramientas que son prácticamente drag and drop, que son ya más familiares y que trabajan con el concepto de nodos. ¿Qué herramientas tenemos que usar? en este segundo grupo del drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial del aprendizaje. Tendríamos herramientas como KNIME, como ORANGE, como BICML. ¿Qué os podría comentar de estas herramientas? Pues bueno, básicamente KNIME. está muy bien para el aprendizaje en fases tempranas y para poder prototipar, para poder defenderte, para poder entender de una forma esquemática lo que son los flujos de un algoritmo respecto en la ETL y en la fase de prueba, de ensayo y error con diferentes algoritmos. Y estamos hablando de un concepto de nodos. Obviamente no son programas profesionales, sino que nos sirven de puente para poder llegar a pensar. y aplicar todos los conceptos tanto teóricos como prácticos. Después tendríamos programas como Orange, que es del pack Anaconda, que Anaconda es el grupo donde se ubica Python, donde han hecho una interfaz mucho más amable, mucho más agradecida y también nos permite prototipar muy rápido y de una forma muy intuitiva, nos permite en definitiva acortar tiempos y sobre todo ponernos bien. barreras al aprendizaje. Y BKML sería un concepto que ya se sale de lo que son los nodos y realmente es un concepto más por ventanas. Estas serían las tres herramientas, las dos primeras son gratuitas, BKML es de pago, pero en definitiva este concepto, sobre todo el de nodos, es el que están utilizando ya plataformas como Azure o como IBM. Watson, en este caso azules de Microsoft y VM Watson, que son ya directamente plataformas drag and drop. ¿Qué significa esto que estamos hablando de conectar nodos? Conectar nodos lo que implica es que nos evitamos muchos errores de sintaxis de código, pero lo que implica también es que no tenemos tanta capacidad de edición como tenemos en el código, es decir, al final el código va a ser la tecnología que siempre va a ir más rápido, la más profesional y la más escalable de cara a ponerlo en producción. Pero sí que es cierto que con el nacimiento de Azure, o IBM Watson, se abrió una puerta a trabajar drag and drop que nos puede dar muchísima agilidad a la hora de trabajar. Desde este punto, es cuando empezaron a salir nuevas tecnologías como Data Robot o H2O, o Data IQ, Data Bricks, Rapid Miner, donde se centraron ya otra vez en los nodos, pero yendo un paso más allá, aplicando Automation Learning. Automation Learning, al final, como hemos visto, para que os hagáis una idea muy sencilla, como hemos visto, tenemos que hacer una serie de comprobaciones, de conceptos claves, como los missing values, los outliers, qué correlaciones hay entre diferentes variables, que esto en código lleva bastante tiempo, siempre teniendo en cuenta que tenemos que visualizar los datos y tomar decisiones, y ver qué factores hay que corregir, que es el proceso de TLE. Esto lleva mucho tiempo en código dependiendo el nivel que tengas a la hora de saber escribir código y de depurarlo. Por tanto, aquí surge una solución que es el Automation Learning. Podríamos decir que el líder es DataRobot. a nivel mundial, pero ya existe un grupo bastante nutrido, RapidMiner, por ejemplo, también está haciendo bastante sombra. El AutoML lo que hace es que todos estos insights, porque al final estas plataformas lo que te hacen es desvelarte insights que están revelando a los datos, ya te están marcando, te están sugiriendo qué columnas habría que eliminar potencialmente, qué tipo de filas... qué tipo de variables, qué correlaciones tienen entre ellas, todo esto de forma automatizada. Esto, al final, lo podemos traducir en tiempo, y al final el tiempo es dinero. Claro, ¿qué problema presenta esto para el usuario? Básicamente, el problema es que las licencias aún son bastante caras. Estamos hablando de que una licencia de AutoML, de estos softwares de pagos, pueden oscilar desde los 50.000 euros por usuario hasta los 150.000 euros anuales. esto es un coste que no todas las empresas se pueden permitir, pero sí que es cierto que lo que a priori era un mundo bastante cerrado, del auto M&L, que aquí la ventaja, vuelvo a repetir, es que te permite realizar de forma automatizada el EDA, lo que es el Exploratory Data Analysis, y la ETL, junto con la selección de algoritmos, Lo que ha sucedido es que han aparecido ya versiones mediante las cuales con código y de forma gratuita podemos utilizar AutoML. En esta sesión de hoy, vengo a exponeros las principales herramientas de código gratuitas que podemos utilizar para trabajar con AutoML. Cuando hablamos de AutoML tenemos que pensar que, como os he dicho siempre, la primera parte es entender el contexto, el negocio, la naturaleza del problema. pero también debemos de visualizar los datos, debemos pintarlos, esta parte es muy importante porque si no visualizamos los datos no llegaremos a entender si hay outliers, si hay correlaciones, hablábamos también de matrices de correlaciones, por ejemplo, de relaciones entre variables. ¿Qué ocurre? Que este proceso es bastante lento también porque a nivel de código hay que ejecutar Muchas sentencias y a día de hoy ya tenemos herramientas que nos permiten hacer autoeda. Autoeda sería automachine learning aplicado al exploratory data analysis. ¿Qué herramientas tenemos? En Python tenemos la librería Pandas Profiling, que desde mi punto de vista es la más potente y la mejor resuelta. ¿Y por qué? Porque estamos hablando que básicamente en 10 líneas de código, cargando nuestro dataset, nos va a ejecutar, nos va a... pintar. todos los datos referentes nos van a visualizar todas las correlaciones, todas las estadísticas de este conjunto de datos y además nos lo va a generar en HTML. Con lo cual estamos hablando que en cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos. No solo tenemos Pandas Profiling, también tenemos SuiteBits que de una forma diferente, con otro planteamiento nos arroja. prácticamente la misma cantidad de datos. El objetivo de la autoeda, así como del automachine learning, será siempre ahorrar tiempos. Os dejamos aquí dos notebooks. para ejecutar con Google Colab, donde podéis probar estas herramientas de forma muy sencilla, simplemente subiendo vuestros datasets. Y veréis que es una forma muy ágil de comenzar a trabajar con una base sólida. Respecto al Automation Learning, creo que cualquier persona que haya trabajado en este sector como científico de datos, este es el sueño de cualquier científico. el poder utilizar con código Automation Learning. ¿Por qué? Porque básicamente, realmente diríamos que es la máxima expresión de la practicidad. ¿Por qué? Porque al final en código tenemos acceso, podemos utilizar todas las herramientas que queramos y van evolucionando de forma muy rápida, pero si podemos utilizar Automation Learning, coger las ventas. de otro tipo de software, que en este caso son corporaciones muy potentes, con mucha inversión, poder utilizar características similares a las que ellos proponen, esto realmente es algo muy práctico. Con Automation Learning, consideraciones a tener en cuenta, al final Automation Learning nos va a hacer una... En el caso del código no tenemos tantas herramientas para... para utilizar el proceso de ETL, pero en el momento que tenemos que probar diferentes algoritmos, tenemos que pensar que esto, hacerlo prácticamente uno a uno, pues nos llevaría, es un proceso muy lento porque hay que invocar el algoritmo, probarlo, comprobar resultados. Aquí la ve, la ventaja que nos da Automachine Learning, que simplemente le subimos los datos, realizamos la ETL trabajamos las variables y automáticamente decimos pruébanos el mejor... algoritmo, o sea, pruébanos, por ejemplo, le podemos decir que nos pruebe 100 algoritmos y que, y ordenanos en un ranking los que mejor performance tienen sobre los datos de test, ordenarlos por accuracy, por predicción. Esto estamos hablando que es un avance total, aparte que ya te ofrece y te proponen los hiperparámetros, es decir, nos soluciona dos partes muy pesadas. al final probar diferentes algoritmos, lleva un tiempo bastante elevado, en tiempos de programación, pero ya una vez hemos seleccionado uno, seleccionar los hiperparámetros es la parte, diríamos, más abstracta, más técnica y más compleja, al final estamos hablando de hilar fino. Cuando hablamos de ajustar hiperparámetros, simplemente, para que os hagáis una idea, es cuando tenemos una precisión del 90%, cómo subir hasta el 95%. Este tramo, pues... es el más complejo de todos, porque parece que ya estamos, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando de que estas herramientas nos pueden resolver muchísimos problemas. Entre las más comunes, y hay que saber que el Machine Learning cambia constantemente, es decir, es un campo que está totalmente... que está totalmente vivo. Aquí os presentamos herramientas de AutoML en código gratuitas como puede ser, por ejemplo, TPOD. También te diríamos H2O o H20, depende de cómo lo queramos leer. AutoKeras. AutoKeras lo que está haciendo es auto-deep learning. Por decirlo de alguna forma es decir, Keras es un framework de deep learning. Normalmente. cuando nos enfrontamos a crear una red neuronal, es decir, y por un poco retomar los conceptos anteriores, cuando tenemos muchos datos, cuando tenemos un dataset muy elevado y podemos utilizar deep learning. Nuestro principal reto va a ser en decidir cuál va a ser la arquitectura de esta red neuronal vía capas o vía neuronas. Lo que nos ofrece es Autoqueras, que realiza AutoML y nos revela cuál es la mejor arquitectura posible para nuestro problema. También tenemos otro Scikit Learn, Scikit Learn es una librería de Machine Learning de Python muy potente. Tenemos otro PyTorch y seguramente por el momento que veáis estas clases habrá salido alguna más. Nosotros por nuestro lado os adjuntamos la documentación para que podáis trabajar con ellos. A mí, personalmente, AutoH2 es la que más me gusta. Y el motivo es porque, aparte, ofrece una serie de librerías que te muestran la explicabilidad del modelo, como los Shape Values, por ejemplo. La explicabilidad del modelo es muy importante, porque es no sólo decir, este modelo es el que mejor predice, sino nos va a explicar en nuestro caso de negocio, cada modelo nos va a explicar también cuáles son las palancas o cuáles son las variables claves y en qué porcentaje para poder pasar a la acción, poder mejorar desde el punto de vista de negocio. Por tanto, os animamos a que probéis los notebooks que os dejamos aquí y que disfrutéis del concepto de Automation Learning.