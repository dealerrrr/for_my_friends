---
title: Introducción a la Inteligencia Artificial | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41429870-u1-introduccion-a-la-inteligencia-artificial-jose-peris
Tags/Keywords: null
lang: es-AR
---

### 470.E1_Introducción_a_la_Inteligencia_Artificial-video

### Introducción a la Inteligencia Artificial
![[470.E1_Introducción_a_la_Inteligencia_Artificial.mp4]]
[Introducción a la Inteligencia Artificial](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41429870-u1-introduccion-a-la-inteligencia-artificial-jose-peris)

Cuando hablamos de inteligencia artificial, en nuestro imaginario colectivo, vemos representado casi un cerebro artificial, lleno de cables, una gran maquinaria, que prácticamente puede alcanzar el nivel de consciencia de una persona. Todo esto no es más que una representación y está bastante alejado de la realidad. Inteligencia artificial son un conjunto de funcionalidades que se desarrollan básicamente para... para realizar predicciones. El nacimiento de la inteligencia artificial, o sobre todo este amplio desarrollo que ha sufrido en los últimos años, está vinculado a la aparición de dispositivos como este. Smartphones, relojes inteligentes, y todo tipo de dispositivos con IoT. ¿Por qué? Porque generamos cada día millones de datos, simplemente bajándolo a nuestras apps favoritas o... moviéndonos, estamos generando una cantidad infinita de datos, que estos datos lo que sucede es que se están ingestando por parte de unas corporaciones que son los propios proveedores de estas apps que utilizamos. En este momento lo que ocurre es que los datos pasan a un sistema de ingestión a la nube, se procesan y se diseñan y se preparan algoritmos de machine learning para poder predecir, poder predecir acontecimientos. Y esto es lo importante, porque pasamos a la acción. Imaginaros que si tienes un reloj inteligente, siguiendo este esquema, sería capaz de predecir si vas a tener una enfermedad, si vas a tener un ataque al corazón, o incluso predecir cuál sería tu nutrición más adecuada. Junto a inteligencia artificial, es importante tener en cuenta el concepto de data driven. Al final, las organizaciones diariamente consumen también y trabajan con una cantidad de datos que sirve para determinar si hay una enfermedad o no. somos capaces de sacarle partido a nuestro favor, podemos obtener ventajas competitivas importantes. Quizás la representación más común que tenemos asociada a los datos son los dashboards, que son elementos de puntos de control donde simplemente un golpe de vista podemos interpretar qué está sucediendo. Al final, es todo bastante más simple de lo que parece y hay que tener en cuenta sólo un hecho. Los datos cuentan con... cuentan historias de personas, de eventos, de sucesos. Y la base de todo este módulo de Machine Learning se trata en comprender primero el storytelling de los datos para después poder ser capaces de desarrollar algoritmos que jueguen a nuestro favor para resolver problemas. Sobre todo lo que resuelve es la optimización de eliminar tareas repetitivas. Para comprender bien este campo, porque escucharéis muy a menudo los conceptos matching learning y deep learning, vamos a intentar establecer la diferencia de una forma sencilla. Matching learning lo podríamos explicar de una forma sencilla como funciones matemáticas que explican un conjunto de datos. Al desarrollar un algoritmo lo que estamos haciendo es obteniendo la fórmula que explica este conjunto de datos, por tanto, no será muy fácil poder hacer predicciones. Deep learning, por contrapartida, lo que hace es... es copiar un poco el funcionamiento de nuestro cerebro. Deep learning es una evolución de machine learning, de hecho, deep learning funciona con algoritmos de machine learning. Deep learning lo que hace es copiar un sistema de capas y de neuronas a través de las cuales se va haciendo un aprendizaje. Pero en estas neuronas tenemos que saber qué es machine learning lo que está operando. Para entender todo este campo de una forma muy sencilla, el ejemplo más simple es el pensar cómo enseñamos a... a los niños, cómo aprenden los niños. Cuando queremos que un niño aprenda qué es un vaso, lo que hacemos es mostrarle muchas veces diferentes vasos de diferente tamaño. de diferente color, de diferentes materiales. Al final, el niño lo que aprende es un patrón en el cual sabe que hay un continente y un contenido, una forma cilíndrica en la cual se puede vertir un líquido. Matching Learning y Deep Learning se basan, esencialmente, en todo esto, en la repetición de patrones, que es lo que llamamos entrenamiento, para después poder predecir y poder reconocer estos patrones. Hay otro concepto que... que se llama algoritmo, que normalmente la gente lo confunde con logaritmo y es bastante divertido. El algoritmo para entender lo que es, es muy fácil visualizar el ejemplo de una receta de cocina. ¿Qué tenemos en una receta de cocina? Nosotros tenemos ingredientes y establecemos una secuencia. Matching learning y deep learning al final no son más que líneas de códigos que invocan a unas librerías que nos están convocando trayendo unas funciones que vamos a ejecutar y simplemente las ordenamos y las ensamblamos. Por tanto, cuando pensemos en algoritmos simplemente tenemos que pensar en una batería de código que ejecuta unas instrucciones de forma ordenada. Abordando este problema de una forma muy simplificada, realmente todos los algoritmos de Machine Learning y de Deep Learning sólo saben hacer tres cosas, predecir, clasificar y agrupar. ¿A qué llamamos predecir? Imaginad que trabajáis en una empresa del sector inmobiliario y estáis constantemente recibiendo viviendas, donde estas viviendas tienen una serie de características. número de baños, tienen un número de dormitorios, están situadas en una zona céntrica, una zona de la periferia, o bueno, todo tipo de características, hay atributos que van a hacer que el precio de esa vivienda sea mayor o sea menor. Imaginaros que durante años el equipo que trabaja en esta empresa inmobiliaria se ha dedicado a analizar cada vivienda con todas sus características y asignarles un precio. Esta tarea repetitiva, manual, si la trasladamos a Machine Learning o Inteligencia Artificial, es muy sencilla, se va a convertir en algo muy sencillo porque lo que va a hacer el algoritmo va a ser, básicamente, va a... a analizar y a entender todas esas variables y en base a esas variables, como ha aprendido de toda la gente que ha estado estableciendo un precio, lo que va a hacer es va a aprender a predecir este precio. Por tanto, el resultado, de una forma práctica, simplemente sería que sólo tendríamos que introducir las características y el algoritmo nos haga una predicción del precio que tiene que tener. ¿Qué es clasificar? Pues imaginaros también que cuando vais al médico y os hacen un chequeo, os hacen una serie de pruebas donde os miden diversos parámetros, la presión sanguínea, arterial, la respiración, etc. Imaginaos que nos miden 10 parámetros. Con estos 10 parámetros, lo que nosotros queremos predecir, por ejemplo, es saber si este paciente va a tener un ataque al corazón o no, o va a tener cualquier tipo de enfermedad o no. ¿Qué ocurre? Que aquí ha habido, imaginaos, una cantidad de médicos que tienen que analizar muchísima información, 15-20 parámetros, que los vamos a llamar variables de ahora en adelante, que van a tener que analizar, que van a tener que analizar y que van a tener que analizar. ¿Qué ocurre? Que aquí hay un parámetro que nos dice si este paciente va a tener un ataque al corazón o no. ¿Qué ocurre? Que aquí hay un parámetro que nos dice si este paciente va a tener un ataque al corazón o no. ¿Qué ocurre? Que aquí hay un parámetro que nos dice si este paciente va a tener un ataque al corazón o no. ¿Qué ocurre? Que aquí hay un parámetro que nos dice si este paciente va a tener un ataque al corazón o no. ¿Qué ocurre? Que aquí hay un parámetro que nos dice y analizando todos de una forma visual, tiene que predecir esto, si esta persona va a ser propensa a padecer una enfermedad o no. Aquí tenéis que pensar que la mente humana no es capaz de procesar más de 150 líneas en un archivo, imaginaros un Excel o un CSV. Por tanto, lo que haría el algoritmo es aprender a clasificar si va a padecer una enfermedad o no. esta forma simplemente, una forma automatizada, ya haría su propia clasificación y en este caso tendríamos que el doctor simplemente tendría que fijarse en los casos críticos en lugar de ir uno por uno. Y el tercer caso, la tercera casuística que resuelve es la agrupación. En este caso, pues bueno, imaginaros, esto es muy utilizado en la segmentación de clientes. Por ejemplo, si tú tienes un comercio que es un e-commerce, por ejemplo, pues tienes registrados a una serie de clientes que tienen unas características sociodemográficas y que ejecutan unos ciertos patrones de navegación en tu página web. Pues bueno, podemos entrar a los sistemas de inteligencia artificial. para que hagan una clasterización, es decir, una agrupación, de modo que cada vez que tú entras y ejecutas tus patrones de compra o de búsqueda, te va a clasificar en un grupo. ¿Esto qué ventaja aporta? Pues bueno, aporta una ventaja que se llama personalización. Es decir, imaginaos el departamento de marketing, no es lo mismo que haga una oferta a un perfil de Valley Persona generalista que pueda dirigirse a ti. con tus características, con tu clasterización, con tu segmentación concreta, donde posiblemente te vas a sentir mucho más atraído por esa oferta porque se ajusta más a tus necesidades. Para entender dónde estamos en el campo de inteligencia artificial, hay que entender muy bien qué pasó en el año 2021, porque realmente hubo un salto muy grande. Ha cambiado realmente el paradigma del avance de esta tecnología. Existen tres tipos de inteligencia artificial. La primera sería la narrow intelligence o inteligencia estrecha, que es la que vivimos cada día. Todos en vuestro smartphone habéis intentado escribir una palabra y os hace una predicción de texto. Esto sería, estamos hablando de predicción, pero sería un texto predictivo. Se está utilizando algoritmos de machine learning por detrás. Todos también hemos intentado hacer una foto con nuestro smartphone y nos marca la cara. Esto es una bounding box. Ahí también estaremos viendo la inteligencia artificial o todos hemos utilizado el traductor de Google. Estos serían casos de inteligencia artificial estrecha, donde la principal característica es que se entrenan a los algoritmos para resolver funciones concretas específicas. El siguiente estadio de la inteligencia artificial es la general. Aquí estaríamos hablando de que la propia inteligencia artificial ya tiene un número tan elevado de funciones implementadas y se relacionan de forma tan eficiente entre ellas que ya diríamos que puede tener conciencia propia. Realmente, nadie sabe en qué punto estamos. Sí que sabemos que estamos entre la narro y entre la general. pero no sabemos exactamente en qué punto justo estamos. ¿Por qué? Pues porque este es un mundo que se apoya muchísimo en la comunidad, en la investigación, y continuamente se van publicando papers a nivel mundial, y la gente sí que es verdad que los desarrolladores e investigadores comparten muchísimo conocimiento, por tanto es un poco difícil especificar el punto exacto. Y ya, por último, la teoría. Tercera categoría sería la superinteligencia artificial, donde aquí ya hablaríamos que se conectarían todas estas inteligencias artificiales generales. Aquí se ha escrito mucho y se ha hablado mucho de este punto, cómo puede afectar a la humanidad. A día de hoy nosotros nos centramos en el estado del arte, que es donde estamos nosotros ahora, y nos ubicaríamos entre la narrow y la general.


### 471.E1.U1.1_La_Inteligencia_Artificial

### La Inteligencia Artificial
Cuando hablamos de Inteligencia Artificial, en nuestro imaginario colectivo, vemos representado casi un cerebro artificial lleno de cables. Una gran maquinaria que prácticamente puede alcanzar el nivel de consciencia de una persona.

Todo esto no es más que una representación y está bastante alejado de la realidad. 
- La inteligencia artificial es un conjunto de funcionalidades que se desarrollan básicamente para realizar predicciones. 
- El nacimiento de la inteligencia artificial o sobre todo, este amplio desarrollo que ha sufrido en los últimos años, está vinculado a la aparición de dispositivos como smartphones, relojes inteligentes…
- Cada día generamos millones de datos, simplemente, bajando nuestras apps favoritas o simplemente moviéndonos a través de ellas. 
- Dichos datos son gestionados por propios proveedores de estas apps que utilizamos, que pasan a un sistema de gestión a la nube. Se procesan, diseñan y se preparan algoritmos de **Machine Learning** para poder predecir acontecimientos. 
- Esto es importante porque pasamos a la acción. Imaginaros que si tienes un reloj inteligente siguiendo este esquema sería capaz de predecir:
1. Si vas a tener una enfermedad 
2. Si vas a tener un ataque al corazón
3. Cuál sería tu nutrición más adecuada

![[471.E1_La_Inteligencia_Artificial_1.png]]

#### Data-Driven
Junto a Inteligencia artificial, es fundamental tener en cuenta el concepto de Data-Driven. Al final, las organizaciones diariamente consumen también y trabajan con una cantidad de datos que si somos capaces de sacarle partido a nuestro favor, podemos obtener ventajas competitivas cruciales.

Quizás, la representación más común que tenemos asociada a los datos son los Dashboards, elementos de puntos de control donde simplemente, de un golpe de vista, podemos interpretar qué está sucediendo.

> Es todo bastante más simple de lo que parece y hay que tener en cuenta solo un hecho; los datos cuentan historias.

**Cuentan historias de personas, de eventos, de sucesos…** Y la base de todo este módulo de Machine Learning se trata de comprender primero el storytelling de los datos para, después, poder ser capaces de desarrollar algoritmos que jueguen a nuestro favor para resolver problemas. Sobre todo, lo que aportan es la optimización para eliminar tareas repetitivas.

#### Machine Learning vs Deep Learning
![[471.E1_La_Inteligencia_Artificial_2.png]]

A menudo, estos dos conceptos son confundidos. 

**El Machine Learning es un conjunto funciones matemáticas que explican un conjunto de datos.** 
- Al desarrollar un algoritmo, lo que estamos haciendo es obtener la fórmula que explica este conjunto de datos.
- Por tanto, nos será muy fácil poder hacer predicciones.

**El Deep Learning trata de copiar el funcionamiento de nuestro cerebro.** 
- Deep Learning es una evolución de Machine Learning. De hecho, Deep learning funciona con algoritmos de Machine Learning.
- La diferencia es que es un sistema de capas y de neuronas a través de las cuales se va haciendo un aprendizaje.
- Pero en estas neuronas tenemos que saber que es Machine Learning lo que está operando para entender todo este campo. Un ejemplo simple es pensar cómo aprenden los niños.
- Cuando le enseñamos a un niño lo que es un vaso, lo que hacemos es mostrarle muchas veces diferentes vasos, de diferente tamaño, de diferente color, de diferentes materiales.
- Al final, el niño lo que aprende es un patrón en el cual sabe que hay un continente y un contenido, una forma cilíndrica, en la cual se puede verter un líquido.
- Machine Learning y Deep Learning se basan esencialmente, en la repetición de patrones, que es lo que llamamos entrenamiento, para después poder predecir y poder reconocer estos patrones.

#### Algoritmo
A nivel conceptual, un algoritmo es una receta de cocina. 

Tenemos ingredientes y establecemos una secuencia de Machine Learning que al final no son más que líneas de código que invocan unas librerías que aplican unas funciones que se ejecutan. Por tanto, cuando pensemos en algoritmos, simplemente tenemos que pensar en una batería de código que ejecuta; unas instrucciones de forma ordenada, abordando este problema de una forma muy simplificada.  
  
Realmente todos los algoritmos de Machine Learning y de Deep Learning solo saben hacer 3 cosas:
1. Predicción
2. Clasificación
3. Agrupación

##### 1 | Predicción
Imaginad que trabajáis en una empresa del sector inmobiliario y estáis constantemente recibiendo viviendas con ciertas características. 

Tienen un número de baños, un número de dormitorios, están situadas en una zona céntrica o una zona de la periferia entre otras. Estos atributos van a hacer que el precio de esa vivienda sea mayor o sea menor.

Imaginaros que durante años, el equipo que trabaja en esta, en esta empresa inmobiliaria, se ha dedicado a analizar cada vivienda con todas sus características y a asignarles un precio. Esta tarea repetitiva o manual, si la trasladamos a Machine Learning o Inteligencia artificial es muy sencilla, se va a convertir en algo muy sencillo, porque lo que va a hacer el algoritmo.

Básicamente, va a analizar y a entender todas esas variables, y con base en esas variables como ha aprendido de toda la gente que ha estado estableciendo un precio, lo que va a hacer es a aprender a predecir este precio. Por tanto, el resultado, de una forma práctica, sería que solo tendríamos que introducir las características y el algoritmo nos hará una predicción del precio que tiene que tener.

##### 2 | Clasificación
Imaginemos una visita al médico para un chequeo, donde nos hacen una serie de pruebas para medir diversos parámetros: La presión sanguínea, arterial, la respiración… Imaginaros que nos miden diez parámetros.

Con estos diez parámetros, lo que queremos predecir, por ejemplo, es saber si este paciente va a tener un ataque al corazón o no, o si va a sufrir cualquier tipo de enfermedad o no.

Normalmente, un doctor tendría que considerar decenas de variables y sus posibles interacciones con una probabilidad de fallo u olvido significativa. Sin embargo, al entrenar al algoritmo, automatizamos ese proceso y aseguramos una probabilidad de acierto mayor

El resultado de aplicar esta tecnología a este tipo de casos es que un doctor podría centrarse en casos más críticos o en la experiencia del paciente, en lugar de utilizar su tiempo en procesos de menor valor.

##### 3 | Agrupación
La agrupación es muy común para la segmentación de clientes. 

Por ejemplo, si tienes un comercio, e-commerce, por ejemplo, tienes registrados a una serie de clientes con unas características socio demográficas y que ejecutan ciertos patrones de navegación en tu página web. Podemos entrarlos a los sistemas de inteligencia artificial para que hagan una “pasteurización”, es decir, una agrupación, de modo que cada vez que un usuario entra y ejecuta unos patrones de compra o de búsqueda, el sistema lo clasificará en un grupo establecido.

¿Esto qué ventaja aporta? La personalización. Es decir, imaginaos el departamento de marketing. No es lo mismo que haga una oferta a un perfil de buyer persona generalista que pueda dirigirse a alguien con unas características y preferencias más concretas lo que hará que se sienta mucho más cómodo, identificado y atraído por esa oferta.


### 472.E1_Introducción_a_la_Inteligencia_Artificial-video

### Introducción a la Inteligencia Artificial
![[472.E1_Introducción_a_la_Inteligencia_Artificial.mp4]]
[Introducción a la Inteligencia Artificial](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41865984-u1-1-2-introduccion-a-la-inteligencia-artificial-jose-peris)

Para entender otro concepto importante en Machine Learning, para poder seguir hacia adelante, debemos saber que este campo se centra mucho en el estado del arte. ¿Qué es el estado del arte? El estado del arte lo definiríamos como en qué punto nos encontramos, cuál es la última tendencia, cuál es el último avance, cuál es el último hito que se ha alcanzado tecnológico. Para que os hagáis una idea, todo esto empezó con la programación tradicional. basada en los bucles, por ejemplo, if else, son condiciones muy simples y bucles. Pero posteriormente apareció el machine learning, que fue el primer gran salto. Machine learning lo que hizo fue poder desarrollar algoritmos y tiene una característica aquí que, bueno, puede trabajar con big data, pero puede trabajar también con menos datos. Por otro lado, machine learning es muy. útil porque permite muchísima explicabilidad de las características del algoritmo. Más adelante veremos un poco qué es todo esto. Posteriormente llegó el deep learning, que al final, como hemos comentado, lo que hace es copiar el funcionamiento del cerebro humano. Sí que es cierto que deep learning es mucho más atractivo y es más avanzado a nivel de estado del arte, pero tenemos un handicap donde tenemos que trabajar con cantidades de datos mucho más elevadas para obtener resultados optimales. Después del Deep Learning vino el Automation Learning. Esto ya prácticamente rizar el rizo, porque estamos hablando de inteligencia artificial que hace inteligencia artificial. Esta ventaja sobre todo se han visto beneficiados o perjudicados, depende de cómo lo miremos, los desarrolladores. ¿Por qué? Porque lo que hace es aplicar inteligencia artificial a procesos de la elaboración del algoritmo que son muy tediosos. Al final, en pocas palabras, lo que hace es simplemente nos permite optimizar el tiempo y ser mucho más ágiles, aunque es cierto que no porque se optimice todo podamos utilizarla sin tener conocimientos de este campo, que sería el data science. Y ya por último, lo que sería el estado del arte de hoy, tendríamos los transformers. Los transformers ha sido la disrupción más fuerte que hemos tenido en 2021. y que veremos a continuación. Cuando hablamos de Transformers, hablamos de GPT-3. GPT-3, en una frase, es el chat más inteligente del mundo. Estamos hablando de hitos que eran impensables alcanzar hace unos años. ¿Por qué? Porque GPT-3 ha entrenado con millones de librerías de Wikipedia, de todos los libros que existen casi disponibles online. Y claro, todos tenemos la imagen del chatbot, que le haces cuatro preguntas y se quita el chatbot. equivoca la tercera o no tiene coherencia en sus respuestas. GPT-3 es un producto de OpenAI, y detrás de OpenAI está Elon Musk, que sabemos ya que es el chat más inteligente del mundo, ¿por qué? Porque es capaz de coger la personalidad, por ejemplo, de un personaje histórico. Es decir, en GPT-3 puedes hablar con Albert Einstein y tener una conversación coherente, tener un diálogo totalmente coherente con él, pero él va a hablar siempre con su lenguaje y con sus palabras. En el mismo chat puedes añadir a Elon Musk, o puedes añadir a Gandhi, o puedes añadir todos esos personajes públicos que hay mucha literatura, en la red, al haber mucha literatura sobre ellos, hay datos, datos para el entrenamiento. Pero la ventaja de los Transformers, y es algo que se utiliza mucho en este campo, es que es un campo muy líquido, es decir, el conocimiento que se descubre en el campo, por ejemplo, del habla, o de la escritura, se puede tener un buen conocimiento. trasladar a la imagen. GPT-3 nació en 2021, la versión beta se liberó. Y, bueno, estamos hablando también que al estar entrenado de esta forma tan eficiente, llega el punto al que tú puedes pedirle, por ejemplo, a GPT-3 que te diseñe una página web, que tenga dos botones en el centro, uno de color rojo y otro de color negro. Y no es solo que te la va a generar, sino que te va a generar el código. Es capaz de programar en casi cualquier lenguaje. de programación. Simplemente tienes que pedirle que te haga una función matemática que cumpla una serie de condiciones y lo que va a hacer es generarte el código. y después con este código puedes ejecutar este programa y te va a funcionar. Estamos hablando de un salto realmente muy, muy potente. Esta es la potencia de GPT-3, que la podéis comprobar en el Playground, que os dejamos aquí adjunto el enlace para que podáis jugar. Simplemente tenéis que registraros con vuestra cuenta de Google. Podéis comprobar que incluso le podéis pedir que os escriba un ensayo entre Platón y Aristóteles hablando sobre el concepto del amor. una longitud de carácteres que vosotros deséis. Y os va a establecer, os va a crear un ensayo coherente. Incluso le podéis pedir que cogeis los ingredientes que tenéis en la nevera y pedir que os haga una receta. Con esos ingredientes yo os la va a hacer. Por supuesto, no es perfecto al 100%. Estamos hablando de una versión beta, pero realmente ha llegado a unas cotas que eran impensables hace unos años. Como os decía, este campo es muy líquido. Y cuando llegan sorpresas suelen llegar juntas, porque lo que hay es mucha capilaridad entre proyectos. También apareció Dalí. obra de OpenAI, donde rompió el paradigma también de lo que es la visión artificial, porque ya estamos hablando que aquí se ha juntado el conocimiento que se ha avanzado en el tema del habla y la escritura, con el conocimiento de la imagen. ¿Qué significa todo esto? Que simplemente mediante palabras podemos crear, por ejemplo, objetos o conceptos gráficos que nunca han existido. que no están en ninguna base de datos. Simplemente, esta combinación de estos dos algoritmos, esto podéis ver, es un ejemplo de la narrow intelligence, como cuando se combinan, pueden llegar a algo mucho más amplio o mucho más elevado. La realidad es que los resultados también son fascinantes, pero sí que es cierto que con Dalí hubo un impasse donde se frenó el desarrollo, Y bueno, vino un desarrollo por otro lado con Glyde. Glyde lo que ha hecho es mejorar todo esto y estamos hablando de que si tú quieres a Glyde, le puedes pedir también que te dibuje una ilustración de Albert Einstein con un vestido de Superman y te la va a hacer. O que le pidas que te dibuje a un gato jugando al ajedrez con el estilo de Dalí. y te lo va a ejecutar. Es decir, imágenes imposibles, imágenes que nunca han existido, porque al final se combina la semántica con la imagen. Pero bueno, esto estamos hablando de, como os decía, del estado del arte. Esto es el punto máximo. Y también comentaros que algunos de estos algoritmos sí que están disponibles para testar la versión beta o tenemos el código ya. Pero, ¿cómo se hace? otros, pues aún no, simplemente se han utilizado para mostrar el potencial de dónde podemos llegar. Por ejemplo, ¿qué puede hacer por nosotros Glide? Para mí, Glide puede hacer algo totalmente fascinante, como es el uncrapping. El uncrapping no es más que tú, imagínate que le tomas una foto a tu amigo o tu amiga, y el uncrapping lo que te hace es, mediante algoritmos de visión artificial, lo que te hace es que te reconstruye, te amplía la escena. Él mismo se va inventando la escena que continuaría, con lo cual nos amplía el campo. El punto en el que nos encontramos ahora es muy excitante, simplemente por el hecho de que comienzan a confluir las tecnologías. Por un lado tenemos las tecnologías descentralizadas y por otro lado tenemos inteligencia artificial. Esto a que nos lleva a que podemos generar NFTs simplemente por la tecnología. utilizando código de inteligencia artificial. Es por ello que algoritmos como vkugan más clip nos están permitiendo generar aquí lo que nosotros queramos simplemente incluyendo tres, cuatro palabras. Es decir, podemos pedirle que nos genere un animal en un estilo modernista, en una... escena del universo. Estamos hablando de imaginación, de creatividad y si os fijáis casi siempre os hablo de que es VQGAN más clip, estamos hablando de la suma de dos algoritmos, del ensamblaje. ¿Por qué? Porque lo que hacemos es unir, unir características de diferentes algoritmos. En este caso, lo que estamos uniendo es Clip, por ejemplo, que lo que hace es un clasificador que describe de forma muy exacta la imagen que está viendo a un nivel de detalle muy elevado. Es decir, no es lo mismo detectar que la imagen que está viendo un algoritmo de visual artificial es un coche o una bicicleta que detecte que, por ejemplo, está viendo una imagen de un husky siberiano sobre una vidriera. Estamos hablando de niveles de detalle muy diferentes. Este avance en la semántica ha permitido, al fusionarlo con la imagen, llegar a cotas muy elevadas. Lo mismo que VQGAN, que viene de las GANs, Generative Adversarial Networks, que lo que hace a nivel de imagen es comprender la profundidad de la imagen que está viendo. Imaginaros que estamos viendo una imagen de un pájaro o de un perro, lo que hace es calcular este vector de profundidad y entiende que esto es un animal que se llama pájaro que puede estar representado con plumas de diferentes colores o con picos de diferentes formas. Lo mismo para un perro, puede tener diferentes texturas, diferentes tipos de arrugas. Claro, todo esto nos lleva a que realmente lo que vamos a ver, y de hecho lo vais a practicar, os vamos a dejar aquí... una pieza del código donde podéis crear vuestro NFT a partir de las palabras que vosotros creáis y os lo convierte a vídeo, también os lo anima. Claro, todo esto realmente... ha pasado a partir del 2021, venía con mucho recorrido detrás. Por tanto, este es el estado del arte de hoy. Como os comentaba también, Automachine Learning básicamente se basa en automatizar las tareas del Data Scientist. ¿Y qué significa eso? Básicamente, de una forma muy genérica, lo que hace un científico de datos es, primero, recopilar datos, gran cantidad de big data para resolver un problema y lo que hace pues bueno estos datos se tienen que limpiar, se tienen que preparar, porque al final estos datos muchas veces están rellenados por personas que han rellenado una base de datos y pueden haber insertado un valor nulo, por ejemplo esto hay que limpiarlo, es un proceso que se llama ETL que significa Extract, Transform, Unload y AutoML ya permite agilizar este proceso. Estamos hablando que un algoritmo se puede desarrollar o diseñar en en horas o en semanas o en meses o en años dependiendo de la complejidad a la que nos enfrentemos. Por tanto, si cogemos una una métrica estándar, como puedan ser dos o tres meses, el proceso de TL representa el 80%. y el proceso de selección del algoritmo puede representar el 20, 30, depende del problema que estemos afrontando. ¿Qué hace el AutoML? En estas dos partes más tediosas nos automatiza los procesos y al final lo que nos va a hacer es limpiar de forma más o menos automatizada y nos va a hacer una clasificación de qué algoritmos son los mejores que encajan para tu problema. Con lo cual, al final, esto se trata de inputs y outputs. Entonces, el input son los datos y el output que te va a dar el AutoML es limpieza de los datos y un ranking con los mejores algoritmos para resolver tu problema. El tercer punto es el deep learning. Deep learning, como os he comentado, se trata en trabajar... con capas y con números de neuronas. Al final, pues imaginaros un clasificador. Hemos dicho que la inteligencia artificial predice, clasifica y agrupa. Imaginaros que, mediante visión artificial, queremos hacer un clasificador que sea capaz de clasificar entre lo que es un perro y un gato. ¿Cómo funciona el deep learning? Pues bueno, el deep learning lo que hace es imaginaros, establecemos... una serie de capas con un número de neuronas donde cada capa de neuronas es encargada de realizar una función específica. Imaginaros que las primeras capas lo que van a hacer es analizar de lo general a lo concreto. Es decir, las primeras van a analizar, van a buscar una forma de una cabeza de un perro o una forma de una cabeza de un gato. Al final, aquí lo que se hace es una técnica que se llaman embeddings, que es... traducir los píxeles a vectores, a unos y ceros, porque los algoritmos se lo entienden de unos y ceros. Entonces, lo que hace es, en una primera instancia, reconoce la forma del animal de forma genérica, después pasaría a analizar la cabeza, una estructura de ojos, nariz y boca, después pasaría a analizar ya diferentes narices y después pasaría a analizar las curvas, que son los vectores que están marcando el contorno. y al final decidiría si es un gato o es un perro. Todo esto, ¿qué ventaja tiene? Que funciona con backpropagation. Para que os hagáis una idea, backpropagation lo que significa es que la red neuronal va aprendiendo... va hacia adelante y en las neuronas hay lo que llamamos los pesos, que es una función matemática que es Matching Learning, y lo que va haciendo es hacer una iteración hacia adelante, comprueba el resultado, aquí cuando hablamos de resultado hablamos de precisión, siempre medimos la precisión del algoritmo, una precisión máxima casi sería un 95-97, el 100% no existe, y lo que va haciendo es, imaginaros, obtiene una precisión del 80, cuando vuelve a hacer otra pasada hacia detrás, actualiza los pesos y los optimiza hasta ir subiendo la precisión de forma iterativa mediante lo que llamamos Epochs. Este es un poco el concepto general de Deep Learning, sí que quiero que os quede claro que en Deep Learning necesitamos trabajar con miles de datos, estaríamos hablando a lo mejor 50.000 filas si estuviésemos hablando de datos tabulares. Pero sí que es cierto que el deep learning a nivel de imagen, cuando estáis viendo, por ejemplo, el caso de los deepfakes, es un caso de visión artificial muy común. Y lo mismo, cuando tú estás viendo un deepfake, hay un algoritmo que está reconociendo todo lo que es el contorno de los ojos, la nariz y la boca, y lo que está haciendo es mapeando otra imagen, está analizando los dos patrones y fusionándolos. Esto es un poco la tecnología que hay detrás, debo explicarlo de una forma sencilla, pero claro, Deep Learning también estaréis viendo en el tema, por ejemplo, trasladándolo otra vez a la descentralización, por ejemplo, estaréis viendo NFTs creados con estilos, con estilos similares a Van Gogh, a Dalí, incluso a Picasso. Otra característica que tiene el Deep Learning es el Transfer Style, es decir, que puede transferir un estilo. Todo esto... en fases primarias es lo que habéis visto también en filtros de diferentes aplicaciones o software de fotografía. Como hemos dicho, Machine Learning, yo casi que diría que es mi favorita, y la realidad es que es mi favorita por un factor, por la explicabilidad. Aquí tendremos que entender una cosa, los algoritmos se miden con la precisión, ¿Qué significa esto? Que, por ejemplo, si yo quiero saber el negocio de mi cliente cuánto va a facturar de aquí a dos semanas, y hacemos un algoritmo predictivo que nos emita esta predicción de si va a facturar este día 2.500 euros, y la realidad nos dice que facturó 2.400 euros, pues estaríamos hablando de una... de una cura así o una precisión elevada, ¿no? 90, 92 por 100. Bien, esto es interesante, ¿no?, porque nos permite anticiparnos. Si yo sé que mi negocio de aquí a dos semanas va a facturar 2.400 euros, pues me está indicando que necesitaré más materias primas, más trabajadores, etcétera. Esto, aunque se traduce en acción y en ventaja competitiva. Pero, ¿qué pasa si yo tengo una previsión, Machine learning tiene otra característica, que es la explicabilidad. Y esto es muy importante. Porque, por ejemplo, para hacer este algoritmo de predicción de cuánto voy a vender de aquí dos semanas, le hemos insertado a nivel de datos de Big Data, que es nuestra gasolina, todas las variables que afectan a nuestro negocio. Una variable podría ser si hizo una promoción o no. Otra variable podría ser número de trabajadores que tenía ese día, otra variable podría ser, pues, número de clientes, etcétera. Entonces, ¿qué ocurre? Que aparte de haceros una predicción, os va a decir el feature importance. Y el feature importance, lo que significa es que voy a saber cada variable cuánto afecta a ese resultado. Es decir, puedo llegar a saber que no es que sepa que vaya a 2.400 euros ese día. Es que sé que las variables que más afectan son el que haya una promoción o no al 82%, el que hayan 10 trabajadores al 72% y así sucesivamente. Es decir, nos habla a nivel de negocio de las palancas que tenemos que activar para mejorar nuestras métricas. Con todo esto ya hemos hecho un recorrido del estado del arte, de la historia de cómo pasamos de la programación básica a los transformers. Muchos alumnos me suelen preguntar qué es mejor, si machine learning, si deep learning o si en un momento dado se puede utilizar programación tradicional o si hay que utilizar un transformer. Y la respuesta es que es más fácil. siempre es la misma. Depende de la naturaleza de tu problema, depende del caso de uso y depende sobre todo de tus datos. Como he dicho al principio, los datos hablan sobre historias, por tanto una vez entendida la historia seremos capaces de decidir qué herramienta es la que mejor encaja para nuestro caso de uso.


### 473.E1.U1.3_Tipos_de_IA

### Tipos de IA
Para entender dónde estamos en el campo de la inteligencia artificial, hay que entender muy bien qué pasó en el año 2021, porque realmente hubo un salto muy grande que cambió realmente el paradigma del avance de esta tecnología. 

Existen tres tipos de inteligencia artificial: 
1. Narrow Intelligence
2. IA General
3. Super IA

![[473.E1_Tipos_de_IA_1.png]]

#### 1 | Narrow Intelligence
Narrow Intelligence (inteligencia estrecha), es la que experimentamos cada día todos en nuestro Smartphone. 
- ¿Habéis intentado escribir una palabra y os hace una predicción de texto? Esto sería predicción, de ahí **Texto Predictivo**.
- Todos también hemos intentado hacer una foto con nuestro smartphone y nos marca la cara. Esto es una **Bounding Box.** 
- Todos hemos utilizado, alguna vez, el **Traductor de Google.** 

Esto serían tres casos, muy conocidos, de inteligencia artificial estrecha, donde la principal característica es que se entrenan a los algoritmos para resolver funciones concretas específicas.

#### 2 | IA General
El siguiente estadio de la inteligencia artificial es la general. Aquí estaríamos hablando de que la propia inteligencia artificial ya tiene un número tan elevado de funciones implementado y se relacionan de forma tan eficiente entre ellas que ya diríamos que puede tener consciencia propia.

Realmente nadie sabe en qué punto estamos, sabemos que estamos en los comienzos, pero no sabemos exactamente en qué punto preciso. ¿Por qué? Porque este es un mundo que se apoya muchísimo en la comunidad, en la investigación y continuamente se van publicando papers a nivel mundial. 

Ha habido mucha innovación en este campo y se está acelerando mucho su desarrollo pero, por el momento, no podemos afirmar que hayamos alcanzado la inteligencia artificial general.

#### 3 | Súper IA
La tercera categoría sería la Súper Inteligencia Artificial, que se conectaría en todas estas inteligencias artificiales generales. 

Se ha escrito y hablado mucho sobre este punto, sobre cómo puede afectar a la humanidad. No obstante, siempre desde un punto de vista de ciencia ficción con matices post-apocalípticos. 

![[473.E1_Tipos_de_IA_2.png]]

#### El Estado del Arte
El estado del arte lo definiríamos como el punto en el que nos encontramos, ¿cuál es la última tendencia, cuál es el último avance, cuál es el último hito que se ha alcanzado?

Para que os hagáis una idea, todo esto empezó con la programación tradicional basada en los bucles. 
- Por ejemplo, “if-then” son condiciones y bucles simples con lógica aplicada, pero posteriormente apareció el Machine Learning, que fue el primer gran salto. 
- Machine Learning permitió desarrollar algoritmos capaces de trabajar con Big Data, pudiendo también adaptarse a menores volúmenes más tradicionales. 
- Unos años más tarde llegó el Deep Learning, que al final, como hemos comentado, lo que hace es copiar el funcionamiento del cerebro humano. 
- Sí que es cierto que Deep Learning es mucho más atractivo y es más avanzado a nivel de estado del arte, pero tenemos un handicap donde tenemos que trabajar con cantidades de datos mucho más elevadas para obtener resultados óptimos.

**Después del Deep Learning vino el Auto-Machine Learning.** Esto ya prácticamente es rizar el rizo porque estamos hablando de inteligencia artificial que hace inteligencia artificial. Al final, en pocas palabras, nos permite optimizar el tiempo y ser mucho más ágiles. Aunque es cierto que no porque se optimice todo podamos utilizarlo sin tener conocimientos de este campo que sería el Data Science.

##### Transformers - ChatGPT-3 y Dall-e
Por último, lo que sería el estado de arte del arte de hoy, tendríamos los Transformers. 
- Estos, han supuesto la disrupción más fuerte que hemos tenido en los últimos años. Cuando hablamos de Transformers hablamos de **ChatGPT-3,** el chat más inteligente del mundo. 
- Estamos hablando de hitos que eran impensables alcanzar hace unos años. GPT-3 se ha entrenado con millones de librerías de Wikipedia de todos los libros que existen casi disponibles online. Y claro, todos tenemos la imagen del chatbot que le haces cuatro preguntas y se equivoca a la tercera o no tiene coherencia en sus respuestas. GPT-3 es un producto de OpenAI y detrás de OpenAI está Elon Musk. 
- Es el chat más inteligente del mundo porque es capaz de tomar la personalidad de un personaje histórico, es decir, gracias a GPT-3, puedes hablar con Albert Einstein y tener una conversación coherente. Pero él va a hablar siempre con su lenguaje y con sus palabras, pero en el mismo chat también puedes añadir a Elon Musk, o a Gandhi, o a cualquier personaje público sobre el que haya mucha literatura y referencias. 
- La versatilidad de GPT-3 es increíble y esto es solo el comienzo. Puedes pedirle, por ejemplo, que te diseñe una página web que tenga dos botones en el centro de uno de color rojo y otro de color negro. Y no solo la va a diseñar sino que la va a generar en código. El código es capaz de programar en casi cualquier lenguaje de programación, simplemente tienes que pedirle que te haga una función matemática, que cumpla una serie de condiciones y lo que va a hacer, lo que vas a hacer es generarte el código y después con este código puedes ejecutar este programa y te va a funcionar. Estamos hablando de un salto realmente potente.

Podéis experimentar con GPT-3 a través de [este enlace](https://chat.openai.com/chat).
- Simplemente, tenéis que registrarnos con vuestra cuenta de Google. 
- Podéis comprobar que incluso le podéis pedir que os escriba un ensayo entre Platón y Aristóteles hablando sobre el concepto del amor con la longitud de caracteres que vosotros deseéis y os va a crear un ensayo coherente. 
- Incluso le podéis pedir que coja los ingredientes que tenéis en la nevera y pedir que os haga una receta con esos ingredientes y os la va a hacer. Por supuesto, no es perfecto al 100%. 
- Estamos usando una versión beta, pero realmente ha llegado a unas cuotas que eran impensables hace unos años. Como os decía, este campo es muy líquido y cuando llegan sorpresas suelen llegar juntas porque lo que hay es mucha capilaridad entre proyectos.

###### Dall-e
También apareció Dall-e, obra también de OpenAI, y ahí es donde rompió el paradigma también de lo que es la visión artificial, porque ya estamos hablando que aquí se ha juntado el conocimiento, que se ha avanzado en el tema del habla y la escritura con el conocimiento de la imagen.

![[473.E1_Tipos_de_IA_3.png]]

¿Qué significa todo esto? Que simplemente mediante palabras podemos crear, por ejemplo, objetos o conceptos gráficos que nunca han existido, que no están en ninguna base de datos. Y la realidad es que los resultados también son fascinantes. 

Actualmente, Dall-e se mejoró dando pie a Dall-e 2, mucho más potente y con capacidades mucho mayores tanto creativas como semánticas.

En paralelo y para mejorar el modelo de Dall-e se desarrolló GLIDE (Guided Language-to-Image Diffusion for Generation and Editing). Este algoritmo no solamente puede crear imágenes totalmente nuevas a partir de texto, sino alterar imágenes añadiendo objetos o texturas. A pesar de un entrenamiento más modesto (GLIDE está entrenado con 3.5 mil millones de parámetros en comparación a Dall-e que cuenta con 12 mil millones), es un algoritmo más eficiente y el cual necesita menos muestras para entender el objetivo.


### 475.E1.U1.5_Machine_Learning_y_Explicabilidad

### Machine Learning y el concepto de Explicabilidad
#### ¿Qué es la Explicabilidad?
Imaginemos que queremos saber cuánto va a facturar el negocio de nuestro cliente durante las próximas dos semanas. 
- Creamos un algoritmo predictivo que nos emita esta predicción que calcula que va a facturar 2.500 € y la realidad nos dice que facturó 2.400 €.
- Estaríamos hablando de una precisión elevada, 90%-92%.

Esto es interesante, porque nos permite anticiparnos.  
Si yo sé que mi negocio de aquí a dos semanas va a facturar 2.400 €, me está indicando que necesitaré:
1. Más materias primas. 
2. Más trabajadores.
3. Etc.

Esto se traduce en acción y en ventaja competitiva.

#### Machine Learning tiene otra característica: La explicabilidad
Para hacer este algoritmo de predicción de venta a dos semanas vista, le hemos insertado a nivel de datos de Big Data, que es nuestra gasolina. Todas las variables que afectan a nuestro negocio. ¿Cuántas?. 
- **Variable 1:** ¿Se hizo una campaña promocional?.
- **Variable 2:** Número de trabajadores que tenía disponibles.
- **Variable 3:** Número de clientes, etc.

¿Qué ocurre entonces? Que aparte de hacer una predicción, os va a decir el **Future Important.** 
- Vamos a saber cómo afecta cada variable a ese resultado. 
- Ya no solo sabremos que hemos vendido 2.400 € ese día, también las variables que más afectan son el que haya una promoción o no, al 82%, o el que tenga diez trabajadores, al 72%, y así sucesivamente.
- Nos dice, a nivel de negocio, de las palancas que tenemos que activar para mejorar nuestras métricas. 

Con todo esto ya hemos hecho un recorrido del estado del arte, de la historia, de cómo pasamos de la programación básica a los Transformers.

#### ¿Machine Learning, Deep Learning, programación tradicional o Transformers?
La respuesta siempre es la misma: Depende de la naturaleza de tu problema, del caso de uso y, sobre todo, de tus datos.

Los datos ahora hablan sobre historias. Por tanto, una vez entendida la historia, seremos capaces de decidir que herramienta es la que mejor encaja para nuestro caso de uso.


### 476.E1_Tratamiento_del_Dato-video

### Tratamiento del Dato
![[476.E1._Tratamiento_del_Dato.mp4]]
[Tratamiento del Dato](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora)

Dentro del mundo de data se considera un concepto que se llama pipeline, en el cual... hay diferentes fases. Un ETL consiste en extracción, transformación y carga. Extraction, transform and load. Entonces la fase de extracción normalmente corresponde a una serie de scripts o tareas que normalmente suelen leer unas bases de datos y cargan toda esa información en memoria y en memoria lo que se hace es mediante... en general se usa Python para esto pero también SQL, se hacen unas transformaciones de estos datos que se leen. Por ejemplo, puedes tener... diferentes tablas en una base de datos y quieres unirlas, pues todo eso se hace en ese momento. Es el momento de solucionar esos problemas, porque luego se hace una carga dentro de un Data Warehouse en general, donde no se permiten las ediciones. Entonces cuando van todos los datos al Data Warehouse ya están preparadas para ser consumidas por especialistas de todo tipo, sean analistas, sean CEOs, sean CCOs. Ahora mismo se ha puesto bastante de moda el LT, que se llama básicamente... porque el precio de almacenaje ha bajado bastante en los últimos años y también la potencia de cómputo de los data warehouse. Entonces lo que se hace es extraer como se hace en la ETL pero se hace transformación dentro del propio data warehouse, se utilizan diferentes herramientas pero ahora mismo la que está más en auge es DBT que utiliza lenguaje SQL para conseguir este tipo de transformaciones. se carga como siempre otra vez en el mismo Data Warehouse. Aunque puedas ver tablas tanto en un Data Warehouse como en una base de datos relacional tradicional, las operaciones que puedes hacer sobre ellas varían bastante. Por ejemplo, de normal se utiliza un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, tener inserciones o borrados de forma continua. Sin embargo, en el Data Warehouse todas estas operaciones no están permitidas, por ejemplo, en general no se permite la edición de filas. Realmente no se hace una edición, sino que lo que se hace es añadir fila. O, por ejemplo, tampoco se permiten los borrados a la ligera, aunque en algunas tecnologías sí que se permiten. En general, el coste por almacenamiento en un Data Warehouse es mucho menor que en una base de datos tradicional. En general un Data Lake también permite almacenamiento masivo, pero el dato está totalmente desestructurado y es ideal para guardar información que nos llegue en forma de eventos, que pueden tener cualquier estructura interna y pueden estar totalmente desordenados. Muchos se pueden preguntar dónde podemos usar un AppIreland tipo ETL o un AppIreland tipo ELT. Realmente no hay tanta diferencia a nivel de... que se puede conseguir, porque al final siempre estamos cargando los datos a un sitio para que sean consumidos, pero realmente sí que hay que valorar esfuerzo económico y esfuerzo de tiempo de desarrollo y de reacción al cambio. Entonces, por ejemplo, tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda y todos esos datos llegan a nuestra pipeline. son extraídos y luego aquí viene la gran diferencia. ¿Qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente? Si cargamos estos datos, realmente tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no nos esperamos, a lo mejor tienen un campo que realmente no es correcto o tienen un campo que realmente no está dentro del esquema que nosotros estamos buscando. Entonces, una buena medida, transformarlos antes. de cargarlos simplemente por asegurarnos de este tipo de errores pero al mismo tiempo también queremos almacenarlos en un sitio ahorranos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar entonces es un tema de... económico y también de potencia de cómputo y de rapidez de la gestión del dato que diferentes negocios pueden valorar de diferentes maneras. En general ahora mismo el modelo ELT es más barato que el ETL. En Internex empezamos usando el modelo ETL pero rápidamente nos dimos cuenta que tenía mucho sentido migrar a un aparitivo tipo ELT básicamente porque nos costaba mucho más económico todas las gestiones que queríamos hacer y la potencia de las nuevas tecnologías como DBT. junto a BigQuery, en este caso es que usamos BigQuery, era una combinación que prácticamente toda la industria data estaba utilizando y la comunidad también se estaba moviendo a desarrollar nuevos conectores, documentación que nos podía ayudar a poder distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente la pipeline que teníamos. No solo eso, también teníamos un problema de que realmente nuestra extracción de datos estaba bastante acoplada directamente guardar en BigQuery, entonces guardar primero y luego transformar tenía mucho sentido. El primer paso de un aparellante de data siempre es la extracción, la mayor parte de veces se hace desde bases de datos pero no siempre es el caso, muchas veces quieres extraerlo de acciones que toman los propios usuarios, pueden ser clics, pueden ser cualquier un tipo de... Gizmos, cualquier cosa, entonces la extracción desde este tipo de fuentes también es importante. Hoy en día se utilizan CDPs para realizarlo y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, porque muchas veces se pierden eventos, se pierde información por el camino, entonces este challenge de tener un sistema robusto. Durante la extracción de datos. es importante tener un esquema prefijado porque esto nos ayudará a que luego el dato en nuestro Data Warehouse esté de una manera clara para el resto del equipo. Entonces lo que se suele hacer es, antes de la ingesta, establecer un protocolo de, oye, me espero estos datos con estos campos, con estos tipos y cuando pase por nuestra Pipeline todo lo que no entre dentro de ese marco lo podemos descartar o hacer una transformación para que entre dentro de ese marco. ¿Por qué? Porque en el mundo real se... pasan un millón de cosas y estamos sujetos a caídas de diferentes sitios o servicios, estamos sujetos a posibles ataques, entonces tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. Se suele decir que el 70 o 80% del trabajo de un data scientist es limpiar datos y por eso no es una... una cuestión trivial. Cuando llegamos a la visualización del dato, muchas veces el equipo de edita tiene que reportar al equipo directivo o a otro departamento. Y es importante tener claras qué herramientas de visualización se van a utilizar y sobre todo, qué implica, cuánto tiempo de trabajo implica utilizar esas herramientas, porque la visualización no es trivial, muchas empresas tienen un departamento exclusivo para ello tenemos que optar un poco entre tener un equipo para ello o usar una herramienta. Vas a tener que hacer consultas de SQL, vas a tener que tener algún experto en usabilidad y que pueda decir, oye, aquí van estos colores, aquí quiero presentar la información de esta manera o de esta otra manera y tienes que tener claro cuáles son los gastos que vas a cobrar. a tener que hacer en los diferentes casos. Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redax que te permiten hacer más cosas, pero son mucho más costosos y necesitas probablemente un equipo que esté dedicado a ello.


### 477.E1.U2.1_Tratamiento_del_Dato

### ¿Qué es una ETL?
Dentro del mundo del Data, es un concepto denominado Pipeline, en el cual encontramos diferentes fases.

Una ETL consiste en Extracción, Transformación y Carga (_Extraction, Transform and Load_).

![[477.E1_Tratamiento_del_Dato.png]]

#### 1 | Extracción
- La fase de extracción, normalmente, corresponde a una serie de scripts o tareas que normalmente suelen leer en las fases de datos y cargan toda esa información en la memoria. 
- En general, se utiliza Python, pero también SQL. 
- Esos datos son leídos para posteriormente ser transformados. 
- Te encontrarás diferentes tablas en una base de datos y dichas tablas pueden tener valores nulos. 
- Como es necesario unir todas esas tablas, debes corregir todos esos valores nulos y otros problemas antes de hacer una carga en Data Warehouse, ya que una vez dado este paso, no se permiten ediciones.

Al trasladar todos estos datos al Data Warehouse, ya están listos para ser consumidos por especialistas de todo tipo, ya sean analistas, CEOs o CTOs. 

#### ETL y ELT | Diferencias
ELT (Extract, Load & Transform) se ha puesto bastante de moda por el bajo precio del almacenaje durante estos últimos años, así como por la potencia de cómputo de los Data Warehouse.
- Lo que se hace es extraer datos, como en la ETL, pero la transformación se realiza dentro del propio Data Warehouse. 
- Se utilizan diferentes herramientas, pero ahora mismo la que está más en auge es DBT, que utiliza el lenguaje SQL para conseguir este tipo de transformaciones. 
- Por último, se carga otra vez en el mismo Data Warehouse.

#### Data Warehouse y Base de Datos | Diferencias
Podemos encontrar tablas, tanto en un Data Warehouse, como en una base de datos tradicional. Lo que realmente las diferencia son las operaciones que puedes realizar sobre ellas. 

Normalmente, se usa un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, inserciones y borrados de forma continua. 

Sin embargo, en el Data Warehouse todas estas operaciones no están permitidas:
- **No se permite la edición de filas.** Realmente no se hace una edición, si lo que se hace es añadir fila. 
- **No se permiten los borrados de forma habitual,** aunque en algunas tecnologías sí que se permiten. 
- **El coste por almacenamiento en un Data Warehouse es mucho menor** que en una base de datos tradicional.

#### Data Lake
Un data lake permite almacenamiento masivo, pero los datos están totalmente desestructurados. 

Es ideal para guardar información que nos llega en forma de eventos, que pueden tener cualquier estructura interna y pueden ser totalmente desordenados.

#### ETL y ELT | Casos de uso
No existe tanta diferencia entre lo que podemos llegar conseguir utilizando cualquiera de estas dos Pipelines. 

Al final siempre estamos cargando los datos a un sitio para que se sean consumidos. 

Lo que hay que valorar es: 
1. Esfuerzo económico 
2. Tiempo de desarrollo 
3. Tiempo de reacción al cambio 

Tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda y todos esos datos llegan a nuestra Pipeline.
- Son extraídos y, aquí viene la gran diferencia, ¿qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente?. 
- Si cargamos estos datos, tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no nos esperamos, que puedan contener un campo que no es correcto o tienen un campo que realmente no está dentro del esquema que nosotros estamos buscando.
- Entonces, una buena medida es transformarlos antes de cargarlos, simplemente para asegurarnos de no tener ese tipo de errores. 
- Pero al mismo tiempo, también queremos almacenarlos en un sitio, ahorrarnos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar. 
- Entonces, es un tema económico y también de potencia de cómputo y de rapidez de la gestión del dato, que diferentes negocios pueden valorar de diferentes maneras. 
- Ahora mismo el modelo ELT es más barato que el ETL.

#### El Caso de Internxt
Empezamos utilizando un modelo ETL, pero rápidamente nos dimos cuenta de que tenía mucho sentido migrar a un Pipeline tipo ELT, básicamente porque resultaba mucho más económico para todas las gestiones que queríamos hacer.

La potencia de las nuevas tecnologías como DBT junto a [BigQuery](https://cloud.google.com/bigquery/?utm_source=google&utm_medium=cpc&utm_campaign=emea-be-all-en-dr-bkws-all-all-trial-e-gcp-1011340&utm_content=text-ad-none-any-DEV_c-CRE_574561847454-ADGP_Hybrid%20%7C%20BKWS%20-%20EXA%20%7C%20Txt%20~%20Data%20Analytics%20~%20BigQuery%23v6-KWID_43700072681098079-kwd-12297987241-userloc_2056&utm_term=KW_big%20query-NET_g-PLAC_&gclsrc=ds&gclsrc=ds&gclid=CJrW0POoxPwCFWdCHQkdj-8PxQ) (en este caso que usamos BigQuery) era una combinación que prácticamente toda la industria Data estaba utilizando y la compañía también se estaba moviendo a desarrollar nuevos conectores, documentación que nos podía ayudar a poder distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente el Pipeline que teníamos.

No solo eso, también teníamos un problema: Nuestra extracción de datos estaba bastante acoplada a directamente guardar en BigQuery.

> Entonces, almacenar primero y luego transformar tenía mucho sentido. El primer paso de un Pipeline de data siempre es la extracción.

##### Otras fuentes de datos
- La mayor parte de veces, la extracción se genera desde bases de datos, pero no siempre es el caso. También se pueden extraer de acciones que realizan los propios usuarios, como clics, heatmaps, puede ser cualquier cosa. 
- La extracción desde este tipo de fuentes también es importante. 
- Hoy en día se utilizan CDPs para realizarlo y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, ya que, muchas veces, se pierden eventos e información por el camino. Entonces, el challenge es tener un sistema robusto. 

Durante la extracción de datos es importante tener un esquema prefijado, esto nos ayudará a que luego el dato en nuestro Data Warehouse esté reflejado de una manera clara para el resto del equipo. 

Entonces, antes de la ingesta de datos, se establece un protocolo: 
1. Esperar con estos datos, campos y tipos, y, cuando pase por nuestro Pipeline, todo lo que no entre dentro de ese marco, lo podemos descartar o hacer una transformación para que entre dentro de ese marco.
2. En el mundo real pueden suceder un millón de cosas; estamos sujetos a caídas de diferentes sitios o servicios o a posibles ataques. Entonces, tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. 
3. Se suele decir que el 70%-80 % del trabajo de un Científico de Datos es limpiar esos datos, por eso no es una cuestión trivial. 

Cuando llegamos a la visualización del dato, muchas veces, el equipo de data tiene que reportar al equipo directivo o a otro departamento. Es importante tener claras qué herramientas de visualización se van a utilizar y sobre todo, qué implica. 

¿Cuánto tiempo de trabajo implica utilizar esas herramientas? La visualización tampoco es trivial. Muchas empresas tienen un departamento exclusivo para ello y tenemos que optar un poco en detener un equipo para ello o usar una herramienta. 

Vas a tener que hacer consultas de SQL, vas a tener que tener algún experto en usabilidad y que pueda decir: Aquí van estos colores, aquí estos otros, y aquí quiero presentar la información de esta manera o de esta otra manera. Y tienes que tener claro cuáles son los gastos que vas a tener que hacer en los diferentes casos.

Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redis que te permiten hacer más cosas, pero son mucho más costosos y necesitas probablemente un equipo que esté dedicado a ello.


### 478.E1.U3.1_Análisis_Avanzado-video

### Análisis Avanzado
![[478.E1.U3.1_Análisis_Avanzado.mp4]]
[Análisis Avanzado](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris)

Vamos a hablar de varios conceptos claves para el correcto funcionamiento de nuestro proceso de desarrollo de nuestros algoritmos, es decir, son aquellos conceptos que siempre tenemos que revisar antes de comenzar a entrenar nuestro algoritmo o cuando estemos observando que la precisión no es la correcta, siempre tendremos que volver a revisar si cumplimos esta serie de características, para ello vamos a... Vamos a intentar analizar punto por punto donde tenemos que prestar la atención. El primero, como ya hemos mencionado anteriormente, sería el concepto de los missing values. Como hemos dicho, el primer punto sería visualizar nuestro dataset. Aquí al visualizar o pedirle, depende del framework o del lenguaje de programación que estemos utilizando, lo que haremos será consultar las estadísticas de este dataset para que nos tratemos de ello. diga de una forma científica cuántos missing values tenemos. No se trata de ir comprobándolo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de missing values tenemos en nuestras columnas, en nuestras variables. Aquí lo importante, una vez los detectamos, es saber qué estrategia seguimos. ¿Cómo manejamos esta situación? Las estrategias a seguir las podríamos dividir en dos grupos. El primero sería eliminar y el segundo sería reemplazar. Aquí hay que hablar de dos escenarios muy diferentes. Uno es cuando tenemos muchísimos datos, estamos hablando de 300.000 o 400.000 filas, de alguna forma vamos sobrados. Entonces, si tenemos un porcentaje de missing values, digamos que es un poco más. que están afectadas el 20% de las filas, el hecho de que las eliminemos no va a importar demasiado. Pero existe otro escenario en el cual vamos justos de datos. Entonces, imaginaos que tenemos 3,000, 4,000 filas. Si aquí estamos hablando de eliminar filas donde hay missing values, estaríamos teniendo un problema, porque realmente si ya íbamos muy justos, estamos yendo aún más justos, con lo cual esto... puede tener una afectación en la performance de nuestro algoritmo. Por tanto, esta sería la primera medida, el eliminar filas que decidiríamos en base a nuestra cantidad de datos. Otra estrategia sería si vemos que una variable tiene una gran cantidad de missing values, pues realmente lo que está afectando a un 40% de los datos, un 40% de las filas, lo mejor es eliminarla porque... porque no va a haber forma de reemplazar estos valores. Y esto nos da pie a hablar de la segunda estrategia, que sería reemplazar. Esta estrategia la aplicaríamos cuando estamos en el escenario de que tenemos pocos datos. Por tanto, lo que tenemos que hacer es exprimir al máximo estos datos. ¿Reemplazar? Se puede reemplazar por la media, se puede reemplazar por la mediana. se puede empezar con una constante o hay otro tipo de estrategias que lo que harían sería si pongamos que por ejemplo en un dataset un factor muy importante es la edad, una variable muy importante y resulta que tenemos missing values en la variable edad que tenemos muchos y que no sabemos la edad de una cantidad muy grande de usuarios pero ¿qué pasa? que imaginaros que también tenemos su sus estudios, su ocupación de los... Imaginaros que es un dataset, ¿no? Otra vez, no sé, por ejemplo, pongamos de... estudios universitarios y tenemos el que han estudiado o a qué se han dedicado después o tenemos diversas informaciones que nos pueden hacer suponer en base a la ocupación o al nivel de estudios podríamos interpolar o averiguar un poco el rango de edad, no es preciso a lo mejor que tengamos la edad exacta, sino podemos... determinar si esta persona, a lo mejor, está en primaria, o está en secundaria, o en estudios universitarios, o trabajando, por explicarlo, o jubilada. Aquí tendríamos cinco grupos, y aquí lo que estaremos haciendo es crear una columna nueva, donde podemos también el parámetro de edad transformarlo, y de esta forma podríamos salvar el problema de los missing values. En definitiva, yo lo resumiría con una frase que es, comprender el contexto y ser creativo. El segundo concepto clave es cross-validation. Cross-validation puede ser un concepto al principio un poco confuso. A mí me gusta una forma de representarlo, de visualizarlo, como el concepto de mezclar las cartas. Cuando jugamos a un juego de cartas... y atascura una partida, después lo que hacemos es mezclarla. Ahí lo que estamos buscando es heterogeneidad. Imaginaros que hemos terminado una partida y los cuatro ases o tres ases están sobre la mesa, porque seguimos una escala. Si nosotros simplemente las cartas las juntamos, lo que nos pasará es que estarán los ases juntos, los caballos juntos, es decir, habrá una homogeneidad. Nosotros, cuando jugamos a cartas, necesitamos mezclar, heterogeneidad. ¿Y por qué es importante el concepto de cross-validation? Recordad que hemos hablado que nosotros partimos en entrenamiento y en test, 80-20 o 70-30. ¿Pero qué ocurre? Vamos a pensar sobre este concepto de entrenamiento y test. Imaginaros, vamos a poner el foco en lo que es el concepto de la captura del dato. Imaginaros que vamos a intentar hacer un algoritmo predictivo, vamos a hacer un clasificador que lo que queremos es en base de ciertos datos sociológicos y de historial médico vamos a intentar predecir si un paciente ¿va a sufrir coronavirus o no? Vamos a visualizar cómo capturaríamos los datos, ya que hemos pasado por procesos de vacunaciones masivas, imaginamos que cogemos un centro sanitario de una capital y ahí lo que hacemos es medir diversos parámetros médicos de estos usuarios. Imaginaros que hacemos una convocatoria masiva, porque necesitamos personas de todas las edades, todo el rango de edades. Imaginaros que nos llega primero un autobús de un parvulario, llegan todos los niños y se les cogen todos los parámetros médicos y se almacenan en una base de datos del 0 al 50, por ejemplo. Imaginaros que después llega un instituto, y lo mismo a todos los alumnos, después llega gente de una empresa y después llega otro autobús con gente de una residencia de ancianos, por ejemplo. ¿Qué ocurriría aquí? Imaginaros que hacemos 80-20 entrenamiento test. ¿Qué ocurre? El primer autobús que ha venido es el de los niños de preescolar y el último es el de los ancianos. Ahora imaginaros, si partimos el 80-20, probablemente nos vamos a dejar fuera el autobús de los ancianos. ¿Qué ocurriría? Que este algoritmo que lo estamos entrenando con unos datos tiene un sesgo porque no ha visto casos de gente anciana no ha visto un rango de edad, por lo tanto aquí hay un sesgo ¿Qué deberíamos hacer? Mezclar las cartas de forma que en este 80% de entrenamiento tendríamos todos los rangos de edad mezclados y en el rango, en el 20% de test, tendríamos ejemplos de todos los rangos de edad. Este es el concepto de cross-validation y se basa en el concepto de k, que son el número de folds o el número de particiones que hacemos. Es una forma de evitar sesgos porque al final no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos. El siguiente concepto clave es el overfitting. y es un concepto que a priori es bastante abstracto, pero tiene mucho sentido. Como hemos dicho, los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. El overfitting simplemente significa que han aprendido demasiado de esos datos del pasado, que se han ceñido demasiado esos datos. El objetivo central de un algoritmo es poder en sus predicciones generalizar bien No es importante No es importante que realmente tenga una precisión súper elevada Es importante que sepa generalizar bien Es decir, ¿por qué? Para que no haga predicciones de aberraciones o de cosas incoherentes En el caso del overfitting, imaginaos que Un ejemplo visual muy sencillo, imaginaos que vais a haceros un vestido a medida a un sastre. Y este sastre os mide vuestras dimensiones corporales y no deja ninguna holgura, simplemente construye. construye un traje totalmente ceñido a tu cuerpo, porque realmente esas son tus dimensiones. Esto nos haría caminar rígidos, siempre dejamos una holgura, un margen de confianza. Cuando hablemos del overfitting, hay que empezar a pensar en algo rígido, en algo que no generaliza bien, que no nos deja margen, que no nos deja tolerancia. O ahora imaginaros que vais a compraros una cama Y verdad que en una cama nosotros unos van a dormir de lado, otros van a dormir hacia arriba, otros boca abajo. Y siempre, fijaos que una cama es rectangular y nos deja un margen, una tolerancia. Fijaros en la gráfica de la derecha y veremos un ejemplo clásico de overfitting. Nosotros estamos hablando que si tenemos que... Al final los datos son una nube de puntos. ¿Qué ocurre con un punto hasta aquí, otro hasta aquí, otro hasta aquí? ¿Qué ocurre? Cuando un algoritmo aprende con overfitting lo que está ocurriendo es que va prediciendo cada punto aprendido, se ha entrenado de forma demasiado exacta. No nos interesa que se ajuste a cada punto, nos interesa que pase por la media de los puntos, es decir, esto ata un poco y liga con el concepto de media y variancia, es decir, nos interesa que nuestro algoritmo no haga una predicción exacta de cada punto. sino que esté en la media correcta más o menos de todos los puntos. ¿Por qué? Porque esto lo que nos va a permitir es que sepa generalizar bien. Porque si un algoritmo se entrena con overfitting, lo que va a ocurrir es que en el momento que tenga que hacer la predicción de ciertos valores que se salen un poco, de este patrón no va a realizar predicciones muy nefastas, dicho de una forma muy sencilla puede pasar de predicciones del 99% a predicciones del 55%, esto no nos interesa, esto es porque se ha entrenado en overfitting se ha ceñido en el entrenamiento demasiado a los datos históricos, por tanto cuando ve un dato nuevo extraño no sabe cómo comportarse, nos interesa más que el algoritmo tenga una precisión más estable por ejemplo del 85 o 89%, pero que cuando vea casos atípicos sepa generalizar bien. Es decir, cuando hablamos de overfitting, estamos hablando en Machine Learning y en Inteligencia Artificial, el 100% no existe, con lo cual estamos hablando que siempre vamos a asumir un error. Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y queremos que al final permita generalizar de forma coherente porque el error ya es asumido. La pregunta es ¿cómo detectar el overfitting? Pues se detecta cuando en los datos, cuando se entrena el algoritmo y en los resultados que arroja sobre los datos de entrenamiento arroja precisiones muy elevadas, estamos hablando del 99%. por tanto aquí tenemos que prestar atención que nuestro algoritmo se ha ajustado demasiado a los datos, por tanto no va a ser flexible ante los datos nuevos y aquí es donde tenemos que iterar y revisar y aplicar técnicas para poder evitar el overfitting ¿Y qué técnicas nos van a ayudar a evitar este overfitting? Lo primero es cross validation, como hemos mencionado partir en un número de carpetas, mezclar las cartas También podemos aplicar data augmentation, que es una técnica que lo que haremos sería aumentar de forma artificial nuestra cantidad de datos. Y podríamos probar también otras técnicas como el dropout o el early stopping, que al final lo que se interesa es buscar heterogeneidad. El dropout sería dejar fuera. ciertos datos de forma aleatoria, al final lo que vamos a buscar es tener esta heterogeneidad y que los algoritmos aprendan de forma correcta. Otro concepto clave sería el concepto de outlier. En primer lugar hay que mencionar que hay que hacer una buena praxis para detectar un outlier. Para ello lo que tenemos que hacer es visualizar y pintar nuestros datos. Es un data point que tiene un valor muy atípico, es decir, que se desvía muchísimo de la media. Intentemos pensarlo en el mundo real. Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce. de un e-commerce que venga todo tipo de productos digamos similares a los que puede tener Amazon. Nosotros capturaríamos, cogeremos la base de datos y empezaremos a analizar los valores. Y resulta que vemos que, imaginaros, que todos los días entre semana se venden de media De media global, Y nos encontramos que un día, un viernes de un mes concreto, ese día se vendieron hay un por diez. Lo primero que haríamos sería visualizar ese dato, lo detectaríamos y comprobaríamos si ese valor es real o no, porque podría pasar que en lugar de mil, en la base de datos se ha insertado manualmente diez mil. Entonces estaríamos hablando de una anomalía, habría que corregirlo, pero resulta que hacemos las comprobaciones, hablamos con el cliente y resulta que nos dice que sí, que efectivamente ese día se vendieron diez mil unidades. Claro, a nivel de variables diríamos ¿qué pasó ese día? ¿había una promoción? No. Pues imaginaos que el hecho que provocó esto fue que en Amazon se terminaron las existencias ese día de un producto muy demandado. Por tanto, como en Amazon se terminó, los clientes llegaron hacia nosotros. Entonces, este hecho ha sucedido y obviamente es importante. Pero si pensamos en términos de matemáticas y nos imaginamos esa gráfica, y estamos hablando de valores de mil o cercanos a los 1900, 1100, 1200 y, de repente, 10000, este valor sucedió, pero nosotros, recordad que un algoritmo puede ser visto de una forma abstracta, conceptual, como una línea. como una línea recta o curva que tiene una fórmula matemática. ¿Qué ocurre? Que esta línea nos la distorsionaría de una forma enorme, ¿no? Este punto, este valor tan elevado. Entonces, es aquí cuando hay que volver al concepto de generalizar. Como queremos que generalice bien, en este caso es más útil eliminar el outlier porque va a generar una distorsión en todo el conjunto. Es aquí donde es muy importante remarcar el entender la naturaleza del problema que estás abordando. Pero no siempre, no siempre es la mejor estrategia tampoco eliminarlos, es decir, tenemos que entender, como hemos dicho varias veces, la naturaleza del problema. Por ejemplo, en los casos de predicción de los stock markets, en el caso de acciones, una bolsa de valores, casos de cripto, los outliers realmente son muy importantes, es decir, hay que prestarles muchísima atención. De hecho, las métricas que hay que utilizar en estos casos son métricas que están preparadas para prestar especial atención a los outliers. ¿Por qué? Porque en la gráfica, por ejemplo, de un proyecto cripto, de un activo digital cripto, son muy importantes los outliers. picos y valles potentes, que al final están hablando de que ese día ocurrió algo, sucedió algo bonito e importante. Por tanto, respecto a los outliers hay que analizar la naturaleza del problema. para ver la estrategia que seguimos. Otro concepto muy importante es la normalización y estandarización, o también conocido como el feature scaling. Como hemos comentado anteriormente, podemos trabajar con múltiples variables. Pueden ir desde 4 hasta 10, 12 o hasta 40. Aquí es muy importante entender que estas variables se encuentran en diferentes escalas. Por ejemplo, si estamos hablando de productos del real estate o del sector inmobiliario, podríamos hablar que la variable euros puede ir desde 0 hasta 4 millones de euros, o 10 millones de euros, por ejemplo, pero la variable cuartos de baño puede ir desde 0 hasta 5 como máximo. ¿Qué tenemos aquí? Escalas muy diferentes. escalas muy desproporcionadas. Entonces la normalización y estandarización lo que pretende de una forma muy sencilla, más conceptual, es reducir esta escala, manteniendo la proporción. Lo que vamos a hacer al normalizar es todos estos valores, transformarlos para que estén entre el rango de 0 y 1. ¿Y por qué vamos a hacer esto? Porque al final estamos hablando... El algoritmo, para que lo visualicemos conceptualmente, es una curva, o es una recta, es una gráfica, y lo que vamos a hacer es facilitarle las cosas, es decir, que entienda de una forma mucho más ordenada estos valores. Al final, lo que ocurre es que, cuando aplicamos una normalización, lo único que va a suceder... es que en lugar de ver 8 millones de euros vamos a ver 0.9 o en lugar de ver 100 mil euros vamos a ver 0.9. Punto Uno. Este es el cambio que vamos a apreciar nosotros, pero lo que vamos a hacer es que durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. También hay que recordar que el proceso de normalización hay que hacerlo y deshacerlo, porque después a la hora de interpretar resultados hay que recordar que tenemos todos estos valores en esta escala. La estandarización es un proceso similar. Lo único es que se utiliza la estandarización cuando estamos resolviendo problemas que se asemejan la distribución de los datos a la campana de Gauss. De esta forma simplemente colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta menos 1. Podemos aplicar una, podemos aplicar las dos, pero bueno, por lo general es una buena praxis el intentar llevar los datos a la escala correcta cuando observemos muchísimas diferencias de escala o de magnitud. Y por último... Otro concepto muy importante, podríamos denominarlo one hot encoder, es de suma importancia porque hay que recordar que podemos tener variables, hemos visto tipos de datos, podemos tener timestamp, podemos tener íntegra, podemos tener double, que son al final números, pero también podemos tener strings. Imaginaros que, en el ejemplo inmobiliario, estamos hablando de posición de una vivienda o situación de una vivienda. Puede ser centro, puerto, periferia o zona montañosa. Claro, aquí tenemos palabras. Puede coger cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo, lo que necesitan los algoritmos son realmente números, son ceros y unos. Al final todo se basa en esto, con lo cual no nos va a admitir palabras. El concepto de One-Hot Encoding lo que hace simplemente es que una variable, si puede tener cinco valores, por ejemplo, si puede ser playa, montaña, etc., la va a desplegar en cinco. Es decir, playa, montaña, etc. periferia, centro, es decir, la variable ubicación se va a convertir en sus cinco columnas, en cinco posibles valores, y a cada fila le va, si estamos en la playa, tendrá, esta casa está en la playa, en la variable, en la columna, en la columna ubicación barra playa, tendremos un 1, un valor 1, y en el resto, valor 0. Imaginemos que la siguiente casa está en la montaña. En la variable ubicación barra playa tendrá un valor 0, en ubicación barra montaña tendrá un valor 1 y en el resto tendrá un valor 0, es decir, al final lo que hacemos es desplegamos. El punto clave aquí es saber. cuántos valores puede coger nuestra variable, si son 5 se va a transformar una variable en 5 variables. De esta forma le permitiremos al algoritmo poder trabajar de forma óptica ya que muchos de ellos solo aceptan números.


### 479.E1.U3.1_Desarrollo_del_Algoritmo

### Desarrollo del Algoritmo
Vamos a hablar de varios conceptos claves para el correcto funcionamiento del proceso de desarrollo de nuestro algoritmo.

Son aquellos conceptos que siempre tenemos que revisar antes de comenzar a entrenar nuestro algoritmo. Cuando observamos que la precisión no es la correcta, tendremos que volver a revisar si cumplimos esta serie de características. 

Para ello, vamos a intentar analizar punto por punto dónde tenemos que prestar más atención. 

![[479.E1_Desarrollo_del_Algoritmo_1.png]]

#### 1 | Missing Values
El primer paso sería visualizar nuestro Dataset, consultar las estadísticas que nos da para que nos diga de una forma científica cuantos Missing Values tenemos. 

No se trata de ir comprobándolo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de Missing Values que tenemos en nuestras columnas o en nuestras variables. 

Aquí lo importante es que, una vez las detectamos, saber qué estrategia seguimos y como manejamos esta situación.

Una de las estrategias a seguir, dividirlas en dos grupos. 
- Grupo 1: Eliminar
- Grupo 2: Reemplazar 

 ##### 1 | Eliminar
Aquí hay que hablar de dos escenarios muy diferentes. 
- **Cuando tenemos muchísimos datos, p. ej. 300k / 400k filas.** Si tenemos un porcentaje de Missing Values de un 20%, El hecho de que las eliminemos no va a importar demasiado. 
- **Cuando vamos justos de datos: 3k o 4k filas.** Si estamos hablando de eliminar filas donde Missing Values estaríamos teniendo un problema, porque si íbamos muy justos, estamos siendo aún más justos. Con lo cual esto puede tener una afectación en la performance de nuestro algoritmo. Por tanto, esta sería la primera medida, no eliminar filas que decidiríamos con base en nuestra cantidad de datos. 
- **Si vemos que una variable tiene una gran cantidad de Missing Values.** Realmente lo que está afectando a un 40% de los datos, un 40% de las filas, Lo mejor es eliminarlas, porque no va a haber forma de reemplazar estos valores.

##### 2 | Reemplazar
Esa estrategia la vamos a aplicar en el supuesto escenario de que tengamos pocos datos. 

Tenemos que hacer es exprimir al máximo estos datos. 
- Se puede reemplazar por la media. 
- Se puede reemplazar por la mediana. 
- Se puede reemplazar con una constante.
- Etc…

Hay otro tipo de estrategias, por ejemplo: 
1. En un dataset, un factor muy importante es la edad. 
2. Tenemos Missing Values en la variable edad, que tenemos muchos y no sabemos la edad de una cantidad muy grande de usuarios. 
3. Pero sí que tenemos su nivel de estudios y su ocupación actual. 
4. Podríamos interpolar o averiguar un poco el rango de edad, no tenemos la edad exacta, pero podemos determinar si esta persona está en primaria, secundaria, en estudios universitarios, trabajando o jubilada. 

Aquí tendríamos ya cinco grupos y aquí lo que estaríamos haciendo es crear una columna nueva donde podemos también el parámetro de edad, transformarlo y de esta forma podríamos salvar el problema de los Missing Values. 

> Hay que comprender el contexto y ser creativo.

![[479.E1_Desarrollo_del_Algoritmo_2.png]]

#### 2 | Cross Validation
Podemos representarlo como el concepto de mezclar las cartas. 

Cuando jugamos a un juego de cartas y cogemos una baraja, lo que hacemos es mezclarla, estamos buscando heterogeneidad. Imaginaros que hemos terminado la partida y los cuatro están sobre la mesa porque seguimos una escala homogénea.

Si nosotros simplemente las cartas las juntamos, lo que nos pasará es que estarán los ases y los caballos juntos, es decir, habrá una homogeneidad. 

Cuando jugamos a cartas, necesitamos mezclar y buscar heterogeneidad. Recordad que hemos hablado que nosotros partimos el entrenamiento en test 80/20 o 70/30. 

Vamos a poner el foco en lo que es el concepto de la captura del dato. 

#### Ejemplo: Algoritmo predictivo 
Queremos saber, basándonos en ciertos datos sociológicos y de historial médico, intentar predecir si un paciente va a sufrir o vamos coronavirus o no. 

Vamos a visualizar cómo capturar los datos, ya que hemos pasado por procesos de vacunaciones masivas. Imaginamos que cogemos un centro sanitario de una capital y ahí lo que hacemos es a medir diversos parámetros médicos de estos usuarios. 

Hacemos una convocatoria masiva porque necesitamos personas de todas las edades, todo el rango de edades.
1. Llega primero un autobús de un parvulario, llegan todos los niños y se les cogen todos los parámetros médicos y se almacena en una base de datos del cero al 50. 
2. Llega a un instituto y aplicamos el mismo protocolo a todos los alumnos. 
3. Llega un autobús de una empresa. 
4. Llega otro autobús con gente de una residencia de ancianos. 

Hacemos un entrenamiento, test 80/20 con todos estos datos.
- El primer autobús que ha venido es el de los niños de preescolar y el último es el de los ancianos. 
- Si partimos el 80/20, probablemente nos vamos a dejar fuera el autobús de los ancianos. 
- Este algoritmo, que lo estamos entrenando con unos datos, tiene un sesgo porque no ha visto casos de gente anciana, un rango de edad. Por lo tanto, aquí hay un sesgo. 

¿Cuál sería la solución? Mezclar las cartas, mezclar los 4 autobuses. De forma que en este 80% de entrenamiento tendríamos todos los rangos de edad mezclados.

En el rango del 20% de test tendríamos ejemplos de todos de test de todos los rangos de edad. 

Este es el concepto de Cross Validation y se basa en el concepto de K, que son el número default o el número de particiones que hacemos. 

Es una forma de evitar sesgos, porque al final no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos.


### 480.E1.U3.1.2_Overfitting_y_Outliers

### Overfitting y Outliers
A priori, puede parecer conceptos bastante abstractos, pero tienen mucho sentido… ¿Qué son y para qué sirven?

#### Overfitting
Los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. Simplemente, significa que pueden haber aprendido demasiado de esos datos del pasado.

El objetivo principal de un algoritmo es poder generalizar en sus predicciones, no es importante una precisión súper elevada, sino que sepa generalizar bien para que no haga predicciones de aberraciones o de cosas incoherentes.

Veamos un ejemplo Overfitting muy sencillo:
- Vamos a un sastre para hacernos un traje a medida. 
- Este sastre mide las dimensiones corporales exactas y no deja ninguna holgura, simplemente, construye un traje totalmente ceñido a tu cuerpo, tus dimensiones. 
- Esto nos haría caminar rígidos.

Cuando hablamos de Overfitting, empezamos a pensar en ese cuerpo rígido con el traje sin tolerancia alguna.

Imaginemos ahora que los datos son una nube de puntos. ¿Qué ocurre cuando un algoritmo aprende con Overfitting? 
- Lo que está ocurriendo es que va prediciendo cada punto aprendido, ha entrenado de forma demasiado exacta.
- De hecho, no nos interesa que se ajuste cada punto. Nos interesa que pase por la media de los puntos, está un poco ligado al concepto de media y varianza.
- Queremos que nuestro algoritmo no haga una predicción exacta de cada punto, sino que esté en la media correcta de todos los puntos porque nos va a permitir generalizar correctamente.

> Si un algoritmo se entrena con Overfitting, en el momento que hace una predicción y ciertos valores se salen un poco de este patrón, va a realizar predicciones nefastas, puede pasar del 90% al 55% y esto no nos interesa, se ha ceñido demasiado a los datos históricos.

- Por tanto, cuando ve un dato nuevo, extraño, no sabe cómo comportarse. 
- Nos interesa más que el algoritmo tenga una precisión más estable, por ejemplo, entre el 85% y el 89%, pero cuando vea casos atípicos, generalice bien.
- En Machine Learning e Inteligencia Artificial, el 100% no existe, con lo que estamos hablando de que siempre vamos a asumir un error. 
- Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y que, al final, permita generalizar de forma coherente porque dicho error ya está asumido.

![[480.E1_Overfitting_y_Outliers_2.png]]

##### ¿Cómo detectar el Overfitting?
Lo podemos detectar en los datos, cuando se entrena el algoritmo. Si en los entrenamientos arroja precisiones muy elevadas (99%), tenemos que prestar atención, pues nuestro algoritmo se está ajustando demasiado a los datos, por lo tanto, no va a ser flexible ante los nuevos datos.

En este punto, debemos iterar, revisar y aplicar técnicas para poder evitar el Overfitting.
- Cross Validation, mezclar las cartas. 
- Data Aumentation, que sería aumentar, de forma artificial, nuestra cantidad de datos.
- Drop Out. Dejar fuera ciertos datos de forma aleatoria.
- Early Stopping.

Al final lo que buscamos es tener Heterogeneidad y que los algoritmos aprendan de forma correcta.

#### Outliers
En primer lugar, hay que tener una buena praxis para detectar un Outlier y, para ello, lo que tenemos que hacer es visualizar y pintar nuestros datos. 

Es un metapoint que tiene un valor muy atípico, es decir, que se desvía muchísimo de la media de temas.
- Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce que venda todo tipo de productos similares a los que puede tener Amazon.
- Capturamos los datos, cogemos la base de datos y empezamos a analizar los valores.
- De lunes a viernes venden, de media global, 1000 productos.
- De repente, un viernes de un mes concreto, se vendieron 10000, hay un x10.

**Lo primero que haremos es visualizar ese dato, detectarlo y comprobar si ese valor es real o no,** porque puede pasar que en lugar de 1000, la base de datos haya insertado manualmente 10000, entonces estaríamos ante una anomalía que habría que corregir.
- Pero hacemos las comprobaciones, hablamos con el cliente y resulta que sí, que, efectivamente, ese día se vendieron 10000 unidades.
- A nivel de variables diríamos que lo que sucedió ese día es que había una promoción, pero el hecho que provocó esto fue que en Amazon, ese mismo día, se acabaron las existencias de un producto determinado. Los clientes llegaron hacia nosotros.
- Este hecho es, obviamente, importante, pero si pensamos en términos de gráficas, de que estamos hablando de valores de 1000 o cercanos y, de repente, 10000.
- Un algoritmo puede ser visto en forma abstracta o conceptual, como una línea recta o curva que tiene una fórmula matemática. 
- Esta línea nos la distorsionaría de forma abrupta con este valor tan elevado.

Entonces es aquí cuando hay que volver al concepto de generalizar. 

Como queremos que generalice bien, en este caso, es más útil eliminar el Outlier porque va a crear una distorsión en todo el conjunto. Es muy importante remarcar que debemos entender la naturaleza del problema que estamos abordando.

![[480.E1_Overfitting_y_Outliers_1.png]]

Pero, ==eliminar el Outlier no siempre es la mejor estrategia==.

Por ejemplo, la naturaleza del problema en los casos de predicción de los Stock Market, de acciones en la Bolsa de Valores o en los casos Cripto, los Outliers son realmente muy importantes y hay que prestarles muchísima atención.

De hecho, las métricas que hay que utilizar en estos casos, son métricas que están preparadas para prestar especial atención a esto porque en la gráfica, esos Outliers, esos picos y valles, son datos muy potentes porque al final están hablando de que ese día ocurrió algo especial, importante, por tanto, hay que analizar la naturaleza del problema para ver la estrategia a seguir.


### 481.E1.U3.1.3_Feature_Scaling

### Normalización y Estandarización (Feature Scaling)
Como hemos comentado anteriormente, podemos trabajar con múltiples variables que pueden ir desde 4 hasta 10, 12 o hasta 40. 

**Es muy importante entender que estas variables se encuentran en diferentes escalas.**
- Si estamos hablando de productos del Real Estate o del sector inmobiliario, podríamos hablar que la variable Euros (€) puede ir desde cero hasta 4 millones de € o 10 millones de €.
- En cambio, la variable a cuartos de baño puede ir desde 0 hasta 5 como máximo.

Aquí nos encontramos escalas muy diferentes y muy desproporcionadas. Lo que pretende normalización y estandarización, de una forma muy sencilla y más conceptual, es reducir esta escala manteniendo la proporción.

#### Normalización
- Al normalizar es todos estos valores vamos a transformarlos para que estén entre el rango de 0 y 1. 
- ¿Por qué vamos a hacer esto? Porque estamos hablando de que el algoritmo, para que lo visualicemos conceptualmente, es una curva o es una recta, una gráfica. 
- Lo que vamos a conseguir es facilitarle las cosas, es decir, que entienda estos valores de una forma más ordenada.

Cuando aplicamos la normalización, lo único que va a suceder es que en lugar de ver 8 millones de €, vamos a ver 0.9, o en lugar de 100.000 €, vamos a ver 0.1. 

Este es el cambio que vamos a apreciar nosotros. Pero lo que vamos a hacer es que, durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. 

También hay que recordar que, el proceso de normalización, hay que hacerlo y deshacerlo, porque después, a la hora de interpretar resultados, hay que recordar que tenemos todos estos valores en esta escala.

#### Estandarización
La estandarización se utiliza cuando estamos resolviendo problemas que se asemejan a la distribución de los datos a [la campana de Gauss](https://es.wikipedia.org/wiki/Funci%C3%B3n_gaussiana). 
- Simplemente, colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta -1. 
- Podemos aplicar una o podemos aplicar las dos, pero bueno, por lo general es una buena praxis el intentar llevar los datos a la escala correcta cuando observemos muchísimas diferencias de escala o de magnitud.

#### One-Hot Encoder
Es de suma importancia, pues hay que recordar que podemos tener muchas variables. 

Podemos tener Timestamp, podemos tener Integer, podemos tener Double que son al final números. 

Pero también podemos tener strings. 

Pensemos en el ejemplo inmobiliario, en la posición o situación de una vivienda, que puede ser centro, puerto, periferia o zona montañosa. Aquí puede coger cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo lo que necesitan realmente los algoritmos son números, ceros y unos. 

  
|  | Playa | Montaña | Periferia | Centro |
| ------ | ------ | ------ | ------ | ------ |
| Casa Playa | 1 | 0 | 0 | 0 |
| Casa Montaña | 0 | 1 | 0 | 0 |
| Casa Periferia | 0 | 0 | 1 | 0 |
|  Casa Centro | 0 | 0 | 0 | 1 |

Lo que consigue es One-Hot Encoding es que si una variable, sí que puede tener 4 valores.
- Playa
- Montaña
- Periferia
- Centro

Por ejemplo, sí puede ser playa, montaña, etc., la va a desplegar en 4. La variable ubicación se va a convertir en sus 4 columnas, en 4 posibles valores.

Si la casa está en la playa, en la variable, en la columna en la columna Ubicación/Playa tendremos valor uno y en el resto valor 0. 

Imaginemos que la siguiente casa está en la montaña. En la variable Ubicación/Playa tendrá un valor 0 en Ubicación/Montaña tendrá un valor 1 y en el resto tendrá un valor 0. 

El punto clave aquí es saber cuántos valores puede coger nuestra variable. Si son 4, se va a transformar una variable en 4 variables. De esta forma le permitiremos al algoritmo poder trabajar de forma óptica, ya que, muchos de ellos, solo aceptan números.


### 482.E1_Predecir_el_Precio_de_Bitcoin-video

### Predecir el Precio de Bitcoin
![[482.E1_Predecir_el_Precio_de_Bitcoin.mp4]]
[Predecir el Precio de BitcoinTitulo](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866107-u4-1-1-predecir-el-precio-de-bitcoin-jose-peris)

En la clase de hoy vamos a analizar un caso práctico que se llama el caso de la como sería el desarrollar un algoritmo predictivo del Bitcoin. Vamos a ver todo el proceso de desarrollo y sobre todo vamos a estudiar paso a paso qué tácticas o qué decisiones tomar de cara a un desarrollo exitoso. En primer lugar, como os he comentado en las otras clases, el primer objetivo es... conocer el caso de negocio, analizar el contexto, entender la naturaleza del problema, porque al final básicamente los datos que nos vamos a encontrar en el caso del bitcoin va a ser un time stamp y un precio. Es una gráfica muy simple, pero claro debemos de entender qué hay detrás de esta gráfica, porque detrás de esta gráfica hay emociones de personas, sentimientos, pueden haber emociones como... como el miedo o el optimismo que impulsan a la gente a comprar o a vender. Podéis ver, por ejemplo, este ejemplo que os muestro aquí en la pantalla, donde podéis ver todas las transacciones que se están ejecutando en tiempo real sobre la red de blockchain de Bitcoin, y os harán entender de una forma gráfica el concepto de volatilidad. Al final, como os decía, detrás de todo esto, simplemente hay personas arrastradas por el optimismo, por la... por el pesimismo o por la estrategia o la táctica para obtener ciertos resultados. Por tanto, el primer factor es entender qué significa esta gráfica. Para ello, un ejercicio muy interesante también es estudiar la gráfica del bitcoin. Al final, si os fijáis, si veis este esquema podéis apreciar que en cada cambio, en cada repunte hay un hecho histórico detrás que ha marcado este cambio. Estas palancas son las que cambian lo que se llama el momentum y es de vital importancia poder trackear este tipo de palancas o de eventos para poder predecir el cambio de tendencia. Esta sin duda va a ser la clave y realmente la clave. La complejidad de este ejercicio radica en saber qué variables escoger, qué variables monitorizar y finalmente qué tipo de algoritmo escoger y qué experimentos realizar para obtener la mejor precisión posible, que es nuestro objetivo. nuestro objetivo. Lo primero que debemos entender es cuál es nuestro input y cuál es nuestro output. Básicamente nosotros tenemos una gráfica con un timestamp y un precio y lo que queremos es prácticamente generar una gráfica predictiva que coincida, que prácticamente se ajuste lo máximo posible, que calque o que clone los movimientos sobre todo a futuro, una ventana temporal hacia adelante. ¿Para qué? Para tener la ventaja con competitiva, de saber cuándo va a subir o cuándo va a bajar el precio del bitcoin. Ya os aseguro que se han realizado muchos estudios sobre este campo. Realmente es muy complejo, es un ejercicio muy complejo, pero es muy interesante para analizar la causa efecto que hay entre las diferentes variables. Y por otro lado, hay algo también que debéis saber, que tampoco sabemos si hay algún tipo de algoritmo que una persona pueda utilizar para conseguir un buen precio. una precisión muy elevada, o si existe, hasta ahora no se ha publicado. Sí que es cierto que se publican muchísimos resultados, pero hay que saber leerlos y entenderlos, porque muchas veces, cuando se presentan resultados en papers, también se presentan sobre datos de entrenamiento, o sobre datos de test que realmente no son demasiado fiables. Con lo cual, hay que tener también en presente esta forma de... de leer los papers y de saber que siempre hay que indagar un poco más hacia lo que tenemos delante que a primera vista puede parecer magnífico. El primer punto, como os expliqué, el primer punto siempre es Data Collection, que es capturar los datos. En este caso, tenemos diferentes APIs, desde Yahoo Finance API, por ejemplo, podemos seleccionar todas las fechas que tenemos disponibles de los precios del Bitcoin, por ejemplo, desde 2015 hasta la actualidad. Y hay que tener claro cuál es nuestro objetivo. Nuestro objetivo es predecir el precio de Bitcoin a un día, hacia adelante, a cinco días y a 30 días. Es decir, tenemos tres escenarios, uno, cinco y 30. El reto es bastante potente y siempre hay que pensar que vamos a ver, vamos a analizar. qué métricas obtenemos a un día, qué métricas obtenemos a 5 y qué métricas obtenemos a 30. Esa va a ser la conclusión de nuestro estudio, nuestro experimento. El segundo paso en el proceso de creación de un algoritmo es siempre el estado del arte. Como hemos visto en anteriores lecciones, el estado del arte se basa en indagar y evaluar cuál es la última tecnología disponible. para nuestro caso de uso. Como estamos hablando de una serie temporal y estamos hablando de una regresión, deberíamos observar todos aquellos experimentos publicados, sobre todo aquellos papers. Un paper, os explico aquí lo que es, un paper es como una tesina, diríamos, es un formato que se utiliza en la comunidad científica para mostrar resultados de una forma simplificada. Por ejemplo, si un equipo de investigación ha estado realizando unos estudios sobre un tipo de algoritmo para resolver un tipo de problema concreto, lo que hacen es exponen, por un lado, los impulsos que tenían, el problema que querían resolver, y por otro lado se explica los resultados a los que han llegado y todos los experimentos que han realizado. Esto es muy útil y permite a la comunidad de Machine Learning y de Inteligencia Artificial avanzar muy rápidamente porque al final es un poco la sabiduría de la comunidad lo que permite poder avanzar de forma rápida. Por tanto, primer punto, estaríamos hablando de una revisión de la literatura de problemas similares y observaríamos el estado del arte. En este caso podríamos observar que tenemos diferentes caminos que se centran prácticamente en dos bloques, Machine Learning y Deep Learning, sí que es cierto que se están haciendo muchos más experimentos, quizás con Transformers o con Reformed Learning, pero nos vamos a centrar básicamente en los dos focos principales. y en los tipos de algoritmo que se han ya probado con cierto éxito para este tipo de predicciones. El siguiente paso, una vez descargados los datos, es el más crucial, porque sería el decidir qué tipo de variables afectan al precio del Bitcoin. Es decir, podemos buscar indicadores de todo tipo, como indicadores de momentum. como podemos utilizar el RSI o incluso podríamos hablar de medias móviles a 20, a 100, a 200 días. Podríamos ver toda clase de indicadores técnicos financieros y por otro lado, pues también, quizás, descubrir e intentar discernir si está vinculado, por ejemplo, con el precio del oro o de otros mercados de valores y una correlación directa o indirecta. Por tanto, aquí de lo que se trata es de coleccionar aquellos indicadores o variables que a priori pueden tener una correlación para partir de ahí hacer una selección grande, potente y estudiar las correlaciones para ver con cuáles nos quedamos. Si os fijáis, después de realizar una serie de experimentos sobre todo basado en correlaciones, al final la correlación no es más que el ver, explicado de una forma muy sencilla, es simplemente observar si dos gráficas se parecen mucho, y cuánto se parecen. Es decir, si se parecen mucho, estamos hablando de que la correlación siempre va desde 0 hasta 1. Si se parecen mucho, la correlación será de 1 o de 0.99. La correlación de 1 es prácticamente imposible, pero estaríamos hablando de 0.92 o estos son correlaciones altísimas. Si no se parece, tendrá un valor muy bajo. Si observáis las siguientes gráficas de correlación, de una forma muy sencilla, lo único que queremos ver es qué variables, qué gráficas coinciden con el precio del bitcoin. Y no son tantas. Las más importantes que vemos son la hash rate. Miner revenue y Google Trends. Vamos a analizar cada una de las tres porque tienen correlación y este es un ejercicio un poco de imaginación y de lógica. Hash rate. ¿De qué estamos hablando? El concepto hash está relacionado a la minería en una red de blockchain. Por tanto, ¿qué está diciendo esta gráfica? Está diciendo que cuando el precio del bitcoin sube lleva a que el valor se desvanezca. la misma correlación con el rate, con la tasa de hash que se está produciendo en la red de blockchain, es decir, con la creación de bloques. Por tanto, ¿qué quiere decir? Mucha creación de bloques influye directamente con que el precio esté subiendo o esté bajando. Por tanto, realmente no nos está aportando ningún valor, porque simplemente describe un mecanismo de funcionamiento de una red de blockchain. Es por eso que para ello el contexto es tan importante. Necesitamos entender la naturaleza del problema. La siguiente gráfica que tiene correlación con el precio del bitcoin es el minor revenue. ¿Por qué tiene relación? Si comprendemos bien el funcionamiento. de una red de blockchain, sabemos que un minero, a medida que va minando y resolviendo la proof of work o proof of stake, obtiene beneficios a cambio, es decir, por tanto, si se están produciendo muchas acciones de compra-venta lo que está sucediendo es que la ganancia de los mineros tiene una relación directa, esto es también bastante obvio, por lo tanto son variables que realmente no aportan ningún valor, pero ¿qué ocurre? si analizamos la gráfica de la correlación entre Google Trends y el precio del Bitcoin. ¿Qué está ocurriendo aquí? Pensad en cómo funcionamos también los seres humanos. Al final lo que hacemos es imaginaros, ¿no? De momento vemos que se está hablando muchísimo de Bitcoin en las noticias, en círculos sociales, te cuentan que alguien ha invertido y ha ganado dinero o ha perdido, da igual en positivo o negativo. Entonces, ¿qué ocurre rápidamente en vez de...? empezamos a ver qué precio tiene, qué está pasando con esta cripto y rápidamente Google Trend empezamos a activar toda esta serie de búsquedas. Esto sí que tiene una correlación directa. Y sí que es una variable importante porque al final nos está hablando, el mensaje que nos está diciendo es, cuando mucha gente a la vez está buscando Bitcoin, después ocurren cosas con el precio del Bitcoin. Aquí hablamos de un lapso de tiempo que puede ser los traders, para los traders a short. Bueno, esta ha sido una pequeña muestra previa de análisis de aquellas variables importantes, pero el experimento se ha realizado con una matriz de correlación. Si os fijáis, la matriz de correlación lo que está midiendo es desde 0 hasta 1 y desde 0 hasta menos 1 para medir correlación. directa o correlación inversa. En la matriz de correlación lo que intentamos buscar es la correlación directa o inversa y analizar este tipo de hechos que acabamos de comentar previamente, es decir, hay que tener mucho cuidado porque en este caso fijaros que el hash rate y el meaner revenue tiene una correlación muy elevada con el precio del bitcoin pero estarían añadiendo ruido a nuestro negocio. nuestro modelo porque es simplemente redundancia, es decir, es una consecuencia de que haya compra o venta de Bitcoin, sin embargo otras como Welltrend tienen una correlación alta, ya sea positiva o negativa y sí que están hablando de indicadores de momentum, que es lo que estamos buscando, por lo tanto, este proceso que se llama feature selection, que se hace con una matriz de correlación y es muy fácil de ejecutar con pocas líneas del código en Python nos permite comenzar a descartar. aquellas variables que aportan ruido y empezar a ver aquellas variables que sí que realmente son importantes. Una vez hemos realizado la matriz de correlación podemos ya empezar también a trabajar con aquellas variables que nos vamos a quedar porque se trata de feature selection, al final lo que está indicando es feature hace referencia a variables, es una selección de variables, lo que queremos es reducir porque en este caso hay tantos indicadores, Hablando de en torno a 80 indicadores técnicos, y esto es demasiado, ¿no? Para el coste computacional sería demasiado elevado. Nosotros nuestro objetivo con el feature selection es reducir. Reducir y quedarnos con aquellas variables que realmente son importantes. Y otra forma de reducir es utilizar, por ejemplo, algoritmos como XGBoost. que al final trabajan con tecnología de árboles de decisión pero con un gradiente mucho más potente que lo que nos permiten es obtener de alguna forma una clasificación de aquellas variables más importantes de mayor o menor, de cuánto peso tienen en la afectación al precio. Por tanto, una buena praxis. es el utilizar la matriz de correlación y después utilizar feature selection mediante algoritmos como XGBoost, ¿para qué? Para al final quedarnos con indicadores técnicos, con los más importantes. Es también curioso si analizáis los indicadores ganadores que obtenemos, en este caso pasamos de 80 a 36, la cual es una reducción potente que, que la media móvil está en el top, es de los más potentes. Y es cierto que muchas herramientas de trading se basan en medias móviles. Y funcionan, al final las herramientas de trading lo que hacen es... utilizan diferentes medias móviles de 5, de 10, de 20, de 200 días y básicamente lo único que hacen es, en el momento que se solapan las diferentes medias móviles, pues estos son puntos donde te indican que puedes comprar o vender. Básicamente este es el funcionamiento, pero veremos que no solo afectan estos indicadores, estas medias móviles, que hablan un poco más de la historia, del pasado, de lo que conocemos a través del pasado, sino con Machine Learning es un enfoque bastante diferente a jugar solo con el pasado. Otro factor importante, ya que con el Bitcoin estamos hablando de una serie temporal, es comprobar su estacionalidad. Esto se puede hacer de forma objetiva mediante el Dick Fuller Test. ¿Al final de qué se trata todo esto? Si os fijáis en la gráfica, esto es el concepto que se llama Rolling Mean. Simplemente se trata de buscarle una coherencia, una historia a la gráfica del Bitcoin. Se trata de entender, por un lado, lo que hacemos es desplegarla. Entonces, al desplegarla, lo que vemos es, por un lado, la tendencia y, por otro lado, intentamos de una forma muy sencilla para que lo entendamos. Lo que intentamos es... desplegarla en cuatro perspectivas. Por un lado, vamos a tener siempre la tendencia, pero después vamos a tener unos picos y valles. Aquí el problema que tenemos, después de hacer el Dick Fuller test, es que observamos que el bitcode no es estacional. ¿Por qué? Porque si descubrimos que una serie temporal es estacional, tenemos unos caminos a seguir. bastante marcados donde nos va a garantizar el éxito, pero lo que ocurre con el Bitcoin es que es tremendamente volátil. Al ser tan volátil no nos podemos apalancar en la estacionalidad, es decir, porque sea navidad o porque sea verano no vas a comprar o vender más Bitcoin. Esa es un poco la conclusión que nos aporta este experimento. Una vez he conexionado los datos, capturado los datos y agregado las variables y habiendo hecho una selección de variables, simplemente el siguiente punto es con este dataset que tenemos que había empezado a recordar, que había comenzado con dos columnas, la tempestamp y el precio, hemos pasado a tener 38 columnas. Aquí lo que empezamos a hacer es ya entrenar nuestros modelos. Para ello, seleccionamos diferentes modelos de matching learning y de deep learning y vamos a un poco analizar la performance. En esta gráfica, lo primero que hacemos es observar la performance que tiene en los datos de entrenamiento. Aquí podéis ver nuestro objetivo. Aquí recordad que partíamos el data set en entrenamiento y test 80-20 o 70-30. 30 y lo que hacemos primero es vamos a ver estos modelos de matching learning o de deep learning cuál es el que mejor aprende cuál es el que mejor aprende y el que mejor se ajusta a la curva recordad que lo que queremos es que se ajuste más o menos bien por decir de alguna forma tampoco queremos que se ajuste exactamente igual punto por punto porque esto nos llevaría a overfitting queremos que tenga una coherencia que sea como una especie de media que pase siempre con ese concepto de media y varianza que al final lo que nos va a decir es que va a poder generalizar bien el modelo. Aquí podéis ver los diferentes experimentos en pantalla y ya estaríamos en condiciones de hacer los experimentos. Una vez realizados los experimentos de entrenamiento vamos a... a pasar a realizar los experimentos en test y recordad estamos haciendo predicción a un día, a cinco días y a 30 días. Vamos a analizar, vamos a analizar las diferentes métricas de regresión, en este caso una métrica útil podría ser el rudmin square error y por qué podría ser útil porque presta muchísima atención a los outliers. Recordad La verdad que estuvimos hablando que hay que analizar el concepto de los outliers, es un concepto clave para entender la naturaleza de mi problema. En este caso, ¿qué ocurre? Fijaros, la gráfica de Bitcoin hace picos muy marcados, picos y valles de forma constante. Esto, si la naturaleza del problema fuese otra, serían outliers probablemente. Pero en todo lo que está relacionado con los serios temporales de stock market, hay que prestar atención a los outliers, porque estos outliers hablan de hitos, imaginaos en este caso, de Bitcoin, hitos en el desarrollo, en el roadmap o en la adopción que han hecho que cambie este momentum. En este caso, para un poco, al final, Rudmin Square Error está hablando de una distancia respecto a la recta de predicción, a nuestra fórmula que hemos sacado de alguna forma, para predecir el tiempo. Bitcoin está hablando de una media, no hay una distancia que hay desde nuestra predicción hasta el valor real. Con lo cual vamos a utilizar la métrica del R cuadrado ajustado, ¿por qué? porque nos arroja valores mucho más entendibles, ¿no? Vamos de 0 a 1, donde 0 es 0% de precisión y 1 es 100% de precisión. Si os fijáis, los resultados a un día son muy buenos. Estamos hablando de valores muy elevados. Estamos hablando de que hay varios modelos que hacen predicciones que superan el 90% a un día. Con lo cual, estaríamos hablando de que nuestros modelos podrían estar haciendo una predicción de que mañana el Bitcoin ya no es que va a subir o a bajar. Hace una predicción de a cuánto va a estar. Y estamos hablando de valores cercanos al 95%. Por tanto, ¿os imagináis saber mañana que el precio del Bitcoin va a estar en lugar de 28.000 a 32.000 o una precisión del 95%? ¿Os aportaría valor? ¿Haríais movimientos en vuestras wallets, por ejemplo? Estamos hablando de que esto tiene un potencial enorme, ¿no? Pero claro, es cierto también que estamos hablando de despacios a short, a un día. ¿Qué ocurre cuando…? cuando observamos los resultados a tres días. Si os fijáis, pasamos prácticamente del 95% de precisión al 65%. Es decir, aquí hay un salto importante. La performance, tanto de Machine Learning como de Deep Learning, comienza a caer. También observar algo que hemos contado en clases anteriores, que los resultados de Deep Learning, en términos generales, son bastante malos. ¿Y por qué son malos? porque realmente hay muy pocos datos para trabajar con Deep Learning. Esto es lo que provoca, sí que es cierto que hay un tipo de algoritmos de Deep Learning como son las redes neuronales recurrentes o RNN, en este caso las LSTM, Long Short Term Memory, que funciona muy bien para este tipo de señales. Es decir, que están mirando el pasado muy lejano y el pasado corto, de alguna forma. Y realmente tiene una performance muy buena para señales de este tipo. Luego ocurre que no tenemos suficiente historial de datos, no tenemos suficiente cantidad de datos. Claro, si observamos ya los resultados a 30 días, realmente lo que estamos viendo es que los resultados son desastrosos. Estaríamos hablando de que no aportamos ningún valor, es decir, aportamos el mismo valor que lanzar una moneda al aire. Por tanto, si os fijáis un poco y analizamos, diríamos que como conclusión tenemos una ventana temporal de un día a tres días, sobre todo a un día vista, tenemos una buena precisión y a medida que se acercan los tres días empieza a caer, lo cual a lo mejor nos da un margen de un día y medio. Claro, a 30 días el momentum es imposible de predecir con esta técnica. Y sobre todo, quiero que os fijéis en algo. La pregunta aquí sería ¿por qué? ¿por qué no nos estamos aproximando? ¿por qué a 30 días no somos capaces de predecir el cambio de momentum? Bueno, la respuesta estaría en entender que hemos utilizado variables que son indicadores que están en el mercado, por decirlo de alguna forma, que ya están generados, que simplemente los hemos ingestado. y que realmente lo que nos está diciendo es que, a corto, estos indicadores, a un día, un día y medio, explican algo, pero a más de dos días ya no explican algo, aquí hay otro tipo de hechos o de eventos que cambian el momento y afectan en la volatilidad. Básicamente, tenemos que entender que... qué agentes o eventos externos están influyendo al cambio de precio del Bitcoin, que no sean los indicadores financieros clásicos. ¿Y qué influye? Pues, por ejemplo, es muy fácil. Lo primero que sabemos que influye son los movimientos de las ballenas. Son operaciones que mueven en torno a entre 1.000 y 5.000 bitcoins. ¿Qué está ocurriendo? Que si de momento se juntan un grupo de inversores y deciden hacer un movimiento muy potente, esto afecta directamente al precio del Bitcoin de forma instantánea. Claro. Esto a priori es difícil de predecir, pero se podría predecir, se podía monitorizar, que es lo que nos interesa. Lo que nos interesa es cómo tener un tracking de estos movimientos de las ballenas, para tener un lapso de tiempo que nos permita tener ventaja competitiva. Claro, ¿qué características tiene Blockchain? En Blockchain sabemos que es mutable, que es segura, pero que es tráfico. Entonces desde aquí podríamos estudiar movimientos de wallets calientes a wallets frías. ¿Por qué? Porque en este momento se retira el dinero. Con lo cual, movimientos de ballenas, si las detectamos y se hace un ejercicio de tracking muy potente, y muy exhaustivo, pero nos podría arrojar luz de traquear movimientos de ballenas, que a priori es lo que más sombras aporta en todo este caso. Claro, sabemos que los movimientos de ballenas son importantes, pero recordad que pasó otro factor muy importante, aparte de las ballenas, eran los círceles negros. ¿Qué pasó cuando vivimos la pandemia? El bitcóin cayó hasta los 6.000 euros. Esto es una afectación muy directa de un cisne negro. Los cisnes negros son eventos que suceden a lo largo de la historia que son impredecibles. En este caso tenemos un ejemplo muy claro como fue el COVID-19. ¿Se pueden monitorizar o trackear los cisnes negros? Sí, los podríamos monitorizar o trackear. Es decir, en este caso lo que haríamos sería añadir una variable que sería cisne negro. negro le daríamos un valor desde 0 hasta 1. Fijaros vosotros que en este impacto social que hemos vivido lo podríamos haber cuantificado de esta forma, desde el momento en que podría salir a cenar y la foro era del 50% o del 25, digamos que a nivel de limitación de vida podríamos medirlo en una escala y lo tenemos registrado por fechas, sabemos las restricciones, en qué momentos han sido, por tanto ya no sólo esto es. Y fijaros, los tides negros, como podría ser una guerra o una pandemia, no se pueden predecir. Es cierto que es muy difícil predecirlos, pero fijaros que sí que se puede. No se puede predecir de forma, digamos, a dos años vista o tres. Eso sería imposible. Pero sí que se puede predecir a un mes, dos, tres vista. Porque, por ejemplo, fijaros en el caso de las guerras. No se invade un país de la noche a la mañana. Por ejemplo, en el caso de la guerra del Sur, no se puede predecir. Es decir, no se puede predecir. Primero, hay muchas noticias al respecto, hay muchas conjeturas, muchas declaraciones, y sí que podemos trackear las noticias, y sí que podemos detectar en el PIB qué está pasando, y detectar qué grado de intensidad hay, qué probabilidad de que haya un conflicto bélico, esto es un ejército que es bastante sencillo, con inteligencia artificial. Por tanto, lo mismo ha pasado con el COVID-19, sí que te podíamos hacer una prueba. una predicción de, hay 100 casos en China, las noticias que empiezan a subir el pulso de esta noticia y sabemos que después se pasan a 500, a 1000, a 2000, es decir, sí que podríamos hacer una predicción de pandemia. Con lo cual, poder monitorizar los cines negros puede darnos una ventaja competitiva también de cara a la predicción de este tipo de algoritmos. Por tanto, Ya tenemos por un lado. tenemos las ballenas y por otro lado tenemos los cisnes negros más fácil de rastrear hasta ahora un cisne negro que el movimiento de ballenas, aunque se puede rastrear, como hemos dicho, haciendo análisis on-chain y el tercer factor sería el sentiment analysis ¿y por qué el sentiment analysis? al final, quien no conoce, los casos en los que por ejemplo El más famoso es el de Elon Musk sobre Twitter, haciendo diferentes afirmaciones, ha generado variaciones importantes sobre el precio del bitcóin. El caso de Elon Musk sería un caso para darle de comer aparte, porque sería un mega influencer, pero no el concepto de influencer que conocemos, sino un influencer a nivel mundial. mundial que está cambiando la historia. Van a haber pocos genios como Elon Musk en la historia que han revolucionado tantos sectores. Es por ello pues que la voz de influencers como Elon Musk o Bill Gates son muy escuchadas porque al final tiene una relevancia histórica muy potente. Cuando hablamos de sentiment analysis no es simplemente escuchar a Elon Musk, estaríamos hablando de, por ejemplo, podíamos hacer un tracking de qué de cuánto se está hablando en las redes sobre Bitcoin, por ejemplo, en Twitter o en otra red social. Al final, el sentiment analysis lo que hace son algoritmos que están entrenados para, cuando leen un texto, cuando capturan un texto, lo que deciden es si ese contenido es positivo, negativo o neutro, respecto al concepto que estamos monitorizando. De esta forma, sí que podemos medir el sentimiento que hay en las redes sociales hacia un concepto o un hashtag concreto. En este caso sería Bitcoin. Y sí que hay estudios que demuestran que hay una correlación bastante directa, como podéis ver en la gráfica, del sentimiento análisis con el precio de productos cripto y en este caso el de Bitcoin. el bitcoin. Por tanto, por recapitular, aquí concluye nuestro experimento donde lo que hemos realizado es capturar en un inicio datos sobre bitcoin. su precio, el timestamp y el precio, hemos añadido variables, hemos hecho una reducción de variables, hemos hecho experimentos, hemos probado diferentes algoritmos, hemos analizado los resultados y también hemos propuesto posibles mejoras de variables para poder desarrollar un algoritmo de predicción del Bitcoin con éxito a una ventana temporal ya que superé que vaya de los 3 días hasta los 30 días. al final en los proyectos de Machine Learning, Inteligencia Artificial, la parte táctica y estratégica y de lógica es muy importante para saber qué variables tengo que trackear para poder alcanzar la máxima precisión.


### 484.E1.U4.1.1_Algoritmo_Predictivo_de_Bitcoin

### Gráficas de correlación
Las gráficas de correlación más importantes que vemos son: 
1.  Hash Rate
2.  Miner Revenue
3.  Google Trends

Vamos a analizar cada una de las tres porque tienen correlación y este es un ejercicio un poco de imaginación y de lógica.

![[484.E1_Algoritmo_Predictivo_de_Bitcoin_1.png]]

#### 1 | Hash Rate
El concepto Hash está relacionado con la minería en una red de Blockchain. 

**Por tanto, ¿Qué nos dice esta gráfica?**
Está diciendo que cuando el precio del Bitcoin sube, lleva la misma correlación con el Rate, con la tasa de Hatch que se está produciendo en la red de Blockchain, es decir, con la creación de bloques. 

![[484.E1_Algoritmo_Predictivo_de_Bitcoin_2.png]]

**¿Qué nos dice esto?** 
Mucha creación de bloques influye directamente con que el precio esté subiendo o esté bajando. Por tanto, no nos está aportando ningún valor, porque simplemente describe un mecanismo de funcionamiento de una red de Blockchain. 

> Es por eso que para ello el contexto es tan importante. Necesitamos entender la naturaleza del problema.

#### 2 | Miner Revenue
**¿Por qué tiene relación?** 
Si comprendemos bien el funcionamiento de una red de Blockchain, sabemos que un minero, a medida que va minando y resolviendo la Proof of Work o Proof of Stake, obtiene beneficios a cambio. Por tanto, si se están produciendo muchas acciones de compraventa, lo que está sucediendo es que la ganancia de los mineros tiene una relación directa. Esto es bastante obvio. 

En consecuencia, son variables que realmente no aportan ningún valor. 

Pero ¿qué ocurre si analizamos la gráfica de la correlación entre Google Trends y el precio del Bitcoin? ¿Qué está ocurriendo aquí?. 

Pensad en cómo funcionamos los seres humanos. De momento vemos que se está hablando muchísimo de Bitcoin en las noticias, en círculos sociales, te cuentan que alguien ha invertido y ha ganado dinero o ha perdido. 

**¿Qué pasa entonces?**
Rápidamente, empezamos a ver qué precio tiene, qué está pasando con esta cripto y rápidamente, en Google Trends empezamos a activar toda esta serie de búsquedas. Esto sí que tiene una correlación directa. Y es una variable importante, porque al final nos está hablando, el mensaje que nos está diciendo es: Cuando mucha gente a la vez está buscando Bitcoin, pues están sucediendo cosas con el precio del Bitcoin. Estamos hablando de un lapso de tiempo que puede ser importante, sobre todo para los Traders a short. 

Esta ha sido una pequeña muestra previa de análisis de aquellas variables importantes, pero el experimento se ha realizado con una matriz de correlación. Si os fijáis, la matriz de correlación, lo que está midiendo es:
- De 0 hasta 1: Correlación directa
- De 0 hasta -1: Correlación inversa

En la matriz de correlación, lo que intentamos buscar es la correlación directa o inversa y analizar este tipo de hechos que acabamos de comentar previamente. Es decir, hay que tener mucho cuidado, porque en este caso el Hash Rate y el Miner Revenue tienen una correlación muy elevada con el precio del Bitcoin, pero estarían añadiendo ruido a nuestro modelo, porque es simplemente redundancia. Esto es una consecuencia de que haya compra o venta de Bitcoin.

#### 3 | Google Trends
Sin embargo, otras como Google Trends tienen una correlación alta, ya sea positiva o negativa, y sí que están hablando de indicadores de Momentum, que es lo que estamos buscando. Por lo tanto, este proceso, que se llama Feature Selection, que se hace con una matriz de correlación y es muy fácil de ejecutar con pocas líneas de código en Python, nos permite comenzar a descartar aquellas variables que aportan ruido y empezar a ver aquellas que, realmente, son importantes.

Una vez hemos realizado la matriz de correlación, podemos ya empezar también a trabajar con aquellas variables que nos vamos a quedar, porque se trata de Feature Selection. Al final, lo que está indicando es feature hace referencia a variables, es una selección de variables. Lo que queremos es reducir, porque en este caso hay tantos indicadores. Estamos hablando de en torno a 80 indicadores técnicos y esto es demasiado. El coste computacional sería demasiado elevado. 

> Nuestro objetivo con el Feature Selection es quedarnos con aquellas variables que realmente son importantes.

Otra forma de reducir las variables es utilizar, por ejemplo, algoritmos como [XGBoost](https://xgboost.readthedocs.io/en/stable/), que trabajan con tecnología de árboles de decisión, pero con un gradiente mucho más potente y lo que nos permiten, es obtener, de alguna forma, una clasificación de aquellas variables más fundamentales de mayor o menor, de cuánto peso tienen en la afectación al precio. 

Por tanto, una buena praxis es el emplear la matriz de correlación y después usar Feature Selection mediante algoritmos como XGBoost. 

**Para qué?**
Para quedarnos con los indicadores técnicos más importantes. 

Si analizamos los indicadores ganadores que obtenemos (en este caso, pasamos de 80 a 36, lo cual es una reducción potente) que la media móvil está en el top, es de los más potentes y es cierto que muchas herramientas de trading se basan en medias móviles. Y funciona, pues las herramientas de trading lo que hacen es utilizar diferentes medias móviles de 5, 10, 20 o 200 días y, básicamente, lo único que hacen es, en el momento que se solapan las diferentes medias móviles, indicarte los puntos en los que puedes comprar o vender. 

Básicamente, este es el funcionamiento, pero veremos que no solo afectan estos indicadores. Estas medias móviles que hablan un poco más de la historia del pasado, con Machine Learning es un enfoque bastante diferente a jugar solo con el pasado. 

Otro factor importante, ya que con el Bitcoin estamos hablando de una serie temporal, es comprobar su estacionalidad. Esto se puede hacer de forma objetiva mediante el [Dickey–Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test).

**¿De qué trata todo esto?**
Si nos fijamos en la gráfica, esto es el concepto que se llama Rolling Min. Simplemente, se trata de buscarle una coherencia, una historia a la gráfica del Bitcoin. Se trata de entender, por un lado, lo que hacemos es desplegarla. Al desplegarla, lo que vemos es, por un lado, la tendencia y, por otro lado, intentamos desplegarla en cuatro perspectivas. 

Por un lado, vamos a tener siempre la tendencia, pero después vamos a tener unos picos y valles. Aquí el problema que tenemos, después de hacer el Dickey–Fuller test, es que observamos que el Bitcoin no es estacional. ¿Por qué? Porque si descubrimos que una serie temporal es estacional, tenemos unos caminos a seguir bastante marcados donde nos va a garantizar el éxito. Pero lo que ocurre con el Bitcoin es que es tremendamente volátil.

Al ser tan volátil, no nos podemos apalancar en la estacionalidad. Porque sea Navidad o porque sea verano, no vas a comprar o vender más Bitcoin. Esta es una de las conclusiones que nos aporta este experimento.


### 485.E1.U4.1.2_Algoritmo_Predictivo_de_Bitcoin

#### 1 | Movimientos de Ballenas
A priori, es difícil de predecir, pero se podría monitorizar.

Es como tener un tracking de estos movimientos de las ballenas para tener un lapso de tiempo que nos permita tener ventaja competitiva. 
- Para ello, es importante conocer las características de Blockchain. 
- Sabemos que es inmutable, que es segura, pero que es trazable. 
- Desde aquí, podemos estudiar los movimientos de wallets calientes y frías en el momento que se retira el dinero, con lo cual podemos detectar los movimientos de las ballenas. 
- Se trata de un ejercicio de tracking muy potente y muy exhaustivo, pero que nos podría arrojar luz para trackear movimientos de ballenas, que es lo que más sombras nos aporta en este caso.

Sabemos que los movimientos de ballenas son importantes, pero recordad que pasó también con otro factor muy importante aparte de las ballenas: Los Cisnes Negros.

![[485.E1_Algoritmo_Predictivo_de_Bitcoin_1.png]]

#### 2 | Los Cisnes Negros
¿Qué sucedió cuando vivimos la pandemia?: El Bitcoin cayó hasta los 6000 €. Esto es una afectación muy directa de un Cisne Negro. 

Los cisnes negros son eventos que suceden a lo largo de la historia que son impredecibles. En este caso tenemos un ejemplo muy claro como fue el Covid-19. 

¿Se pueden monitorizar o trackear los Cisnes Negros? 
- Sí, los podríamos monitorizar o trackear, es decir, lo que haríamos sería añadir una variable “Cisne Negro” y darle un valor desde 0 hasta 1. 
- Con respecto a la limitación de vida podríamos medirlo en una escala y lo tenemos registrado por fechas. 
- Sabemos en qué momento se han producido las restricciones. 
- Los Cisnes Negros, una guerra o una pandemia, no se pueden predecir a 2 o 3 años vista, eso sería imposible. Pero sí que es posible predecirlos a varios meses, porque no se invade un país de la noche a la mañana. 
- Antes, hay muchas noticias al respecto, muchas conjeturas, declaraciones y sí que podíamos trackear las noticias y detectar, mediante los sucesos y su grado de intensidad hay, la probabilidad de que haya un conflicto bélico. 

Esto es un ejercicio bastante sencillo, con IA. Lo mismo ha pasado con el Covid-19, podríamos hacer una predicción de casos, por ejemplo, 100 casos en China. Monitorizamos las noticias y, si empiezan a subir el pulso, sabemos qué se pasa a 500, 1000, 2000 casos. Podríamos hacer una predicción de pandemia. 

> Ser capaces monitorizar los cisnes negros puede darnos una ventaja competitiva de cara a la predicción de este tipo de algoritmos.

Hasta ahora, es más fácil de rastrear un Cisne Negro que el movimiento de ballenas, aunque se puede rastrear, como hemos dicho, haciendo análisis On Chain.

#### 3 | Sentiment Analysis
Y el tercer factor sería el Sentiment analysis, cuyo más famoso es Elon Musk y sus hilos sobre y declaraciones sobre Twitter, que han generado variaciones importantes sobre el precio del Bitcoin. 

El caso de Elon Musk es un caso extraordinario, estamos hablando un mega influencer, pero no el concepto de influencer que conocemos, sino un influencer a nivel mundial que está cambiando la historia. Van a haber pocos genios como él en la historia, capaces de revolucionar tantos sectores. 

Es por ello que la voz de influencers como Elon Musk o Bill Gates, son muy escuchadas. Tienen una relevancia histórica muy potente. Cuando hablamos de Sentiment analysis no es simplemente escuchar a Elon Musk, podíamos hacer un tracking de cuánto se está hablando en las redes sobre Bitcoin, ya sea en Twitter o en otra red social.

Al final, los algoritmos de Sentiment analysis están entrenados para que, cuando lean un texto, decidan si ese contenido es positivo, negativo o neutro respecto al concepto que estamos monitorizando. De esta forma, sí que podemos medir el sentimiento que hay en las redes sociales, hacia un concepto o un hashtag concreto. En este caso sería Bitcoin. 

Y sí que hay estudios que demuestran que hay una correlación bastante directa, como puedes ver en la gráfica del Sentiment analysis con el precio del proyecto escrito y, en este caso, el Bitcoin. 

![[485.E1_Algoritmo_Predictivo_de_Bitcoin_2.png]]

**Aquí concluye nuestro experimento, resumamos lo que hemos hecho:**
1. Capturar sobre Bitcoin: Timestamp y precio 
2. Añadir variables
3. Reducir esas variables
4. Hacer experimentos
5. Probar diferentes algoritmos 
6. Analizar los resultados 
7. **Para terminar:** Proponer posibles mejoras de variables para poder desarrollar un algoritmo de predicción del Bitcoin con éxito a una ventana temporal que supere los 30 días

En los proyectos de Machine Learning e Inteligencia Artificial, la parte táctica, estratégica y lógica es muy importante para saber qué variables hay que trackear para poder alcanzar la máxima precisión.


### 486.E1.U4.1.3_Algoritmo_Predictivo_de_Bitcoin

Una vez hemos capturado los datos, agregado las variables y hecho una selección, nuestro Dataset, que había comenzado con dos columnas: _Timestamp_ y _Precio_, ha pasado a tener 38 columnas.

Aquí ya podemos empezar a entrenar nuestros modelos. Para ello, seleccionamos diferentes modelos de Machine Learning y de Deep Learning y vamos a un poco analizar la performance. En esta gráfica, lo primero que hacemos es observar la performance que tiene en los datos de entrenamiento. 

Recordemos que partíamos el Dataset en entrenamiento y test, 80/20 o 70/30. Lo que vamos a hacer primero es ver estos modelos de Machine Learning o de Deep Learning para saber cuál es el que mejor aprende y el que mejor se ajusta a la curva. 

> Es importante remarcar que, lo que queremos, es que se ajuste más o menos bien y no exactamente, punto por punto, para no llegar al Overfitting.

Queremos que tenga una coherencia, que pase siempre con ese concepto de media y varianza, que al final lo que nos va a decir es que va a poder generalizar bien el modelo.

Una vez realizados los experimentos en entrenamiento, vamos a pasar a realizar los experimentos en test. Estamos haciendo predicción a 1, 5 y 30 días. Vamos a analizar las diferentes métricas de regresión.  En este caso, una métrica útil podría ser el [Root-mean-square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation). 

![[486.E1_Algoritmo_Predictivo_de_Bitcoin.png]]

#### Root-mean-square error
**Por qué puede ser útil?**
Porque presta muchísima atención a los Outliers, un concepto clave para entender la naturaleza de nuestro problema.

**¿Qué ocurre en este caso?**
Fijémonos en que la gráfica de Bitcoin hace picos muy marcados, picos y valles de forma constante. Esto, si la naturaleza del problema fuese otra, serían, probablemente, Outliers. 

Pero en todo lo que está relacionado con series temporales de Stock Market, hay que prestar atención a los Outliers, porque estos Outliers hablan de hitos en el desarrollo, en el Roadmap o en la adopción que han hecho que cambie este Momentum.

Root-mean-square error está hablando de una distancia respecto a la recta de predicción, a nuestra fórmula que hemos sacado de alguna forma para predecir el Bitcoin, está hablando de una media, de una distancia que hay desde nuestra predicción hasta el valor real. Con lo cual, vamos a utilizar la métrica del R2 ajustado.

**¿Por qué?**
Porque nos arroja valores mucho más entendibles. Vamos de 0 a 1, donde 0 es 0% de precisión y 1 es 100% de precisión. 

#### Resultados a 1 día.
Los resultados a 1 día son muy buenos, valores muy elevados. Hay varios modelos que hacen predicciones que superan el 90%. 

Nuestros modelos podrían estar haciendo una predicción de que mañana el Bitcoin, ya no es que va a subir o va a bajar, hace una predicción de a cuánto va a estar. Y estamos hablando valores cercanos al 95%. 

Por tanto, ¿os imagináis saber mañana que el precio del Bitcoin va a estar en lugar de 28000 a 32000 o una precisión del 95%? ¿Os aportaría valor? ¿Haríais movimientos en vuestras wallets, por ejemplo? 

Estamos hablando de que esto tiene un potencial enorme ¿no? Pero claro, es cierto también que estamos hablando de espacios a short, a 1 día.

#### Resultados a 3 días.
Pasamos, prácticamente, del 95% de precisión al 65%, aquí hay un salto importante.

La performance, tanto de Machine Learning como de Deep Learning, comienza a caer. 

También observar algo que hemos contado en clases anteriores, que los resultados de Deep Learning, en términos generales, son bastante malos.

**¿Por qué son malos?**
Porque realmente hay muy pocos datos para trabajar con Deep Learning. Esto es lo que provoca. Sí que es cierto que hay un tipo de algoritmos de Deep Learning, como son las redes neuronales recurrentes o RNN, en este caso, las LSTM, _long short term memory_, que funcionan muy bien para este tipo de señales. 

Es decir, que están mirando el pasado muy lejano y el pasado a corto, de alguna forma. Y realmente tiene una performance muy buena para señales de este tipo. Lo que ocurre es que no tenemos suficiente historial, suficiente cantidad de datos.

#### Resultados a 30 días.
Si observamos ya los resultados a 30 días, estamos viendo que son desastrosos. 

Estaríamos hablando que no aportamos ningún valor, es decir, aportamos el mismo valor que tira lanzar una moneda al aire. Por tanto, si os fijáis un poco y analizamos, diríamos que como conclusión, tenemos una ventana temporal de 1 día a 3 días donde tenemos una buena precisión y a medida que se acerca a los 3 días, empieza a caer, lo cual a lo mejor nos da un margen de 1 día y medio. 

Claro, a 30 días el momento es imposible de predecir con esta técnica y sobre todo quiero que os fijéis en algo.

**¿Por qué a 30 días no somos capaces de predecir el cambio de Momentum?**
La respuesta estaría en entender que hemos utilizado variables que son indicadores que están en el mercado, por decir alguna forma, que ya están generados, que simplemente los hemos ingestado. Y que, realmente lo que nos está diciendo es que a corto plazo, estos indicadores a un día, un día y medio, explican algo.

Pero a más de dos días ya no explican algo. Aquí hay otro tipo de hechos o de eventos que cambian el Momentum y afectan en la volatilidad.

**¿Cuáles son estos elementos?**
Básicamente, tenemos que entender que agentes o eventos externos están influyendo al cambio de precio del Bitcoin, que no sean los indicadores financieros clásicos.

**¿Qué influye?**
Lo primero que sabemos que influye son los movimientos de las ballenas. Son operaciones que mueven en torno a entre 1000 y 5000 Bitcoin.

**¿Qué está ocurriendo?**
Que si de momento se juntan un grupo de inversores y deciden hacer un movimiento muy potente, esto afecta directamente al precio del Bitcoin.


### 487.E1_Machine_Learning-video

### Machine Learning
![[487.E1_Machine_Learning.mp4]]
[Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866144-u5-1-1-machine-learning-jose-peris)

Big Data es la gasolina de nuestros algoritmos. Cuando hablamos de Big Data, tenemos que hablar de las 4 Vs. ¿Y por qué 4 Vs? Volumen, variedad, velocidad y veracidad. Muy a menudo en los medios, sobre todo escritos, vemos esta palabra también utilizada de forma errónea en lo que al Machine Learning se refiere. Por ejemplo, si estamos viendo una publicación sobre las el Big Data del clásico, de un partido de fútbol de Madrid contra Barça. ¿Qué ocurre? Que aquí tenemos un histórico, hay una estadística de muchos años de enfrentamientos de muchos jugadores y muchos eventos y acciones que han pasado. Estábamos comentando que los datos cuentan historias. Cuando se plantean este contexto, el concepto de Big Data, estamos hablando de visualización de datos, de KPIs, de Key Points Indicators. Realmente es una representación visual de datos. de estos indicadores. Pero cuando hablamos de Big Data en Machine Learning, estamos hablando de nuestra gasolina. Estamos hablando de poner los datos a trabajar. ¿Qué significa volumen? Volumen significa que necesitamos cantidades inmensas de datos. Para que os hagáis una regla muy básica, a más datos, más precisión o más accuracy, que es nuestro objetivo. Pero también necesitamos variedad. ¿Por qué? Porque necesitamos datos heterogéneos. Es mucho mejor tener datos heterogéneos que homogéneos. ¿Por qué? Porque necesitamos que nuestro algoritmo vea muchísimas casuísticas. Es como el ejemplo del padre que le enseña al niño lo que es un vaso. Es mejor que vea mil modelos de vasos que vea diez modelos de vasos. ¿Por qué? Porque cuando realmente salga a la calle y tenga que detectar lo que es un vaso, a más ejemplos vistos, más capacidad de reconocimiento. ¿Qué es la velocidad? Bueno, cuando hablamos de velocidad... básicamente nos referimos a la ingesta de los datos. En este caso haría referencia a la capacidad también que tenemos hoy de trabajar con datos en streaming. También añadir aquí que es importante el saber que los algoritmos se entrenan pero se pueden constantemente ir reentrenando. A más datos que entran se van reentrenando y van mejorando su precisión. Ya que no referimos con veracidad pues bueno estamos hablando de la calidad del dato. Estamos hablando de que estos datos hay que procesarse, hay que transformarse, lo primero que hacemos siempre es visualizarlos para ver que no hayan incongruencias. ¿Por qué? Porque pueden haber pasado eventos específicos en una fecha referente a cualquier caso de uso que realmente estemos hablando de un caso aislado. Y esto puede realizar una distorsión en el planteamiento de nuestro algoritmo. Por tanto, simplemente hace referencia. al proceso de ETL. Para comprender mejor este ejemplo quiero que hagamos el caso de Teachable Machine de Google donde vais a poder comprobar vosotros en casa qué son las cuatro V's. Para ello lo que me gustaría es que presionaseis en el enlace que tenéis aquí en la plataforma y que entrenéis un detector de objetos. Aquí lo que veréis es que vamos a entrenar un algoritmo de visión artificial. Lo que va a hacer es clasificarnos diversas clases las que nosotros determinemos. Podéis coger cualquier objeto que tengáis en casa, asignarlo a una clase, podéis coger una botella, podéis coger un lápiz y simplemente lo que tenemos que hacer es configurarlo y al presionar en la cámara lo que veréis es que os genera en la videocámara, la webcam os genera una cantidad infinita de imágenes. Lo que tenéis que hacer es mover el objeto en muchas posiciones. para comprobar el efecto de variedad. Seguidamente, simplemente tenéis que seguir los pasos, entrenar el modelo y comprobar su performance. Siguiendo con el concepto de Big Data, la traducción que tiene de una forma física o de una forma más tangible sería el dataset. ¿Qué es un dataset? Un dataset es un conjunto de datos. Para que os hagáis una idea, a día de hoy el Big Data puede llegarnos en forma de dataset de diversas formas. Una muy simple y común sería un archivo.txt que simplemente deberemos transformarlo para llevarlo a nuestro terreno. Pero el tipo de archivo dominante es el CSV. También podemos tener ficheros Excel o incluso archivos PDF. Cabe destacar también que el sonido también son datos, aunque debo remarcar aquí que el sonido como tal no existe para el Machine Learning, se convierte en un espectrograma, igual pasa a ser una imagen. Y podemos tener también vídeos e imágenes. ¿Qué es un dataset? Un dataset es nuestro conjunto de datos. Cuando estamos hablando de dataset, vamos a diferenciar entre dos grandes grupos como serían los datos tabulares, el primer grupo que es el más común o cuando estamos hablando del tema de visión artificial que serían imágenes o vídeo. Vamos a centrarnos en el primer grupo de los datos tabulares y vamos a ver al final lo que vamos a ver de una forma muy simple, es una tabla, imaginaos una tabla de Excel con filas y con columnas. Normalmente, hablamos de un dataset de calidad, por ejemplo, una buena cifra sería tener, por ejemplo, 5.000 o 10.000 filas hacia adelante y de 8 a 10 columnas, que estas serían nuestras variables. Ahí estaría nuestro target, nuestro objetivo a predecir. Es cierto que se pueden llegar a hacer pruebas de concepto con 1.000 filas, utilizando Machine Learning, pero no es lo recomendable. Si queremos hablar de buen tamaño, pues estaremos hablando de 100,000 filas para garantizar una buena calidad. Y esto nos permite utilizar todo tipo de algoritmos, tanto machine learning como deep learning. Cuando abrimos un data set de datos tabulares, nos podemos encontrar diferentes tipos de datos. Por un lado, tenemos los datos double, que serían simplemente números con fracción, por ejemplo, 1.7854. o nos podemos encontrar tipos de data como íntegra, que en este caso no habrían partes fraccionales, por ejemplo un 7. También tenemos las strings que hacen referencia a categoría, por ejemplo imaginemos que estamos hablando de una variable donde simplemente añadimos si un paciente padece algún tipo de enfermedad y tenemos cinco opciones, por ejemplo, una de ellas es diabetes. Esto sería... una string, estamos hablando de una categoría, no estamos hablando de números, estamos hablando de palabras, por decirlo de alguna forma simple. Por otro lado también podemos encontrar el timestamp, como su nombre indica es simplemente una fecha, 24 del 7, 1997. Simplemente tenemos que tener en cuenta que este tipo de... hay que también enseñarle a nuestro sistema que al ser marcado como timestamp le estamos diciendo que no es un número corriente. sino que está haciendo una relación a un evento temporal sucesivo. O por ejemplo podemos tener un diccionario. Un caso muy común del diccionario sería cuando vamos al supermercado y hacemos nuestra bolsa de la compra, ahí tenemos un diccionario de artículos, de palabras, con los objetos, con los ítems que hemos comprado. Otro tipo de datos serían imágenes. JPEG o JPEG o simplemente documentos donde encontraríamos por ejemplo, pues, reviews, por ejemplo, documentos de texto donde encontraríamos, pues, Great Food, Excellent Service. Esto sería un poco el abanico de datos, de tipología de datos que podemos encontrar. Nuestra primera misión es, en el momento que abrimos un dataset, es identificar los diferentes tipos y dependiendo del objetivo que queramos veremos qué tratamiento hay que aplicar a cada tipo de datos. ¿Qué es ETL? ETL en inglés significa Struct, Transform and Load. La ETL quizás es el proceso más pesado y más largo para un científico de datos porque como os decía estábamos hablando que los algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial. Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente, en unas condiciones diferentes. Por tanto, nuestra primera misión como científico de datos es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. Normalmente, el principal problema que solemos encontrar y el que primero vamos a buscar es si hay valores nulos. Esto es bastante común, que haya diferentes variables, recordad que las variables son las columnas donde encontremos valores nulos. y muchas veces vienen marcados por NAN o un simple interrogante. Lo primero que tenemos que hacer siempre es visualizar los datos, porque si no visualizamos los datos nunca vamos a entender la historia. Cuando pintamos las gráficas de las diferentes variables, vamos a observar qué distribución tienen los datos, vamos a también a observar cuántos valores nulos tenemos y vamos a observar diferentes incongruencias. Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo y simplemente implica ir comprobando columna a columna el tipo de dato que tenemos y ver qué transformación le tenemos que aplicar. para que podamos trabajar de forma correcta con nuestros algoritmos. También comentaros que a día de hoy, cuando empezamos en este mundo, ya sabéis que el tema de los datos y la privacidad es un tema bastante recurrente. Lo primero que quiero comentar es que en el campo de Machine Learning los datos se tratan siempre de forma anonimizada, es decir, para nosotros no tiene valor el nombre de una persona, se trabaja con IDs. Pero, ¿es cierto? que si queremos practicar y entrar en este mundo, tenemos repositorios como Kaggle o como Google Dataset, donde podemos descargar datasets de todo tipo de casos de uso. Esto nos será muy útil para poder entrenar nuestros algoritmos y ampliar nuestro conocimiento y nuestra destreza. El desarrollo de un algoritmo sigue unos pasos pautados. que vamos a narrar a continuación. El primer paso se llama Data Collection. Aquí se trata simplemente de descargar los datos, de capturarlos. Esta parte es muy importante. Normalmente el que mejor captura el dato es el que mejor algoritmos desarrolla. Por tanto, la primera parte va a ser siempre descargarlo. Nos podemos encontrar en muchos escenarios. Muchas veces, pues a lo mejor toca descargarlos desde una API. O sucede que a lo mejor tenemos que incluso nosotros buscarnos unos datos por un lado y otros por otro y después fusionarlos. Aquí nos podemos encontrar en múltiples escenarios. Vamos a hablar del proceso de desarrollo de un algoritmo. El primer punto indispensable es el business analysis, es decir, para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema, es decir, imaginaros que viene una empresa a pedirnos que desarrollemos un algoritmo predictivo de ventas, por ejemplo. El primer punto sería hacer una entrevista con la empresa para obtener un briefing y comprender el caso de uso a la que se trata. perfección. Debemos entender todos los skate holders, todas las variables que están afectando, cómo funciona ese negocio y hacer de forma intuitiva una primera hipótesis de qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. Esta parte es fundamental. De hecho es una parte en la cual muchos científicos de datos quizás se pierden demasiado en la programación. Aquí una parte que es de táctica y estrategia. de lógica, de entendimiento de negocio y otra parte que es de programación y desarrollo. Una vez hemos hecho esta primera parte de Business Analysis, pasaríamos al Data Collection o la captura del dato. Aquí simplemente analizaremos, seguimos con el caso de esta empresa, si los datos son internos, son externos, si nos vamos a descargar de una API, desde una base de datos o si nos lo van a proporcionar de forma aislada en un CSV. Una vez hemos coleccionado estos datos, los hemos capturado, pasaríamos a prepararlos, a Data Preparation o también ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, pasamos a limpiarnos y evaluamos la calidad del dato, porque muchas veces nos va a pasar que a lo mejor tenemos 10.000 filas, pero cuando hacemos el proceso de ETL nos podemos quedar en 4.000 filas. Esto es muy común, ¿por qué? porque la calidad del dato es anómala, con lo cual hay que pensar que el algoritmo trabaja de una forma muy matemática y muy fina, por tanto si nuestra gasolina contiene algún error la va a reproducir en el resultado, por tanto esta parte es crucial y nos puede llevar muchísimo tiempo. El siguiente paso sería escoger un modelo, como os decía estos son cajas de herramientas y a medida que vamos avanzando en conocimiento de un golpe de vista ya prácticamente sabemos qué tipo de modelo puede encajar en nuestro en nuestro problema, de ahí pasaríamos a entrenar el modelo, luego explicaremos qué significa un entrenamiento de un algoritmo y después pasaríamos a la evaluación, la evaluación al final, esto se trata básicamente de que nuestro modelo coge unos datos, aprende unos datos históricos, hace unas predicciones y contrastamos los resultados. ¿Cómo los contrastamos? Con datos de test. ¿Por qué? Porque no utilizamos todos los datos. Nosotros vamos a lanzar predicciones que las vamos a contrastar con la realidad. Esto es la evaluación. Aquí normalmente también lo que se hace es iterar con el parameter tuning, imaginaros que hemos llegado a una curacy del 80% y no es suficiente para nuestro caso de uso, porque un 80% puede ser muy positivo para ciertos casos de uso, imaginaros para una predicción de ventas pues ya es un valor interesante, pero si estamos prediciendo si una persona tiene cáncer o no, no podemos fallar un 20%. es donde aquí estableceremos nuestros umbrales, si el 80% es suficiente iteraremos y pasaremos al parámetro tuning donde aquí lo que vamos a hacer es optimizar los parámetros del algoritmo para aumentar la precisión y finalmente la última parte serían las predicciones o el deploy o puesta en producción. Imaginemos que somos un banco y esto es un caso real, porque a día de hoy sucede. Un banco quiere saber a cuántos clientes les quiere dar un crédito o les puede dar un crédito. ¿En qué se basa un banco para darte un crédito? Pues va a estudiar todo tu historial de ingresos, gastos, de impagos, de cobros, etc. Imaginaros una base de datos de millones de personas. Si tenemos que hacer un estudio manual, fila a fila, ID a ID, esto sería una labor muy tediosa. ¿Cómo se entrenaría? un algoritmo de machine learning para hacer una predicción de si a esta persona debemos darle el crédito o no. En este caso lo que haríamos sería ir a la base de datos, descargaríamos un dataset donde tendríamos un histórico de 10 años atrás de aquellos usuarios con unas características y al lado tendríamos una etiqueta supervisada, es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no. Es decir, si se le debe dar un préstamo o no. ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo. Entonces, a partir de aquí, podríamos hacer un algoritmo predictivo de forma que en el momento que entre un usuario nuevo y el algoritmo esté funcionando haría una predicción si a este usuario hay que darle crédito o no. ¿Por qué? Porque en base a su historial, de alguna forma. Entonces, aquí estamos hablando de que en el dataset contenía una columna donde si teníamos el histórico de un sí o un no, o de impago o de pago. Esto es supervisado. Supervisado significa que tenemos etiqueta, que conocemos el resultado de antemano, en el pasado. Cuando hablamos de lenguaje de aprendizaje no supervisado, estamos hablando de que los datos nos vienen desordenados. O sea, imaginaos fichas de diferentes formas, ¿vale? Triángulos, cuadrados, círculos, y nos llevan todas las fichas desordenadas. Nosotros, cuando es aprendizaje no supervisado, lo que vamos a hacer es ordenarlas, clasificarlas. ordenarlas, clusterización, esto significa poner los triángulos con los triángulos, los círculos con los círculos y los cuadrados con los cuadrados. Por tanto quiero que diferenciemos aquí no supervisado datos desordenados. Está supervisado, tenemos una etiqueta, sabemos qué ha pasado, sabemos el resultado, por tanto esto nos define el tipo de algoritmos que vamos a utilizar. Normalmente, aprendizaje no supervisado, los algoritmos más comunes serían los de clustering, donde lo que se hace es, en el momento que un cliente entra a tu página web, y esto está funcionando en muchos e-commerce, a día de hoy hay un algoritmo que en base a tus características y a tu patrón de uso, como experiencia de usuario, ya te clasifican con un tipo de cliente o un tipo de buyer persona. Imaginaros que se podría incluso clasificar como un cliente arriesgado, o un cliente conservador, o un cliente dudoso. Otro ejemplo muy común del aprendizaje no supervisado son los sistemas recomendadores, algo que vemos todos los días cuando estamos haciendo una compra en Amazon o cuando estamos en Netflix. Al final siempre tenemos recomendaciones. Estamos hablando en este caso de aprendizaje no supervisado. En el caso de aprendizaje supervisado vamos a diferenciar entre dos grandes grupos, como es clasificación y regresión, con esto retomamos la idea de que los algoritmos de IA de una forma muy sencilla y muy simplificada lo que hacen es predecir, esto sería regresión, predecir un precio, clasificación, sería predecir si va a suceder algo o no y clustering que es agrupar. Y un tercer bloque. que sería un poco más técnico y de nicho, sería el Reinforcement Learning. Reinforcement Learning es una técnica que se utiliza sobre todo en el ámbito de los videojuegos. Es la típica afirmación de estoy jugando contra la máquina o la máquina me ha vencido. Al final, este tipo de aprendizaje se basa en un sistema de castigo o recompensa. Así que es cierto que está cogiendo bastante forma y se está utilizando ahora un poco más incluso en el trading. Ya existen modelos de reinforcement learning que se están utilizando para el tema del trading, porque al final por este castigo y recompensa aprende a hacer trading en short. Esto sería el concepto de los market makers, pero aún son modelos experimentales y falta por demostrar si pueden sustituir o reemplazar de forma eficiente en este campo a la programación tradicional.


### 488.E1.U5.1.1_Introducción_al_Machine_Learning

### ¿Qué es Big Data?
![[488.E1_Introducción_al_Machine_Learning.png]]

El Big Data es la gasolina de nuestros algoritmos. 

Cuando hablamos de Big Data, tenemos que hablar de las cuatro uves:
1. Volumen
2. Variedad
3. Velocidad
4. Veracidad

#### 1 Volumen
**Necesitamos cantidades inmensas de datos.**
A más datos, más precisión, así que es el principal objetivo.

#### 2 | Variedad
**También necesitamos variedad, datos heterogéneos. Es mucho mejor tener datos heterogéneos que homogéneos para que nuestro algoritmo vea muchísimas casuísticas.**

Es como el ejemplo del padre que le enseña al niño lo que es un vaso. Es mejor que vea mil modelos de vasos que diez modelos de vasos, porque cuando salga a la calle y tenga que detectar lo que es un vaso, a más ejemplos vistos, más capacidad de reconocimiento.

#### 3 | Velocidad
**Básicamente, nos referimos a la capacidad de ingesta de los datos.**
En este caso haría referencia a la capacidad también que tenemos hoy de trabajar con datos en streaming. Los algoritmos se entrenan constantemente a medida que más datos entran y van mejorando su precisión.

#### 4 | Veracidad
**La calidad del dato.**
Hay que procesar y transformar los datos para, posteriormente, visualizarlos para comprobar que no existan incongruencias. Pueden haber sucedido eventos específicos en una fecha, un caso aislado, y que esto cree una distorsión en el planteamiento del algoritmo.

#### Google Teachable Machine
**Este es un ejemplo perfecto para ver el funcionamiento de las 4 V.**
Para ello, debéis ir [este enlace](https://teachablemachine.withgoogle.com/train/image) y que entréis un detector de objetos.
Aquí lo que veréis es que vamos a entrenar un algoritmo de visión artificial. 
- Lo que va a hacer es clasificarlos en diversas clases, las que nosotros determinemos. 
- Elegid cualquier objeto que tengáis en casa, podéis coger una botella, un lápiz y, simplemente, lo que vamos a hacer, es configurarlo, le asignaremos una clase.
- Al presionar en la cámara veréis que lo que os genera, es una cantidad infinita de imágenes. 
- Ahora tenéis que mover el objeto en muchas posiciones para comprobar el efecto de variedad. 
- Seguidamente, solo hay que seguir los pasos, entrenar el modelo y verificar su performance.

#### Dataset
**La forma tangible del Big Data, sería el Dataset: Un conjunto de datos.**
El Big Data puede llegarnos en forma de Dataset de diversas formas. Una muy simple y común sería un archivo txt. Simplemente, deberemos transformarlo para llevarlo a nuestro terreno. 

No obstante, el tipo de archivo dominante, es el CSV. También podemos tener ficheros excel o incluso archivos PDF. Cabe destacar también que el sonido también son datos, aunque hay que remarcar aquí que el sonido como tal no existe.  
  
Para el Machine Learning se convierte en un espectrograma, con lo cual pasa a ser una imagen. También podemos tener videos e imágenes en nuestro Dataset.

Vamos a diferenciar entre dos grandes grupos.
1. Datos Tabulares (el más común)
2. Visión Artificial (imágenes o vídeo)

#### Vamos a centrarnos en el primer grupo, Datos Tabulares.
- Vamos a imaginar una tabla de Excel con filas y columnas, un Dataset de calidad con de 5000/10000 filas hacia adelante de 8 a 10 columnas. 
- Esto serían nuestras variables y ahí estaría nuestro Target, nuestro objetivo a predecir. 
- Es cierto que se pueden llegar a hacer pruebas de concepto con filas utilizando Machine Learning, pero no es recomendable. 

Si queremos hablar de un buen tamaño, estaríamos hablando de 100.000 filas para garantizar una buena calidad y esto nos permite poder emplear todo tipo de algoritmos, tanto Machine Learning como Deep Learning.

Cuando abrimos un Dataset de datos tabular nos podemos encontrar diferentes tipos de datos: 
- **Por un lado, tenemos los datos Double,** que serían simplemente números con fracción, por ejemplo, 1.7854. 
- **También podemos encontrar tipos de data como integral.** En este caso no habría partes fraccionales, por ejemplo, un 7.
- **También tenemos las Strings, que hacen referencia a categorías:** Imaginemos que estamos hablando de una variable donde simplemente añadimos si un paciente padece algún tipo de enfermedad y tenemos cinco opciones, una de ellas la diabetes, esto sería una string usando una categoría. No estamos hablando de números, estamos hablando de palabras. 
- **Timestamp: Como su nombre indica, es simplemente una fecha (Día / Mes / Año).** Simplemente, tenemos que tener en cuenta que hay que enseñarle a nuestro sistema que al marcarlo como Timestamp, estamos diciendo que no es un número corriente, sino que está haciendo una relación o un evento temporal sucesivo. 
- **También podemos tener un diccionario:** Un caso muy común del diccionario sería cuando vamos al supermercado y hacemos nuestra lista de la compra. Ahí tenemos un diccionario de artículos de palabras con los objetos, con los ítems que hemos comprado. 
- **Imágenes:** En PNG o JPG.
- **Documentos de Texto:** Por ejemplo, reviews.

Este sería un resumen del abanico de tipología de datos que podemos encontrar. Nuestra primera misión es en el momento que abrimos un Dataset, será identificar los diferentes tipos y dependiendo del objetivo que queramos lograr, ver qué tratamiento hay que aplicar a cada tipo de datos.


### 489.E1_Machine_Learning-video

### Machine Learning
![[489.E1_Machine_Learning.mp4]]
[Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866154-u5-1-2-machine-learning-jose-peris)

Cómo evaluamos nuestro algoritmo. Aquí vamos a diferenciar dos grandes grupos y recordad que siempre hablando desde el punto de vista de aprendizaje supervisado cuando tiene etiqueta, cuando hay una mano detrás, cuando sabemos el resultado de antemano los grandes grupos, uno era la regresión y otro era la clasificación la regresión, pues cómo vamos a evaluarlo, pues bueno como podéis ver en la gráfica básicamente lo que tenemos son los data points y nuestro algoritmo se vería representado como es la línea verde que tenemos, es realmente esta línea verde, que esto es una regresión lineal, simplemente si os fijáis lo que ocurre es que pasa más o menos por el medio de la nube de puntos, ¿qué significa esto? Significa que esa recta tiene una fórmula matemática y esa fórmula matemática es la que nos permite predecir dónde va a caer el próximo punto. ¿Cómo medimos la precisión? Lo que hacemos es ver. nuestras predicciones frente a los resultados reales y lo que vamos a hacer es vamos a medir la distancia de cada punto real respecto a nuestra recta. y a partir de aquí midiendo esta distancia cada punto vamos a sacar una media y esa va a ser la media de nuestro error, esto cuando estamos hablando de regresión donde el valor sería 1824 euros, por ejemplo. ¿Cómo evaluamos la clasificación? La clasificación se evalúa mediante un instrumento llamado matriz de confusión y el nombre está muy buen puesto porque realmente estamos hablando que vamos a elaborar una matriz donde vamos a clasificar los diferentes resultados. En este caso, por ejemplo, vamos a tener dos columnas, una va a ser los resultados reales, y dos filas donde van a ser los resultados del algoritmo. Aquí, básicamente, lo que hacemos es contar cuántas veces hemos acertado y cuántas veces hemos fallado en la clasificación. Este concepto es un poco lioso, pero el hecho de que hemos pasado... a una pandemia ha ayudado a entenderlo un poco mejor, porque esto se basa de falsos positivos y falsos negativos. ¿Qué ocurre con el coronavirus, por ejemplo? Tenemos el caso de falso positivo y falso negativo, ¿no? A día de hoy todos sabemos ya lo que significa. Te pueden diagnosticar el coronavirus y no lo tenías. Eso sería un falso positivo. Pero también hay un falso negativo. te pueden diagnosticar que no tienes coronavirus y sí que lo tienes, ya has contagiado a todo tu entorno. Este es el concepto clave para la clasificación, porque cuando clasificamos si es sí o no, tenemos que ver si es un falso positivo o falso negativo. A continuación os voy a dejar aquí un ejemplo interactivo donde podéis un poco experimentar, experimentar lo que es el concepto de la matriz de confusión. Lo que vais a ver son dos grupos de imágenes, es un clasificador que representa imágenes que representan a comida y imágenes que no representan a comida. Lo que tenéis que hacer es clasificar las que van al lado. Tenéis que pensar que para evaluar una clasificación es muy importante tener en cuenta cuántos hemos acertado. En este caso miraríamos los... los true positive y los true negative y cuánto hemos fallado, los false positive o false negative. Lo que hace es comparar los que hemos acertado contra los que hemos fallado, lo que pasa que claro no es lo mismo fallar prediciendo una enfermedad que fallar prediciendo si una campaña de marketing va a funcionar sí o no, es aquí donde debemos establecer nuestro umbral. Existen otros métodos más avanzados pero a nivel general vamos a crear un método más avanzado. con estos conceptos base de medición y de evaluación de algoritmos. Vamos a seguir hablando de los dataset y sobre todo un concepto muy muy importante que es el entrenamiento y el test. Fijaros que estamos hablando que tenemos que medir y evaluar la predicción de nuestros algoritmos, ya sea en regresión como en clasificación. Para ello lo que vamos a hacer, nunca vamos a trabajar con el 100% de los datos. ¿A qué me refiero con esto? Imaginaros que me envían un dataset de 100.000 filas y 15 columnas, 15 variables. El primer paso que tenemos que hacer es... aparte de que primero haríamos un análisis del negocio, realizamos un proceso de ETL y lo siguiente que haríamos sería partir el dataset en entrenamiento y test. Hay diversas formas de partirlo, normalmente es un 80-20 aunque puede ser un 70-30. y hay formas de incluso partirlo más, entrenamiento, test y validación, pero en este punto en el que nos encontramos vamos a hablar de entrenamiento y test. Lo que vamos a hacer siempre es partir nuestro dataset, el 80% será de entrenamiento y el 20% será de test. ¿Esto qué significa? Que desde la fila 0 hasta la fila 80.000, van a ser nuestros datos de entrenamiento y desde la fila 80.000 hasta la 100.000 van a ser los datos de test. ¿Por qué? Porque lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos y una vez, una vez está entrenado, lo que vamos a hacer es imaginaros que íbamos, seguimos con el ejemplo, del algoritmo de predicción de si un cliente va a pagar un crédito o no, para saber si se lo podemos ofrecer o no. En este caso, lo que sucedería, que una vez nuestro algoritmo está entrenando con el 80% de los datos, nosotros simplemente cogeríamos los datos desde el test, ese 20% donde lo que veríamos realmente sería un ID de un usuario y todas sus características, todo su histórico, todo su histórico de cobros, de pagos, de ingresos de gasos con el banco y el algoritmo aquí haría una predicción. ¿Qué ocurre? Que como tenemos el dato real porque hemos partido en 80-20, podemos contrastar la predicción del algoritmo con el dato real, predicción contra realidad, siempre es este el patrón que seguimos, por tanto analizaríamos... dataPoint a dataPoint, ¿cuánto hemos acertado o cuánto hemos fallado? Si estamos hablando de regresión será un número, estaremos hablando de un error que este error pues haciendo la inversa es la precisión. Si estamos hablando de clasificación, en una primera instancia lo haríamos con una matriz de confusión y de forma más avanzada ya utilizaríamos análisis como el ROC o AUC. Pero nos vamos a quedar en esta base, en que para medir, para poder trabajar, tenemos que partir siempre en entrenamiento y en test. Vamos a hablar sobre la parte más importante. en Machine Learning y en Inteligencia Artificial, que son las variables. Recordad la estructura que es augmentada de un dataset, tendríamos muchas filas y tendríamos columnas. Estas columnas son las variables. Las variables realmente, de una forma entendible, serían aquellos parámetros que explican todo el contexto, es decir, vamos a llevarlo a cabo. un caso muy fácil, muy entendible como puede ser el caso de la hostelería. Bar, restaurante, un hotel... Pongamos que un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar a dos semanas vista. Si pensáis bien, si sabemos cuánto vamos a facturar, cuántos trabajadores vamos a necesitar, tenemos mucha ventaja competitiva porque realmente sabemos que no va a haber rotura. de stock, sabremos que va a haber un buen servicio, podemos preparar a sala y el personal para un determinado escenario, con lo cual estamos hablando de bastante retorno, esto es el roll de trabajar con Machine Learning y con IA. Sigamos en este caso, imaginaros que el caso común es que un cliente por ejemplo te dé su base de datos. y te dice, bueno, aquí tienes mi base de datos y lo que vas a tener es un timestamp, como hemos dicho anteriormente, porque tendremos fechas. tendremos un histórico de fechas, pongamos que tenemos 10 años y al lado tendremos una columna con la facturación de cada día. A día de hoy, lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo realmente a ojo. Entonces, ¿qué significa hacerlo a ojo? Pues bueno, que si calculamos un incremento o un descenso de x, porque este año es mejor o es peor. Esto nos lleva a que son predicciones muy inexactas, que rozan el 50%. Que 50% en Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada. Vamos a ver cómo configuraríamos este algoritmo, estas variables, para poder realizar un buen algoritmo. Lo primero es pensar con la lógica. Nosotros tenemos una fecha, un timestamp y una facturación, unos datos de facturación en euros. De una forma muy sencilla, ¿creéis que se factura lo mismo en hostelería un sábado o un viernes que un lunes o un martes? Evidentemente no, aquí un componente psicológico en el cual un lunes o un martes va a ser difícil que la gente acude más a los bares, sin embargo viernes o un sábado o un domingo sí, sabemos que obviamente esto va a afectar a la facturación. ¿En qué día de la semana estoy? Va a afectar directamente a mi facturación. Por tanto, el día de la semana es una variable muy importante que haciendo ETL la podemos añadir a nuestro dataset esto se traduciría si el día de la semana es el 1 o es el 7, o es el 2 o es el 3 ¿O qué otra variable podría afectar? Por ejemplo, la variable festivos por ejemplo, si es Halloween es obvio que ese día es probable que afecta la facturación o incluso la víspera Por tanto, el factor festivo en el calendario nacional es una variable también muy importante. Pero pensad que este bar está, este restaurante o este hotel está en España. Cuando está lloviendo, la gente va a los bares o a los restaurantes, le cuesta muchísimo más, nos cuesta salir de casa. Sin embargo, cuando hace sol, nos lanzamos a la calle. Por tanto, la... temperatura va a influir claramente también en nuestro objetivo, nuestro target, nuestra Y que se denomina Machine Learning que es la facturación. o por ejemplo las precipitaciones, la lluvia, que estaría también, lo podemos englobar dentro de esta misma variable, pero tenemos que tener en cuenta que hay otro tipo de variables que pueden afectar, porque imaginaros que alrededor de este restaurante se realiza un evento, hay un fin de semana de conciertos, donde vienen artistas de todo el mundo, ¿qué va a pasar? que va a haber mucha más gente alrededor Por lo tanto, esto en Matching Learning lo veríamos como evento, si hay evento sería un 1 y si no hay evento es un 0, por ejemplo. Es como traduciríamos esta información de forma binaria. Pero podrían afectar también otra serie de variables, como por ejemplo, qué día del mes es. No es lo mismo, no gastamos lo mismo el día 5 que el día 30. Los ánimos no son los mismos. Pero, por otro lado, también hay otras variables que podrían influir, que esto es Open Data, que son variables externas. Imaginaros que la tasa del paro se sitúa, juvenil, en un 40%. esto va a afectar también, esta variable externa, este OpenData va a afectar, o digamos que el PIP ha bajado muchísimo, todo este tipo, estos son lo que nosotros denominamos variables y es aquí donde está la génesis de la creación de un buen algoritmo y es realmente la clave, es comprobar qué variables son las que mejor explican este suceso o evento, a partir de aquí. ya podríamos, una vez hemos conformado nuestro dataset, hemos añadido esta serie de variables y hemos comprobado su importancia, podemos pasar a partir en entrenamiento y test y desarrollar nuestro algoritmo de nuestra caja de herramientas, tunearlo y conseguir la precisión adecuada. Para que os hagáis una idea, estaríamos hablando en términos cuantitativos, por ejemplo, en términos de precisión estaríamos hablando que si simplemente lo que vamos a tener es un timestamp de una columna con un valor de euros, pues podríamos situarnos en precisiones alrededor del 60%. En el momento que añadimos variables, variables que apuntan a qué paso en esos días y utilizamos Open Data, podríamos subir a precisiones del 90%, incluso del 95%. Lo cual, como podéis comprender, pues, como os decía, puede ser una ventaja competitiva muy potente. El campo del Machine Learning y la Inteligencia Artificial es un campo técnico, es cierto, pero no todo es desarrollo y tecnicismos. Realmente puede acceder gente desde el mundo del ámbito del negocio que va a tener que aprender una serie de técnicas que son perfectamente alcanzables. Al final, realmente lo que importa es una buena combinación de la parte táctica estratégica con la parte técnica.


### 490.E1.U5.1.2_Extract_Transform_and_Load

### Extract, Transform and Load (ETL)
Es, quizás, el proceso más pesado y largo para un científico de datos; los Algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial.

Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente en unas condiciones diferentes. Por tanto, nuestra primera misión, como científicos de datos, es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. 
- El principal problema que solemos encontrar son los **valores nulos,** situación bastante común, ya que hay diferentes variables (columnas). Los valores nulos, muchas veces, vienen marcados por un NAN o un simple interrogante.
- **Lo primero que tenemos que hacer siempre es visualizar los datos,** porque si no visualizamos los datos, nunca vamos a entender la historia.
- Cuando dibujamos las gráficas de las diferentes variables vamos a observar que distribución tienen los datos. Vamos a observar también, cuántos valores nulos o diferentes incongruencias tenemos. 
- **Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo** y, simplemente, implica ir comprobando, columna a columna, el tipo de datos que tenemos para ver qué transformación le tenemos que aplicar para trabajar de forma correcta con nuestro algoritmo.

#### Privacidad
Para que podamos trabajar de forma correcta con nuestros algoritmos, la Privacidad es un tema bastante recurrente.

En el campo del machine learning, los datos se tratan siempre de forma anonimizada, es decir, para nosotros no tiene valor el nombre de una persona; se trata de trabajar con ID's.

Pero es cierto que si necesitamos practicar y entrar en este mundo tenemos repositorios como Google Dataset, donde podemos descargar Dataset de todo tipo de casos de uso. 

Esto nos será muy útil para poder entrenar a nuestro Algoritmo y ampliar nuestro conocimiento y destreza.

#### El proceso de desarrollo de un algoritmo. 
##### 1 | Data Collection 
Esta parte es muy importante, pues, normalmente, quien mejor captura los datos es quien mejores algoritmos desarrolla.

Podemos encontrar muchos escenarios para realizar esta descarga, como hacerlo desde una API o, incluso, buscarnos unos datos, por un lado, y otros por otro para después fusionarlos. 

Aquí nos podemos encontrar en múltiples escenarios.

##### 2 | Business Analysis
Para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema.

Ejemplo:
Nos piden que desarrollemos un algoritmo predictivo de ventas.
1. Briefing con la empresa, expectativas y casos de uso para comprender todos los Script Holders y variables que afectan al negocio para elaborar una primera hipótesis.
2. A través de esa hipótesis averiguaremos qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. 

Esta parte es fundamental. De hecho, es una parte en la cual muchos científicos de datos se pueden perder por pensar demasiado en la programación. 

Hay dos partes muy bien diferenciadas
- Parte 1: Táctica, estrategia y lógica de entendimiento de negocio. 
- Parte 2: Programación y Desarrollo.

##### 3 | Programación y desarrollo
Una vez hemos hecho esta primera parte de Business Analysis de nuestro cliente, pasaríamos a analizar los datos que hemos capturado con la Data Collection.

Seguimos con el caso de la empresa.

Ahora analizaremos:
- Si los datos son internos o externos.
- Si los hemos descargado de una API o desde una base de Datos. 
- Si no nos lo van a proporcionar de forma aislada en un CSV.

Una vez hemos coleccionado estos datos, pasamos a preparar los ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, a limpiarlos y evaluamos la calidad del dato. Porque muchas veces nos va a pasar que a lo mejor partimos de 10.000 filas y cuando hacemos el proceso de ETL nos podemos quedar en 4000.

Esto es muy común cuando la calidad del dato es anómala, por lo que tenemos que pensar que el algoritmo trabaja de una forma muy matemática y muy fina, por tanto, si nuestra "gasolina" encuentra algún error, lo va a reproducir en el resultado. Esta parte es crucial y nos puede llevar muchísimo tiempo.

##### 4 | Escoger un modelo
Como decíamos, esto funciona como cajas de herramientas y, a medida que vamos avanzando en conocimiento, de un golpe de vista, ya podemos saber qué tipo de modelo puede encajar en nuestro problema. 

Una vez hecho esto, pasamos a entrenar el modelo. Luego explicaremos que significa un entrenamiento de un algoritmo.

##### 5 | Evaluación final
Nuestro modelo coge unos datos, aprende unos históricos, realiza unas predicciones y contrastamos los resultados con datos de test, y lo haremos así porque no vamos a utilizar todos los datos.

Nosotros vamos a lanzar predicciones que vamos a contrastar con la realidad. 

También es común iterar con el Parámetro Tuning.

Imaginemos que hemos llegado a una predicción de una precisión del 80%. pero no es suficiente para nuestro caso de uso. Este valor puede ser bueno para una predicción de ventas.

Pero en el caso de predecir una posible enfermedad, no podemos errar un 20%, por lo que estableceremos nuestros umbrales e iteraremos, pasando al Parámetro Tuning, que no es otra cosa que optimizar los parámetros del algoritmo para mejorar la precisión.

##### 6 | Deploy o Puesta en Producción
Imaginemos que somos un banco y queremos saber a qué potenciales clientes les podemos ofrecer un crédito.

¿En qué se basan?: En el historial de ingresos, gastos, impagos, cobros, etc. Imaginad si tenemos que hacer una revisión manual de una BBDD de millones de personas, fila por fila, ID por ID… Sería una labor muy tediosa.

¿Cómo se entrenaría un algoritmo de machine learning para hacer esta predicción?

Descargaremos un Dataset con un histórico de 10 años con unas características y al lado tendríamos una etiqueta supervisada, es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no, si se le debe dar un préstamo o no, ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo.

A partir de aquí, podemos crear un Algoritmo Predictivo de forma que, cuando entre un usuario nuevo y este algoritmo esté en funcionamiento, hará una predicción si a este usuario se le puede o no ofrecer un préstamo.

¿Por qué? Basándonos en su historial.  

Aquí estamos hablando de que en él dataset contenía una columna, donde si teníamos el histórico de un "sí" o un "no", de impago o de pago.

**Esto es supervisado, significa que tenemos una etiqueta donde colocamos el resultado de antemano en el pasado. Vamos a profundizar sobre esto en la siguiente sección.**


### 491.E1.U5.1.3_Aprendizaje_Supervisado_y_No_Supervisado

### Aprendizaje Supervisado y Aprendizaje No Supervisado
Cuando hablamos del lenguaje de aprendizaje no supervisado nos referimos a que los datos nos llegan desordenados.

Imaginemos fichas de diferentes formas: Triángulos, cuadrados, círculos y nos llegan todas las fichas desordenadas.

Cuando ese aprendizaje no está supervisado lo que vamos a hacer es ordenarlas y clasificarlas. Dicho de otra manera: Clusterizarlas.

#### Clusterización
La Clusterización sería ordenar esas formas; poner los triángulos con los triángulos, círculos con círculos y cuadrados con cuadrados.

![[491.E1_Aprendizaje_Supervisado_y_No_Supervisado_1.png]]

Diferenciemos.
1. **No supervisado:** Datos desordenados.
2. **Supervisado:** Etiquetado y clasificado, sabemos el histórico y el resultado.
3. **Reinforcement Learning:** Este es un campo más técnico y de nicho.

Todo esto nos define el tipo de algoritmos que vamos a utilizar.

#### 1 | No Supervisado
Los más comunes, en este caso, son conocidos como algoritmos de Clustering.

Por ejemplo: 
Un cliente entra a una web de e-commerce, donde, a día de hoy, hay un algoritmo que, basándonos en sus características y a su patrón de uso como experiencia de usuario, ya lo clasifica con un tipo de cliente o de buyer-persona.

Los podría clusterizar como:
- Cliente arriesgado
- Cliente conservador
- Cliente dudoso

**Otro ejemplo son los Sistemas Recomendadores,** algo que podemos ver todos los días mientras realizamos una compra en Amazon o cuando estamos en Netflix, pues al final siempre nos aparecen recomendaciones de otros productos, de series o de películas basándose en lo que consumimos habitualmente.

#### 2 | Aprendizaje Supervisado
En el caso de aprendizaje supervisado vamos a diferenciar entre dos grandes grupos:
1. Clasificación
2. Regresión

Con esto retomamos la idea de que los algoritmos de guía, de una forma muy sencilla y simplificada, lo que hacen es:
- **Regresión:** Predecir un precio.
- **Clasificación:** Predecir si va a suceder algo o no.
- **Clustering:** Agrupar.

#### 3 | Reinforcement Learning
Esta es una técnica que se utiliza, sobre todo, en el ámbito de los videojuegos. 
- Es la típica afirmación: “No estoy jugando contra la máquina” o “La máquina me ha vencido”.
- Este tipo de aprendizaje se basa en un sistema de castigo o recompensa, así que está cogiendo bastante forma y se está empezando a implementar incluso en el trading, donde ya existen algunos modelos.
- Por este castigo y recompensa se puede aprender a hacer trading en short.
- Esto sería el concepto de los Market makers, aunque todavía se trata de modelos experimentales y falta por demostrar si pueden sustituir o reemplazar, de forma eficiente en este campo, a la programación tradicional.

![[491.E1_Aprendizaje_Supervisado_y_No_Supervisado_2.png]]

#### Evaluando nuestro algoritmo
¿Cómo evaluamos nuestro algoritmo? Siempre teniendo en cuenta de que hablamos del Aprendizaje Supervisado, cuando tiene etiqueta, cuando hay una mano detrás, cuando sabemos el resultado de antemano.

Recordemos los dos grandes grupos: la Regresión y la Clasificación.

#### Evaluando la Regresión
Lo que tenemos son Data Points y nuestro algoritmo se vería representado con la línea verde, que no es más que una Regresión Lineal que pasa más o menos por en medio de la nube de puntos.

**¿Qué quiere decir esto?**
Significa que esa recta tiene una fórmula matemática, y que esa fórmula nos permite predecir donde va a caer el próximo punto.

**¿Cómo medimos la precisión?**
Lo que hacemos es, ver nuestras predicciones frente a los resultados reales y, lo que vamos a hacer es medir la distancia de cada punto real respecto a nuestra recta.

A partir de aquí vamos a ir midiendo esta distancia a cada punto y vamos a sacar una media, que va a ser la media de nuestro error.

Cuando estamos hablando de Regresión, donde el valor sería, por ejemplo, 1824 €.

#### Evaluando la Clasificación
Evaluamos la Clasificación mediante un instrumento llamado Matriz de Confusión.
- El nombre está muy bien puesto porque, realmente, vamos a elaborar una matriz donde vamos a clasificar los diferentes resultados.
- En este caso vamos a tener dos columnas, en una van a ir los resultados reales y dos filas a ir los resultados del algoritmo. 
- Aquí básicamente lo que hacemos es contar cuantas veces hemos acertado y cuantas veces hemos fallado en la clasificación.

> Este concepto, que puede resultar confuso, pero hemos pasado una pandemia que ha contribuido a que lo entendamos un poco mejor. ¿Por qué?

Porque se basa en falsos positivos y en falsos negativos.

¿Qué ocurre con el Covid? Tenemos casos de falsos positivos y falsos negativos y, a día de hoy, todos sabemos lo que significa.
1. **Falso positivo:** Te pueden diagnosticar con un virus y no lo tenías.
2. **Falso negativo:** Te dicen que estás bien, pero tenías el virus.

¡Ya has contagiado a todo tu entorno!.

Este es el concepto clave para la clasificación porque cuando clasificamos, SÍ es SÍ o es NO, tenemos que comprobar si se trata de un falso positivo o un falso negativo.

**En conclusión, tenemos que medir y evaluar la predicción de nuestros algoritmos, ya sean tanto de Regresión como de Clasificación.**

Para ello, lo que nunca vamos a hacer, es trabajar con el 100% de los datos. 

Imaginemos que nos envían un dataset de 100000 Filas y 15 Columnas, 15 Variables. 

El primer paso será, aparte de un primer análisis del negocio o un proceso de ETL, partir el Dataset en entrenamiento y Testing.


### 492.E1.U5.1.4_Variables

#### Nunca debemos trabajar con el 100% de los datos.
**Imaginemos, por ejemplo, que nos envían un Dataset de 100000 Filas y 15 Columnas: 15 Variables.**

El primer paso será, aparte de un primer análisis del negocio o un proceso de ETL, partir el Dataset en Entrenamiento y Testing.  

Hay diversas formas de partirlo. Normalmente, es un 80/20, aunque puede ser un 70/30. Y hay formas de incluso partirlo más: 
- Entrenamiento
- Test
- Validación

#### Entrenamiento / Test
**Cuando decimos 80/20, nos referimos a que el 80% será de entrenamiento y el 20% será de test.**
- Desde la fila 0 hasta la fila 80.000 van a ser nuestros datos de entrenamiento.
- Desde la fila 80.000 hasta la 100.000 van a ser los datos de test.

Seguimos con el ejemplo práctico de predecir si un cliente va o no a pagar un crédito.
1. Lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos.
2. Una vez entrenado ese 80%, usaremos el 20% restante para tener un ID de un usuario, todas sus características y todo su histórico de movimientos con el banco (pagos, ingresos, gastos) y el algoritmo aquí haría una predicción.
3.  Como tenemos el dato real porque hemos partido en 80/20, podemos contrastar la predicción del algoritmo con el dato real, pues nuestro patrón siempre es Predicción vs Realidad.
4.  Analizaremos, Data Point a Data Point, cuanto hemos acertado o fallado.
5.  Si estamos hablando de Regresión, el resultado será un número, estaremos hablando de un error.
6.  Este error, haciendo la inversa, nos da la precisión.
7.  Si, en cambio, estamos hablando de Clasificación, lo haríamos con una Matriz de Confusión y de forma más avanzada, como el ROC o el AUC.

  

### Las Variables
**Estamos hablando de la parte más importante en Machine Learning y en Inteligencia Artificial.**

Para ello, vamos a recordar cómo estaba compuesta la estructura de un Dataset: Filas y columnas. Las columnas son las variables.

Las variables son aquellos parámetros que explican todo el contexto. 

Vamos a imaginar un caso práctico como la hostelería (un bar, restaurante u hotel). 
- Un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar. 
- Si sabemos estos datos, tendremos mucha ventaja competitiva porque, realmente, sabemos que no va a haber rotura de stocks y habrá un buen servicio. 
- Podemos preparar la sala y el personal para un determinado escenario, con lo cual estamos hablando de bastante retorno. Este es el ROI de trabajar con Machine Learning e IA.

El caso más habitual es que un cliente te proporcione su base de datos, con lo que vas a tener un Timestamp, un histórico de fechas, por ejemplo,10 años. 
- Al lado de cada fecha, tendremos una columna con la facturación de cada día. 
- Lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo a ojo. ¿Y qué implica hacerlo a ojo? 
- Si calculamos un incremento un descenso de X porque este año es mejor o es peor, lo cual nos lleva a que a predicciones muy inexactas que rozan el 50%.  
- En Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada.

Vamos a ver cómo configurar estas variables, para poder realizar un buen algoritmo.

Lo primero es pensar con la lógica. Nosotros tenemos una fecha Timestamp y una facturación, unos datos de facturación en euros. Vamos a seguir con el caso práctico en la siguiente unidad.


### 493.E1.U5.1.5_Variables_-_Caso_Práctico

### Variables, Dataset, Open Data
**¿Se factura lo mismo en hostelería un sábado? ¿O un viernes que un lunes o un martes?**

Evidentemente no, aquí hay un componente psicológico: Un lunes o un martes va a ser difícil que la gente acuda más a los bares. Sin embargo, el viernes, un sábado o domingo sí y, obviamente, esto va a afectar a la facturación. 

Vamos a estudiar el porqué de posibles variables
1. Día de la semana
2. Festivos
3. Climatología
4. Eventos
5. Día del mes

#### 1 | Día de la semana 
El día de la semana es una variable muy importante y, haciendo el ETL, la podemos añadir a nuestro Dataset. Esto se traduciría si el día de la semana es el uno, o es el siete, o es el dos o tres. 

#### 2 | Festivo
Otra variable podría afectar, por ejemplo, los festivos, tanto el mismo día como la víspera. Por tanto, el factor festivo en el calendario nacional es una variable muy importante. 

#### 3 | Climatología
Tengamos en cuenta que este establecimiento hostelero está en España. Imaginemos un día de lluvia, a la gente le cuesta más salir de casa.

Sin embargo, cuando hace sol nos lanzamos a la calle, en consecuencia, la temperatura va a influir claramente también en nuestro objetivo, nuestro target, nuestra “Y”, que se denomina Machine Learning, que es la facturación. La lluvia que estaría también lo podemos englobar dentro de esta misma variable. 

#### 4 | Eventos
Pero tenemos que tener en cuenta que hay otro tipo de variables que pueden afectar, porque imaginaros que alrededor de este restaurante se realiza un evento, hay un fin de semana de conciertos. 

Esto atraerá a mucha más gente, por lo tanto, esto en Machine Learning lo veríamos como evento. Si hay evento sería un 1, y si no hay evento es un 0. 

#### 5 | Día del mes
Otra serie de variables puede ser el día del mes en el que nos encontremos. 

No gastamos lo mismo el día 5, cuando acabamos de ingresar la nómina, que el día 30, a finl de mes, pues los ánimos y el dinero disponible no son los mismos.

#### Open Data
**Pero también existen otras variables que pueden influir, los Open Data o Variables Externas.**
6. Tasa de desempleo juvenil
7. Aumento / Destenso del PIB

#### 6 | Tasa de desempleo juvenil
Imaginad que la tasa del paro juvenil se sitúa en un 40%. Esto va a afectar también esta variable externa, este Open Data va a afectar a la clientela del establecimiento, pues perderemos público de ese margen de edad.

#### 7 | Aumento / Descenso del PIB
Pensemos en un caso en que el PIB ha bajado muchísimo, la gente va a gastarse menos dinero en hostelería. 

En las variables y está la génesis para la creación de un buen algoritmo, es la clave. 

**Debemos comprobar qué variables son las que mejor explican este suceso o evento. **

Ya hemos conformado nuestro Dataset, y hemos: 
1. Añadido las variables.
2. Comprobado la importancia de estas variables.
3. Partido nuestro Entrenamiento / Test.
4. Desarrollado nuestro algoritmo.
5. Entrenar nuestro algoritmo.
6. Conseguido la precisón adecuada.

Si simplemente nos limitamos a un Timestamp y una columna con un valor de en euros, podríamos situarnos en precisiones alrededor del 60%.

> En el momento que añadimos variables, que apuntan a que pasó en esos días y utilizamos Open Data, podríamos subir a precisiones del 90%, incluso al 95%, lo que puede ser una ventaja competitiva muy potente. 

El campo del Machine Learning y la Inteligencia Artificial es un campo técnico, es cierto. Pero no todo es desarrollo y tecnicismos. 

Realmente, puede acceder gente desde el mundo del ámbito del negocio, que va a tener que aprender una serie de técnicas que son perfectamente alcanzables. Lo realmente importante es la combinación de la parte táctica estratégica con la parte técnica.


### 494.E1_Deep_Learning-video

### Deep Learning
![[494.E1_Deep_Learning.mp4]]
[Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez)

Para definir lo que es la inteligencia artificial, primero hay que describir un poco lo que es la inteligencia que nosotros conocemos, que nos asociamos a nosotros como seres humanos. A mí me gusta definir la inteligencia normal, la natural o la que nosotros poseemos, como una combinación de diferentes aptitudes, como puede ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. Y para, digamos, de alguna manera... para entender la inteligencia artificial, tenemos que saber que, a día de hoy, las técnicas que nosotros llamamos inteligencia artificial, son un subconjunto de tareas que son muy buenas en la tarea del aprendizaje, es decir, a partir del conocimiento que le aportan los datos, aprender a resolver tareas. Por tanto, la inteligencia artificial, desde un punto de vista algorítmico-tecnológico, no es una inteligencia general como la que tenemos nosotros, que es capaz de resolver todas estas tareas que he mencionado, o tiene todas estas características que he mencionado previamente, sino es una serie de algoritmos que consiguen aprender a resolver tareas a partir de datos de entrada, y generalmente una gran cantidad de estos datos. Y de hecho, estas técnicas que aprenden a partir de datos, se engloban en un campo que se llama Machine Learning. el cual es justo eso, diferentes tipos de algoritmos. de sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos de entrada. Ya pueden ser estos supervisados, es decir, con la respuesta asignada ya al problema que buscamos, o no supervisados, es decir, datos que no tienen a priori la respuesta al problema que buscamos, pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos. Y, concretamente, lo que ha permitido que la inteligencia artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas. del Machine Learning que se llaman Deep Learning. Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos. Y, por tanto, cuando, o en la mayoría de ocasiones, que estamos hablando de inteligencia artificial en las innovaciones que hemos tenido en los últimos años, estamos hablando, en realidad, de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas a partir de grandes bases de datos. que generalmente están supervisadas o etiquetadas en realidad, tanto los algoritmos de Machine Learning como los de Deep Learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos pero concretamente, los algoritmos de Deep Learning no es que sean diferentes, sino que son una subclase del Machine Learning simplemente, estas subclases están basadas en redes neuronales de las cuales hay diferentes tipos, que posteriormente veremos y este tipo de algoritmos basados en redes neuronales artificiales que al final simplemente son una... inspiración o una imitación algorítmica o una simplificación incluso algorítmica de las redes neuronales que funcionan en nuestro cerebro, aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática. Aunque ahora mismo estamos muy habituados ya a escuchar hablar de inteligencia artificial y parece un término relativamente nuevo, que empezó a popularizarse a partir del año 2012 más o menos de forma muy pronunciada, en realidad las primeras redes neuronales surgieron en la década de los 50-60 más o menos. En ese momento, no estaba la tecnología suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. Por dos motivos principales, estas técnicas requieren de alto poder de computación y además requieren de una gran cantidad de datos. Por tanto, estas dos factores, tanto la capacidad de computación como... La cantidad de datos ha crecido de forma exponencial desde los años 60, porque la digitalización se ha ido expandiendo en todos los dominios. Y, por tanto, en 2012 se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponibles era suficiente para que estas redes neuronales funcionasen. Y a partir de entonces es cuando hemos podido sacar su máximo potencial. Pero... desde su surgimiento hasta la actualidad, han habido diferentes intentos, o diferentes hitos, mejor dicho, en el campo de la Inteligencia Artificial, en el que han ido, progresivamente, superando a los seres humanos en diversas tareas. Por ejemplo, en el año 1996, Deep Blue de IBM derrotó a Garry Kasparov, al ajedrez, que era el campeón mundial en ese momento, y, por lo tanto, en esa tarea concreta, la Inteligencia Artificial ya nos lleva muchos años superando. Pero podemos ver también que han habido hitos muy recientes, como por ejemplo, en el año 2016, una inteligencia artificial, concretamente desarrollada por DeepMind, que es una compañía que fue adquirida por Google, derrotó al campeón mundial del Go, que es un juego muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, muy, mucho más complejo que el ajedrez en términos de árboles de posibilidades. Además, en el año 2019, el algoritmo de OpenAI derrota al mejor equipo del mundo en el videojuego Dota 2, que es un videojuego que requiere de coordinación de diferentes jugadores y de mucha intuición, por lo tanto, demostró otro hito. E incluso en el 2020, otro algoritmo también de DeepMind, la compañía que fue desarrollada por Google, resolvió un problema que hasta entonces no había podido resolverse que era el predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo, va a ser extremadamente prometedor para hacer nuevos avances en el campo. Y, en general, a partir de estos hitos y esta evolución, hemos visto cómo, progresivamente, las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, sino que han transicionado a ser compañías basadas en los datos y, por lo tanto, en la inteligencia artificial. Ahora las compañías más grandes del mundo son Google, Facebook, Apple, etc., compañías puramente basadas en datos, software e inteligencia artificial. y se prevé que esta evolución exponencial del valor que la inteligencia artificial va a aportar al mercado y a los consumidores, va a seguir creciendo. Y, por tanto, este campo es de extremada relevancia su estudio para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías. El Deep Learning es un campo que ha crecido mucho en los últimos años, se han desarrollado muchas nuevas tecnologías, y cada vez es un campo que, de hecho, es incluso vertiginoso el ritmo que está cogiendo, porque cada vez se dedican más recursos y, por tanto, más novedades aparecen de forma más rápida. Por tanto, es un poco difícil mantenerse en la ola de estar a la última en todo lo que va surgiendo. Pero los tipos principales de redes normales que podemos encontrar, o los que están más establecidos, y que ya han demostrado resultados muy significativos en los últimos años, son las redes neuronales totalmente conectadas, que serían las redes neuronales originales, las que se basan en neuronas individuales que se conectan a través de distintas capas. Después tendríamos las redes neuronales recurrentes, que se desarrollaron específicamente para la evaluación o para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros. Luego tendríamos las redes neuronales convolucionales, las cuales están específicamente diseñadas para trabajar con datos que estén ordenados en dos o tres dimensiones. También tendríamos una, digamos, subtipología de entrenar redes neuronales, que serían las redes neuronales generativas adversarias. Estas redes, principalmente, convolucionales, tienen la característica de que al final sirven para generar datos de forma artificial que son muy realistas y finalmente tenemos una red neuronal que ha surgido en los últimos años, un tipo de redes neuronales que surgió en 2020 que son los transformers, este es un tipo de red neuronal que a través de la ingesta de grandes cantidades de datos y a través de desarrollos de modelos con muchos parámetros son capaces de hacer tareas como generación de texto y de imágenes con un realismo que no tiene precedentes. Por tanto, este tipo de redes son capaces de trabajar tanto con entradas de texto, con entradas de imagen o con cualquier tipo de entrada y, por lo tanto, es capaz de combinar información de ambos mundos, del mundo del lenguaje natural, es decir, el texto, y el mundo de la imagen. Pero es muy bueno trabajando con datos no estructurados. Una neurona es un núcleo de cálculo muy simple. Simplemente es una función que tiene como entrada un número o varios números y le aplica una función de activación específica. que puede ser determinada por el programador para obtener un número diferente al de entrada. Esta neurona, como mencionado, tiene una o diversas entradas. Estas entradas tienen asociada un peso. Por lo tanto, si una neurona tiene diversas entradas, como por ejemplo, si estamos hablando en un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa, como número de habitaciones, metros cuadrados, número de baños, etc. Y esta información... entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada, y cuando combina todos estos números con estos pesos, les aplica una función de activación que convierte en este número en otro, o igual puede ser el mismo, depende de la función de activación que se utilice y el número de entrada. Esto al final lo que introduce en el sistema son no linealidades que lo que permiten es adaptarse a entornos de datos muy complejos. Sabiendo ya lo que es una neurona, lo que podemos ver es cómo se crea una red neuronal. Pues es muy simple, una red neuronal, lo que es un conjunto de neuronas ordenadas por capas, es decir, en la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etc. Así hasta la capa de salida, que es la que nos va a dar la respuesta a nuestro problema, que en este ejemplo que he puesto sería el precio de la casa que queremos predecir. Por lo tanto... lo que vemos es una estructura por capas, y al final lo que vamos a conseguir con esto es que... cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto es lo que nos permite transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para conseguir o obtener, desgranar, unas relaciones de muy alto nivel, muy complejas, que quizá no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa que estemos evaluando en cuestión. Las redes nuevas recurrentes, la principal diferencia es que están generadas o diseñadas para entender o procesar la información. datos que tengan una relación temporal, es decir, por ejemplo, en una frase, las palabras, el orden de las palabras, puede cambiar el significado de una frase, por tanto, las redes neuronales recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. Por lo tanto, estas redes neuronales lo que hacen es procesar secuencialmente las palabras de entrada, pero teniendo como información también, a la hora de analizar cada una de las palabras, cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma. Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate. mejoró de forma drástica en muy poco tiempo. ¿Por qué fue? Porque se pasó de utilizar técnicas más convencionales para la traducción, a utilizar técnicas basadas en redes neuronales. Esto lo que permitió es que los resultados de traducir un texto, del español al inglés, por ejemplo, ahora sean extremadamente robustos y prácticamente... apenas se cometen incoherencias textuales. Y cada vez, según se vayan incrementando los datos y los modelos vayan siendo más grandes, este error y esta, digamos, poco similaridad con lo que diría una persona, se van a ir reduciendo paulatinamente. También tenemos, por ejemplo, el reconocimiento de voz, que no es más que la conversión de audio a texto y posterior interpretación de este texto. Así como el texto escrito, el audio también es una secuencia temporal y por lo tanto las redes neuronas recurrentes pueden utilizarse para transcribir o para convertir este audio en una cadena de texto que sea posteriormente interpretada por otra red recurrente para emitir acciones específicas en los asistentes de voz. Por otra parte tenemos las redes normales convolucionales, las cuales están específicamente diseñadas para analizar datos que están organizados en dos o tres dimensiones, como es el caso de las imágenes. Al final una imagen lo que es es una matriz bidimensional de píxeles ordenados en filas y columnas. Por las redes normales convolucionales lo que nos permite es utilizar una operación que se llama convolución, la cual nos permite aplicar diferentes filtros a las imágenes para poder analizarlas. procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etc., a características de alto nivel. Si, por ejemplo, estamos detectando un sistema de... entrenando un sistema de detección de caras, las características de más alto nivel, por ejemplo, serían los ojos, la nariz, la boca, etc. Es decir, la red neuronal aprende, convolucional concretamente, aprende a ver cómo se combinan todas estas características de bajo nivel, es decir, bordes, contornos, etc., para convertirlas en características de alto nivel, boca, ojos, etc., para seguir procesándolas y subiendo el nivel hasta obtener representaciones de lo que sería una cara. Por lo tanto, al final, las redes neuronales convolucionales están trabajando en comprender la información que tienen estas matrices bidimensionales que son las imágenes. La convolución es una operación matemática que lo que nos permite es, dado un filtro, que no es más que una matriz... generalmente de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante, es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente procesada con una información específica resaltada. Las aplicaciones que podemos ver en redes neuronales convolucionales son variadísimas, es decir, son extremadamente extensas, pero, por ejemplo, una de las más conocidas es el reconocimiento facial. es decir, para empezar, gracias a una renombre convolucional, podemos detectar las caras que hay en una imagen, es decir, dónde están esas caras dentro de la imagen, y posteriormente, hacer una identificación de estas caras con respecto a una base de datos, como por ejemplo, el reconocimiento facial en China, está en la orden del día, y por tanto, todos sus ciudadanos están altamente identificados de forma automática gracias a estos sistemas, por lo tanto, evidentemente, las aplicaciones de la identidad artificial, como cualquier otra herramienta. pueden ser positivas o negativas, depende del análisis y de las consecuencias que tengan, pero desde luego son herramientas extremadamente potentes. De hecho, volviendo al caso de China, se descubrió a un fugitivo entre 60.000 personas en un evento, gracias a este reconocimiento facial. Por tanto, esto nos hace darnos cuenta del potencial que tiene esto, de la precisión a la que estos algoritmos están llegando, ya que en un contexto muy caótico, como es un evento de 60.000 personas, la inteligencia artificial, artificial es capaz, concretamente la basada en redes nuevas convolucionales para detección de rostros, identificar a un fugitivo. Adicionalmente, también es amplísimo el uso de las redes nuevas convolucionales para el análisis y proceso de imágenes médicas, ya que nos permiten, por ejemplo, hacer segmentaciones volumétricas de imágenes en 3D como los tags o las resonancias magnéticas, para obtener volúmenes de regiones de interés, como pueden ser pulmones, hígado, cerebro, etcétera, y además utilizar también estas redes neuronales para extraer información y detectar diferentes patologías, como el COVID, nódulos, y una gran otra variedad de enfermedades pulmonares, o incluso de cualquier otra parte del cuerpo, y estas aplicaciones nos están haciendo más que crecer y crecer, es decir, cada día, cada día, cada día, cada día, cada día, cada día, cada día, o cada año son más numerosas las diferentes empresas y startups que se dedican a este sector. Evidentemente, otra aplicación muy común de las redes neuronales convolucionarias son los coches autónomos. Al final, un coche autónomo es un coche que tiene que replicar la conducción de una persona y para ello es primordial la evaluación de la información del entorno. Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras. y por tanto las cámaras producen imágenes que son el target ideal para ser analizadas con redes neuronales convolucionales que detectan desde objetos en la vía, como pueden ser coches, peatones, ciclistas, etc. a también las líneas del carril, semáforos, entre otras muchas cosas. Por lo tanto también son clave para este sector. He mencionado algunas aplicaciones de las GAN, pero también podemos ver otras como por ejemplo, la alteración de imágenes médicas, esto sería una aplicación que no sería positiva, sino de todo lo contrario, sería negativa, pero para que veamos que las redes neuronales también se pueden utilizar con fines nocivos y por lo tanto hay que estar prevenidos a esta clase de situaciones. En un estudio lo que hicieron fue evaluar cómo alterando imágenes médicas mediante estas GAN, los científicos que realizaron el estudio eran capaces de engañar a los usuarios. radiólogos expertos en observar las imágenes médicas. Por lo tanto, lo que hicieron fue incluir en imágenes de sujetos sanos nódulos pulmonares y en imágenes de sujetos con cáncer el cáncer de la pérdida de la sangre. quitar de forma artificial estos nódulos. Y los resultados que se vieron es que, tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico, de hecho, en más de un 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales. También, otra de las aplicaciones no positivas para la sociedad que podríamos evaluar de las redes neuronales generativas, serían, por ejemplo, los deepfakes, en los que somos capaces de generar, de forma artificial, vídeo y audio de una persona concreta, para hacer creer al resto que esa persona ha dicho unas declaraciones concretas o ha hecho una acción específica, induciendo error a la población sobre lo que esa persona ha hecho y, por lo tanto, dañando su imagen. Y, de hecho, hay decenas de vídeos en... por Internet en las que podemos ver esta clase de situaciones. Las redes neuronales generativas adversarias son un tipo de red neuronal que lo que nos permite es generar datos artificiales. Para resolver esta tarea de forma óptima lo que entrenamos son dos redes neuronales una red neuronal que es la encargada de generar estos datos que son artificiales y luego tenemos otra red neuronal que es la encargada de inferir o de discriminar si estos datos son reales o son falsos por lo tanto, cuando hacemos diversas iteraciones de entrenamiento la red neuronal de generación de imágenes es muy buena creando imágenes que se parecen a la realidad porque intenta engañar a la red discriminadora que es la red que intenta detectar si estas imágenes son reales o son ficticias. Por tanto, entramos en un proceso, un círculo virtuoso, en el que cada vez la imagen generadora genera mejores imágenes o más parecidas a la realidad, y la red discriminadora cada vez es más buena detectando estas imágenes que son falsas. Por lo tanto, entrar en un juego de intentar superarse la una a la otra, consiguiendo que la red generadora, finalmente, que es nuestro objetivo, genere imágenes muy, muy, muy... similares a las imágenes reales. Las GANs al final se pueden utilizar para toda una diversidad de tareas que tengan como objetivo un objetivo. generar información de una distribución de datos específica que antes no existía, como por ejemplo generación de caras, de hecho hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe, es decir, y es extremadamente realista. Por lo tanto, las GAN... lo que nos permiten es generar datos de cualquier distribución con la que la hayamos entrenado y por tanto replicar toda esta información. Incluso también nos puede servir, hay muchísimas aplicaciones de las GAM, pero nos puede servir también para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. Por ejemplo, dada una imagen en blanco y negro, conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permite hacer cualquier tipo de técnica que tenga como salida que debería ser realista, es decir, la generación de datos que son realmente plausibles, como por ejemplo, ya os digo, la coloración de una imagen, la generación de una cara artificial, la generación, por ejemplo, de imágenes que sean realistas para tener más datos con el que entrenar otras redes neuronales, etc. Ahora tenemos los transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que empezó a ser un proceso de transformación. como una revolución en la inteligencia artificial, concretamente en el procesador del lenguaje natural, ya que lo que permitía era superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. Los transformers lo que pueden hacer es obtener, sin un coste computacional disparado, la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, cosas en las que las redes neuronales recurrentes no eran tan óptimas. Y, además, nos permite hacer esta clase de relaciones y de entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en términos de parámetros y, por lo tanto, finalmente, de desempeño final. Uno de los... Ejemplos más disruptivos de los últimos años de Transformers ha sido el GPT-3, que no es más que un generador de texto tan preciso y tan realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido, puede ser, por ejemplo, la comprensión de contenido. a través de un resumen del mismo, la traducción de ese contenido, el diseño incluso de aplicaciones que se basen en esta generación de texto para multitud de aplicaciones finales, la programación automática desde un punto de vista informático, es decir, se han creado herramientas que te permiten, o permiten a los desarrolladores, tener una generación automática del código que ellos quieren realizar, o al menos una sugerencia que después ellos pueden adaptar, haciendo mucho más eficiente el trabajo, toda clase de tareas de generación de contenido online que se os puedan imaginar, porque tenemos una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, podemos hacer que esta herramienta específica escriba sobre multitud de temas. Otra aplicación que ha tenido un gran impacto en los últimos años de Transformers es la generación de imágenes dada un texto descriptivo. Es decir, lo que podemos hacer es generar cualquier tipo de imagen dada una descripción específica. Esto lo que nos permite es, de alguna manera, acelerar o hacer más eficiente el proceso creativo. ya que, dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto que nos puede inspirar. Esto puede servir para, por ejemplo, inspirar a distintos grupos de diseño, incluso se han creado NFTs a partir de este arte hecho por redes neuronales, etc., que al final aportan un valor de mercado muy relevante. Finalmente, yo lo que pondría el foco es que la Inteligencia Artificial, y por lo tanto el Deep Learning, lo que se centra es en automatizar una serie de tareas específicas a partir de datos concretos. Y estamos muy lejos de esta Inteligencia Artificial general, que puede estar en el imaginario colectivo por las películas, que es capaz de tomar decisiones autónomas, de hacer una multitud de tareas. No, lo que tenemos son sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. Aunque la Inteligencia Artificial, hemos comentado que tiene ya muchos años, es decir, se empezó a desarrollar en la década de los 50-60, pero su uso decayó debido a que no teníamos ni los datos, ni el poder de computación, en esta era, o en esta época de desarrollo de Inteligencia Artificial, no se había dado cuenta de que había un problema. Y eso es lo que nos ha dado la oportunidad de ver este video, que es el de Inteligencia Artificial, que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver el desarrollo de Inteligencia Artificial, y que es el que nos ha dado la oportunidad de ver ya se están desarrollando herramientas tan potentes que están proporcionando un montón de valor de mercado y, por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, sino que, todo lo contrario, una tecnología que está aportando valor de mercado, que cada vez aporta más valor de mercado y, por tanto, es una tecnología que está impactando e impactará nuestras vidas de forma muy pronunciada. Sin embargo... teniendo en cuenta todo lo mencionado y el valor de mercado que ya está produciendo, hay que tener en cuenta que solo estamos al principio, es decir, este último boom, esta última serie de desarrollos en la integración artificial, se han empezado a producir mayoritariamente a partir del 2012, por tanto, solo llevamos una década viendo el potencial que estas técnicas pueden tener, pero el cambio, la adopción y la evolución de estas técnicas está creciendo de forma más exponencial y, por lo tanto, cada vez veremos... aplicaciones que nos sorprenderán más. Incluso ya somos capaces de ver todas estas aplicaciones que son incluso capaces de generar información tan real que son capaces de engañar al ojo humano. Y esto solo es el principio, por tanto, esto es un campo apasionante que, sin duda, seguiremos con detenimiento.


### 495.E1.U6.1.1_Deep_Learning

### Inteligencia Artificial
Para definir lo que es la IA, primero habría que describir lo que es la inteligencia que asociamos a nosotros mismos como seres humanos.

La inteligencia normal es una combinación de diferentes aptitudes, como pueden ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. 

> A día de hoy, las técnicas que nosotros llamamos Inteligencia Artificial son un subconjunto de tareas que son muy buenas en la tarea del aprendizaje, es decir, a partir de el conocimiento que le aportan los datos: Aprender a resolver tareas.

La Inteligencia Artificial, desde un punto de vista algorítmico o tecnológico, no es como la nuestra, capaz de resolver todas estas tareas que hemos mencionado previamente, sino es una serie de algoritmos que consiguen aprender a resolver tareas a partir de una gran cantidad de datos..

Estas técnicas que aprenden a partir de datos, se engloban en un campo que se llama Machine Learning, el cual es justo eso, diferentes tipos de algoritmos de sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos.

Estos datos pueden ser:
1. **Supervisados:** Con la respuesta asignada ya al problema que buscamos. 
2. **No supervisados:** Datos que no tienen, a priori, la respuesta al problema que buscamos.

Pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos y concretamente **lo que ha permitido que la Inteligencia Artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas del Machine Learning que se llaman Deep Learning.** 

Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos y, por tanto, cuando o en la mayoría de ocasiones que estamos hablando de Inteligencia Artificial en las innovaciones que hemos tenido en los últimos años, estamos hablando en realidad de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas y a partir de grandes bases de datos que generalmente están supervisadas o etiquetadas.
  
![[495.E1_Deep_Learning.png]]

#### Diferencias etre Machine Learning y Deep Learning
**La diferencia primordial es el tipo de algoritmos que utilizan.**

Tanto los algoritmos de Machine Learning como los de learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos. 

> Los algoritmos de Deep Learning son una subclase del Machine Learning.

Estas subclases están basadas en redes neuronales, de las cuales hay diferentes tipos que posteriormente veremos y este tipo de algoritmos basados en redes neuronales artificiales que simplemente son una inspiración o una imitación algorítmica, o una simplificación incluso algorítmica de las redes neuronales que funcionan en nuestro cerebro. Aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática.

#### Primeros algoritmos basados en Internet
Ahora mismo estamos muy habituados a escuchar hablar de Inteligencia Artificial y parece un término relativamente nuevo que empezó a popularizarse a partir del año 2012, más o menos de forma muy pronunciada. 

En realidad, las primeras redes neuronales surgieron en la década de los 50, 60 más o menos. En ese momento no estaba la tecnología suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. 

Por dos motivos principales: 
1. Poder de computación
2. Gran cantidad de datos

Estos dos factores han sido desarrollados de forma exponencial desde los años 60, ya que, la digitalización se ha ido expandiendo en todos los dominios. En 2012, se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponible era suficiente como para que estas redes neuronales empezasen a funcionar. 

A partir de entonces, es cuando hemos podido sacar su máximo potencial. Pero, desde su surgimiento hasta la actualidad, han sucedido diferentes intentos o diferentes hitos en el campo de la Inteligencia Artificial, en el que han ido progresivamente superando a los seres humanos en diversas tareas:

- **En el año 1996, el algoritmo Deep Blue de IBM derrotó a Gary Kasparov,** que era el campeón mundial de ajedrez en ese momento y en esa tarea concreta la Inteligencia Artificial. Ya nos lleva muchos años superando. 
- **En el año 2016, una Inteligencia Artificial concretamente desarrolla Deep Mind,** una compañía que fue adquirida por Google, derrotó al campeón mundial del GO, que es un juego mucho más complejo que el ajedrez en términos de árboles de posibilidades. 
- **En el año 2019 el algoritmo de Open EU derrota al mejor equipo del mundo en el videojuego DOTA 2,** que es un videojuego que requiere de coordinación de diferentes jugadores y de mucha intuición, consiguiendo otro hito. 
- **En el 2020, otro algoritmo también de DEEP Mind, la compañía que fue desarrollada por Google, resolvió un problema que hasta entonces no había podido resolverse,** que era el predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo va a ser extremadamente prometedor para para hacer nuevos avances en el campo y en general.

A partir de estos hitos y esta evolución, hemos visto como progresivamente las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, sino que han transicionado a ser compañías basadas en los datos y, por lo tanto, en la Inteligencia Artificial. Las compañías más grandes del mundo son Google, Facebook, Apple, etcétera compañías puramente basadas en datos, software e Inteligencia Artificial. 

Se prevé que esta evolución exponencial del valor que la Inteligencia Artificial va a aportar al mercado y a los consumidores va a seguir creciendo y, por tanto, este campo es de extremada relevancia. Su estudio para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías.

#### Tipos de Redes Neuronales
El Deep Learning es un campo que también ha crecido mucho en los últimos años. Se han desarrollado muchas nuevas tecnologías y avanza a un ritmo vertiginoso porque, cada vez, se dedican más recursos y aparecen más novedades. 

Resulta hasta difícil mantenerse en la ola de las últimas novedades que van surgiendo. 

Pero los tipos principales de redes neuronales que podemos encontrar, o los que más establecidos y que ya han demostrado resultados muy significativos en los últimos años son: 

- **Redes neuronales totalmente conectadas:** Serían las redes neuronales originales las que se basan en neuronas individuales que se conectan a través de distintas capas. 
- **Redes neuronales recurrentes:** Se desarrollaron específicamente para la evaluación o para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros. 
- **Redes neuronales convencionales:** Específicamente diseñadas para trabajar con datos que estén ordenados en dos o tres dimensiones.
- **Redes neuronales generativas o adversarias:** Estas redes, principalmente convolución, tienen la característica de que al final sirven para generar datos de forma artificial que son muy realistas. 
- **Transformers:** Red neuronal que surgió en 2020. Este es un tipo de red neuronal que a través de la ingesta de grandes cantidades de datos y a través de desarrollos de modelos con muchos parámetros, son capaces de hacer tareas como generación de texto y de imágenes con un realismo que no tiene precedentes. Por tanto, este tipo de redes son capaces de trabajar tanto con entradas de texto, con entradas de imagen o con cualquier tipo de entrada y es capaz de combinar. Es capaz de combinar información de ambos mundos, del mundo del lenguaje natural, es decir, el texto y el mundo de la imagen. Pero es muy bueno trabajando con datos no estructurados.


### 496.E1.U6.1.2_Redes_Neuronales

### Redes Neuronales, cómo se interconectan
**Una neurona es un núcleo de cálculo muy simple.**

Simplemente es una función que tiene como entrada un número o varios números y le aplica una función de activación específica que puede ser determinada por el programador para obtener un número diferente al de entrada.

![[496.E1_Redes_Neuronales_1.png]]

Esta neurona tiene una o diversas entradas y dichas entradas tienen asociado un peso. 

Por lo tanto, si una neurona tiene diversas entradas, por ejemplo, un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa:
1. Número de habitaciones
2. Metros cuadrados
3. Número de baños
4. Etc…

- Esta información entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada.
- Cuando combina todos estos números al final con estos pesos, les aplica una función de activación que convierte en este número en otro, o igual puede ser el mismo, depende de la función de activación que se utilice y el número de entrada. 
- Esto al final, lo que introduce en el sistema son no linealidades, que lo que permiten es adaptarse a entornos de datos muy complejos.

**Sabiendo ya lo que es una neurona, lo que podemos ver es cómo se crea una red neuronal.**

Una red neuronal es un conjunto de neuronas ordenadas por capas. Es decir, en la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etc. Así hasta la capa de salida, que es la que nos va a dar la respuesta a nuestro problema, que en este ejemplo que he puesto sería el precio de la casa que queremos predecir. 

Por lo tanto, lo que vemos es una estructura por capas y al final lo que vamos a conseguir con esto es que cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto es lo que nos permite es transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para conseguir u obtener, desgranar unas relaciones de muy alto nivel, muy complejas y que quizá no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa o la teoría que estemos evaluando en cuestión.

#### Redes Neuronales Recurrentes vs Redes Neuronales Convolucionales
##### Redes Neuronales Recurrentes
**Están generadas o diseñadas para entender o procesar datos que tengan una relación temporal. Por ejemplo, en una frase, el orden de las palabras, puede cambiar el significado de una frase.**

![[496.E1_Redes_Neuronales_2.png]]

Las Redes Neuronales Recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. 

Por lo tanto, lo que hacen es procesar secuencialmente las palabras de entrada, pero teniendo como información también a la hora de analizar cada una de las palabras cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma.

> Un buen ejemplo de caso de uso de la Red Neuronal Recurrente puede ser el reconocimiento de voz, que no es más que la conversión de audio a texto y posterior interpretación de este texto, así como el texto escrito.

El audio también es una secuencia temporal y, por lo tanto, las Redes Neuronales Recurrentes pueden utilizarse para transcribir o para convertir este audio en una cadena de texto que sea posteriormente interpretada por otra recurrente para emitir acciones específicas en los asistentes de voz. 

#### Redes Neuronales Convolucionales
**Están específicamente diseñadas para analizar datos que están organizados en dos o tres dimensiones, como es el caso de las imágenes.**

![[496.E1_Redes_Neuronales_3.png]]

Una imagen es una matriz bidimensional de píxeles ordenados en filas y columnas.

- Las Redes Neuronales Convolucionales nos permiten utilizar una operación que se llama Convolución, la cual nos permite aplicar diferentes filtros a las imágenes para procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etcétera, a características de alto nivel.
- Por ejemplo, estamos detectando un sistema de, entrenando un sistema de detección de caras, las características de más alto nivel, por ejemplo, serían los ojos, la nariz, la boca, etcétera. 
- La retina aprende, convolucional concretamente, aprende a ver cómo se combinan todas estas características de bajo nivel, es decir, bordes, contornos, etcétera, para convertirlas en características de alto nivel, boca a ojos, etcétera, para seguir procesándolas y subiendo el nivel hasta obtener representaciones de lo que sería una cara. 
- Por lo tanto, al final las resonancias convolucionales están trabajando en comprender la información que tienen estas matrices bidimensionales, que son las imágenes.

##### Definición de Convolución
**La Convolución es una aplicación matemática que lo que nos permite es, con un filtro que no es más que una matriz de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante.**

Es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente procesada con, digamos, una información específica resaltada.

![[496.E1_Redes_Neuronales_4.png]]

####  Redes Neuronales Generativas Adversarias (GAN)
**Es un tipo de red neuronal que nos permite generar datos artificiales.** 

Para resolver esta tarea de forma óptima, lo que entrenamos son dos redes neuronales.
- Una red neuronal es la encargada de generar estos datos que son artificiales. 
- Otra red neuronal que es la encargada de inferir o de discriminar si estos datos son reales o son falsos. 

Por lo tanto, cuando hacemos diversas iteraciones de entrenamiento, la red neuronal de generación de imágenes es muy buena creando imágenes que se parecen a la realidad porque intenta engañar a la red discriminadora, que es la red que intenta detectar si estas imágenes son reales o son ficticias. 

Así, entramos en un círculo virtuoso en la que cada vez la imagen generadora genera mejores imágenes o más parecidas a la realidad y la red discriminadora cada vez es más buena detectando estas imágenes que son falsas. 

Por lo tanto, entrar en un juego de intentar superarse la una o la otra, consiguiendo que la red generadora finalmente, que es nuestro objetivo, genere imágenes muy, muy, muy similares a las imágenes reales.

##### Aplicaciones de las GAN
- Las GANs, se pueden utilizar para toda una diversidad de tareas que tengan como objetivo generar información de una distribución de datos específica que antes no existía, como por ejemplo generación de caras. 
-   De hecho, hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe y es extremadamente realista. 
-   Por lo tanto, las GAN, lo que nos permiten es generar datos de cualquier distribución con la que la hayamos entrenado y, por tanto, replicar toda esta información. Incluso también nos puede servir, hay muchísimas aplicaciones de las GAN, pero nos puede servir también para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. 
-   Por ejemplo, dada una imagen en blanco y negro, conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permite hacer cualquier tipo de técnica que tenga como salida un dato que debería ser realista. 
-   La generación de datos que son realmente plausibles, como por ejemplo, ya os digo, la coloración de una imagen, la generación de una cara artificial, la generación, por ejemplo, de imágenes que sean realistas para tener más datos con el que entrenar otras redes neuronales, etcétera, etcétera.

#### Transformers
Ahora tenemos los Transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que empezó como una revolución en la inteligencia artificial, concretamente en el procesador del lenguaje natural, porque permitía superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. 

![[496.E1_Redes_Neuronales_5.png]]

- Los Transformers lo que pueden hacer es obtener sin un coste computacional disparado la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, cosas en las que las redes neuronales recurrentes no eran tan óptimas. 
- Además, nos permite hacer esta clase de relaciones y de entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en término de parámetros y, finalmente, de desempeño final. 

Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate mejoró de forma drástica en muy poco tiempo.

Se dio este hecho porque se pasó de utilizar técnicas más convencionales para la traducción a utilizar técnicas basadas en redes neuronales. 

Esto lo que permitió es que los resultados de traducir un texto del español al inglés, por ejemplo, ahora sean extremadamente robustos y prácticamente apenas se cometen incoherencias textuales y cada vez, según se vayan incrementando los datos y los modelos vayan siendo más grandes, este error y esta, digamos, poco similaridad con lo que diría una persona, se van a ir reduciendo paulatinamente.


### 497.E1.U6.1.3_Redes_Neuronales-Aplicaciones

### Aplicaciones de las Redes Neuronales
**Las aplicaciones que podemos ver son muy variadas y extremadamente extensas. **

Una de las más conocidas es el reconocimiento facial. 

Es decir, para empezar, gracias a una Red Neuronal Convolucional podemos detectar las caras que hay una imagen. Es decir, dónde están esas caras dentro de la imagen, para, posteriormente, hacer una identificación de estas caras con respecto a una base de datos. 

El reconocimiento facial en China, por ejemplo, está a la orden del día y por tanto todos sus ciudadanos están altamente identificados de forma automática en estos sistemas.

> Las aplicaciones de inteligencia artificial, como cualquier otra herramienta, pueden ser positivas o negativas, dependiendo del análisis y de sus consecuencias. Pero es innegable que son herramientas extremadamente potentes.

Volviendo al caso de China, se descubrió a un fugitivo entre 60.000 personas en un evento gracias a este reconocimiento facial. Por tanto, esto nos hace darnos cuenta del potencial que tiene, de la precisión a los que estos algoritmos están llegando.

#### Uso en medicina

![[497.E1_Redes_Neuronales_-_Aplicaciones_1.png]]

También es amplísimo el uso de las Redes Neuronales Convolucionales para el análisis y proceso de imágenes médicas, ya que nos permiten hacer segmentaciones volumétricas de imágenes en 3D, como los TAC o las resonancias magnéticas para obtener volúmenes de regiones de interés: Pulmones, hígado, cerebro, etc. 

Además, también se pueden utilizar estas redes neuronales para extraer información y detectar diferentes patologías como nódulos y una gran variedad de enfermedades pulmonares o incluso de cualquier otra parte del cuerpo.

Y estas aplicaciones no están haciendo más que crecer y crecer. Cada día o cada año son más numerosas las diferentes empresas de startups que se dedican a este sector.

#### Coches autónomos

![[497.E1_Redes_Neuronales_-_Aplicaciones_2.png]]

Otra aplicación muy común de las Redes Neuronales Convolucionales son los coches autónomos. 

Un coche autónomo es un coche que tiene que replicar la conducción de una persona y para ello es primordial la evaluación de la información del entorno. 

Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras y por tanto las cámaras producen imágenes que son el target ideal para ser analizadas con redes neuronales convencionales que detectan desde objetos en la vía, como coches, peatones, ciclistas, etc. 

También las líneas del carril entre semáforos, entre otras muchas cosas, por lo tanto, son clave para este sector.

#### Usos nocivos o fraudulenotos
Las GAN también tienen otro tipo de aplicaciones, por ejemplo, la alteración de imágenes médicas. Esto sería una aplicación, digamos que no sería positiva, sino todo lo contrario, sería negativa.

- Las redes neuronales también se puede utilizar con fines, nocivos y hay que estar prevenidos de esta clase de situaciones. 
- En un estudio, lo que hicieron fue evaluar cómo, alterando imágenes médicas mediante estas GAN, los científicos que realizaron estudios eran capaces de engañar a los radiólogos expertos en observar las imágenes médicas.
- Por lo tanto, lo que hicieron fue incluir en imágenes de sujetos sanos, nódulos pulmonares y en imágenes de sujetos con cáncer.
- Quitar de forma artificial estos nódulos y los resultados que se vieron es que tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico. 
- De hecho, en más de un 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales.

#### Deep fakes
Los Deep Fakes en los que somos capaces de generar de forma artificial video y audio de una persona concreta para hacer creer al resto que esa persona ha dicho unas declaraciones concretas, un hecho, una acción específica, induciendo a la población sobre lo que esa persona ha hecho y por lo tanto, dañando su imagen. 

Hay decenas de vídeos en internet en las que en las que podemos ver esta clase de situaciones.

#### Generación de Texto con ChatGPT
Ejemplos más disruptivos de los últimos años de Transformers ha sido el GPT3, que no es más que un generador de texto tan preciso y tan realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido.

Permite entre otras cosas:
1. Generación de texto o contenido.
2. Comprensión de un contenido a través de un resumen del mismo.
3. Traducción de dicho contenido.
4. Diseño de aplicaciones que se basen en esta generación de texto.
5. Programación automática.
6. Generación automática de código para desarrolladores.

Tenemos es una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, puedes hacer que esta herramienta específica escriba sobre multitud de temas.

#### Generación de Imágenes
Otra aplicación de gran impacto de los Transformers es la generación de imágenes a partir de un texto descriptivo.

![[497.E1_Redes_Neuronales_-_Aplicaciones_3.png]]

- Podemos hacer es generar cualquier tipo de imagen, dada una descripción específica. 
- Permite acelerar o hacer más eficiente el proceso creativo, ya que dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto que nos puede inspirar. 
- Esto puede servir para, por ejemplo, inspirar a distintos grupos de diseño. 
- Incluso se ha creado NFT’s de arte utilizando las redes neuronales.

#### Conclusiones
Estamos todavía muy lejos de esta inteligencia artificial general que puede estar en el imaginario colectivo por las películas, que es capaz no de tomar decisiones autónomas, de hacer una multitud de tareas.
- Tenemos sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. 
- Aunque la inteligencia artificial se empezó a desarrollar en la década de los 50 60, su uso solo decayó debido a que no teníamos todavía ni los datos ni el poder de computación, ya no es así. 
- En esta era de desarrollo artificial ya se están desarrollando herramientas tan potentes que están proporcionando mucho de valor de mercado y, por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, al contrario, ha llegado para quedarse e impactará en nuestras vidas de forma muy pronunciada.
- Hay que tener en cuenta que solo estamos al principio. Este último boom de desarrollo de la Inteligencia Artificial. se ha empezado a producir, mayoritariamente a partir del 2012. Por tanto, solo llevamos una década viendo su potencial.
- El cambio o la adopción y la evolución de estas técnicas está creciendo de forma más exponencial y por lo tanto, cada vez veremos aplicaciones que nos sorprenderán más. 
- Incluso ya somos capaces de ver todas estas aplicaciones que son incluso capaces de generar información tan real que son capaces de engañar al ojo humano.

Es un campo apasionante que hay que seguir con detenimiento.


### 498.E1_Blockchain_y_Machine_Learning-video

### Blockchain y Machine Learning
![[498.E1_Blockchain_y_Machine_Learning.mp4]]
[Blockchain y Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948168-p6-1-diego-bonilla-blockchain-y-machine-learning)
[Blockchain y Machine Learning (PDF)](499.E1.Blockchain_y_machine_learning-230120-152730.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98)

Hola y bienvenidos a todos a una nueva sesión de Web3. En esta sesión veremos Machine Learning aplicado a Blockchain. Como ya sabéis, tanto Machine Learning como Blockchain son dos tecnologías que ahora mismo son bastante punteras, tienen muy poco tiempo en algunos casos, y digamos que ambas son tecnologías muy potentes y pues hoy explicaremos los... los avances que ha habido en mezclar esas dos tecnologías tan potentes y tan prometedoras y también veremos casos prácticos y aplicaciones en las que se está utilizando o ramas que lo están utilizando o lo están empezando a utilizar y también el futuro que un poco nos... vamos a ver con el uso de ambas tecnologías. Para ello, vamos a empezar explicando un poco el Machine Learning y el Blockchain. En ambas diapositivas vamos a verlo un poco por encima, no vamos a entrar en profundidad porque muchos de vosotros ya sabréis, ya tenéis más que un buen contexto sobre ambas tecnologías. Pero bueno, simplemente para poner en contexto el resto de la explicación y para que estemos todos en la misma página y también explicarlo desde un punto de vista también para que sea más fácil aplicarlo, tanto de relacionar las dos en una nueva tecnología y para luego poder ver un poco la teoría que hay detrás de unir ambas y casos prácticos de aplicaciones. Pues como hemos comentado, vamos a empezar con Machine Learning. El Machine Learning, como ya sabréis, es una tecnología que tiene una capacidad de aprender de datos nuevos de forma automática, es decir, es un conjunto de algoritmos que pueden adaptarse a cualquier tipo de datos o a casi cualquier tipo de datos. de forma intrínseca, que no necesita muchas modificaciones en el algoritmo para tener que adaptarse a varias tareas. Y es esta potencia o esta capacidad lo que hace que sea muy aplicado en muchísima cantidad de campos y en muchísima cantidad de dominios con diferentes datos. y muchas tareas. De esas tareas no hace falta tampoco mencionarlas porque son muchísimas, tanto en audio, en texto, datos estructurados, datos no estructurados, datos en el dominio continuo, dominio discreto, imágenes, vídeos, de todo. Todo esto hace que el Machine Learning sea una tecnología que sea una de las con más tendencia ahora mismo gracias a estas increíbles capacidades. Blockchain por otro lado también es el corazón de mucha de la tecnología, bueno de todas las criptomonedas. Blockchain por otra parte también se está volviendo cada vez más popular y esto ayuda a... hacer que el almacenamiento de información, el paso de información, sea mucho más descentralizado, mucho más seguro y que no haya ningún intermediario. Machine Learning, por lo tanto, puede ser aplicado a la tecnología blockchain para hacerlo más eficiente y mucho mejor. En este gráfico vemos un poco cómo se está utilizando o en qué área se puede utilizar este blockchain basado en frameworks de deep learning o machine learning en general. Podemos ver que tanto el tipo de blockchain, los diferentes tipos que hay, los diferentes modelos y luego ya protocolos. basados en Machine Learning o en Deep Learning y diferentes áreas de aplicación, servicios, tipos de datos con los que trabajarían y diferentes objetivos de despliegue. Vamos a ver ahora también rápidamente la tecnología Blockchain, que significa cómo se usa. para que, pues eso, más que nada para estar en contexto. Ya sé que muchos de vosotros y vosotras lo entendéis mejor que yo seguramente, pero simplemente para el resto. Básicamente blockchain se puede entender simplemente como un tipo de base de datos distribuida que, digamos, almacena cualquier tipo de datos y hace que sea, la forma en la que lo hace, hace que sea muy difícil de... hackear o cambiar o engañar al sistema. El resto de bases de datos, las más convencionales digamos, almacenan la información en tablas, mientras que la blockchain lo que hace es almacenarlo en bloques que están encadenados entre sí, por eso se llama blockchain. Esto sería una arquitectura... digamos muy normal de blockchain. Podemos ver las partes que contiene el blockchain, que son, para empezar, los bloques, que son las entidades de alto nivel que vemos aquí. Cada bloque contiene, bueno podéis ver una cantidad arbitraria de bloques. Cada bloque contiene tres tipos principales de elementos, que son los datos, Contiene también el nonce y también contiene el hash. Estos son los datos que se contienen dentro de un bloque de blockchain, pero también encontramos mineros y nodos. Los mineros o miners son utilizados para crear nuevos bloques utilizando el sistema de minado. Y los nodos. pueden ser entendidos como un dispositivo que contiene una copia del blockchain. Por ejemplo, para una transacción completa tiene que haber diferentes nodos y cada nodo tiene que tener una copia del blockchain. Aquí vemos lo que hemos explicado antes sobre los nodos. Esto sería una transacción completa. Vamos a ver ahora la combinación de ambas tecnologías, ya que hemos hecho un overview un poco simple por encima de ambas. Sabiendo que el Machine Learning, como hemos explicado antes, podemos entenderlo como un sistema que aprende de datos pasados e intenta aprender de esos datos para mejorar los datos futuros, lo cual se puede entender como predecir un poco el futuro, es tomada como una tecnología que se adapta a autoadaptativa y que, además de eso, todas estas reglas que necesita para autoadaptarse no tienen que ser escritas a mano. Eso significa que las aprende por sí solas. La combinación de estas dos tecnologías que hemos hablado, de blockchain y machine learning, Es un... digamos que cambiaría el juego bastante por ejemplo, finanza o empresas de seguros. Para, por ejemplo, un ejemplo muy normal sería identificar las transacciones fraudulentas. Aquí lo que podemos ver es un poco en esta tabla las diferentes... cuatro diferentes ejemplos o cuatro diferentes beneficios que se puede entender de unir estas dos tecnologías. El primero sería aumentar la seguridad del dato utilizando blockchain que como ya hemos explicado y como ya sabréis aumenta la seguridad con respecto al resto de bases de datos utilizadas más. más convencionales, hace que sea automático gracias a la potencia de deep learning y de machine learning. Por lo tanto, como hemos hablado, se vuelve un proceso automático que aprende de sí mismo. También utiliza esta capacidad que tiene de predecir el futuro y por lo tanto, una vez que se tenga esta más seguro y mejor capacidad de decisión, pues la robustez del sistema también se ve aumentada. Algunas de otras capacidades o mejoras que puede entender esta mezcla de ambas tecnologías puede ser, por ejemplo, mejorar la seguridad. Como hemos explicado, los datos en blockchain son mucho más seguros porque tiene implícito un sistema encriptado en el sistema y es un sistema perfecto que, digamos, es el sistema perfecto para almacenar datos personales de mucha sensibilidad, como por ejemplo, una recomendaciones personalizadas. Pero por mucho que esté seguro, la tecnología blockchain sea muy segura, muchas otras aplicaciones que utilizan esta tecnología o capas que se ponen por encima. pueden hacerlo vulnerable, entonces por lo tanto Machine Learning puede ser aplicado, la capacidad que tiene de predecir puede ser aplicado para predecir posibles brechas de seguridad o amenazas que pueda haber en las aplicaciones que vayan a utilizar esta tecnología. Luego también se puede optimizar el mercado del dato. Muchas compañías muy grandes como por ejemplo puede ser Google o Meta o LinkedIn tienen muchísimas data pools que son enormes y contienen la información de millones de usuarios. Entonces por lo tanto, esto también sería utilizar los datos que han almacenado y estos datos tener acceso a una inteligencia artificial que pueda sacarle su uso mediante procesos de inteligencia artificial sería extremadamente útil y también sería muy útil que estos datos también fueran accesibles por otros, que no fueran estas grandes empresas. Entonces, utilizando la tecnología blockchain, muchas startups y compañías más pequeñas podrían acceder a ese data pool de estas grandes empresas. utilizando los mismos procesos de inteligencia artificial que utilizarían estas empresas sin que hubiera ninguna brecha en la seguridad. Por otra parte, otro beneficio se podría también entender como un optimizar el consumo de energía. Ya que el minado del que hemos comentado un poco por encima antes, que es una parte principal del blockchain, consume muchísima energía, como ya sabéis, y es un gran problema. Es uno de los mayores, de hecho, para diferentes industrias es un problema muy grande. Pero gracias a algoritmos de inteligencia artificial, Machine Learning, Deep Learning, Google consiguió reducir en un 40% el consumo de energía del minado usando una red neuronal. En la compañía de Google que se llama DeepMind AI, optimizaron el consumo de energía usado por los robots por los data centers, sobre todo en la parte de refrigeración de estos data centers, y digamos que la optimización se hizo utilizando redes neuronales y consiguieron optimizar el consumo de energía en un 40%. Y por último, implementar un pago de seguro a tiempo real. También es un buen beneficio. ya que utilizando blockchain y machine learning podemos crear un proceso de pago mucho más seguro, inteligente y que puede ser implementado completamente en el entorno de blockchain. Utilizando por ejemplo el encriptado inteligente de machine learning que es muchísimo más inteligente y muchísimo más adaptativo que cualquier otro encriptado que puede haber en el resto que se haya usado hasta ahora. por ejemplo, en la imagen de aquí abajo, describe un modelo, un sistema, o sea, el modelo de un sistema que coge una entrada, que en este caso serían, por ejemplo, unos datos de personales de un hospital que podríamos comentarlos como algo muy confidencial, se guardarían en los datos del Hyperledger Fabric para utilizar el protocolo de IPFS. que sería el interplanetary file system utilizando un algoritmo. Y aquí los datos podrían, digamos que una vez que están guardados de forma tan segura, se pueden desencriptar o se pueden recuperar utilizando un agente de deep learning. En este caso una LSTM, que es una red neuronal optimizada para datos temporales o con cierta dependencia temporal, que sería la sigla son long... Short Term Memory, está basado también en redes neuronales, al final es una capa más de una red neuronal y luego al final, una vez que están recuperadas esos datos utilizando esta red neuronal, pues se puede, por ejemplo, enviar a un dispositivo como una alerta de diferentes actividades, como por ejemplo una consulta del doctor, que se tiene que tomar cierta medicación, un diagnóstico hacia un paciente o cualquier cosa. Más información www.alimmenta.com Vamos a ver ahora aplicaciones un poco más... Hemos visto los beneficios que puede llegar a tener la combinación de ambas tecnologías. Y ahora vamos a ver aplicaciones que reales y un poco más en concreto que se llevan a cabo gracias a... O se pueden llevar a cabo gracias a la combinación de estas tecnologías y los beneficios que hemos comentado. La primera que podemos entender como una muy buena aplicación sería mejorar el servicio al cliente. Como ya sabéis, la satisfacción de los clientes es un gran desafío para todas las organizaciones, pero ya se están utilizando actualmente muchas tecnologías de Machine Learning para mejorar el servicio al cliente. Se puede, por ejemplo, de todo, tanto sistemas de recomendación como una experiencia mejorada del cliente en la tienda. Pero aparte de lo que también se está haciendo, también se puede combinar con una aplicación basada en blockchain para mejorarlo, para llevarlo a otro nivel. La patria que ya actualmente se está haciendo. También. Viendo desde un punto de vista de seguridad, como ya hemos visto antes, ambas tecnologías pueden incrementar la seguridad y llevarla a otro nivel. Por lo tanto, sabiendo que la seguridad es algo muy importante para las personas y que además cada vez la gente la está teniendo más en cuenta, y que también la seguridad es algo que se puede mejorar. hay cada vez más cibercrimen en el escenario presente, machine learning y blockchain. Ambas tecnologías pueden ser usadas para... digamos, para vigilar o para vigilancia de estas aplicaciones donde blockchain, por ejemplo, puede ser utilizado para controlar estos datos continuos y Machine Learning puede ser utilizado para estos datos que se están controlando por el blockchain ser analizado. También de forma encriptada, de forma anónima, Machine Learning, como ya hemos visto, necesita supervisión humana, entonces por lo tanto... también incrementa la seguridad al no haber ninguna persona por medio. Las Smart Cities están desarrollándose cada día. Cada día se intenta ayudar a las personas a mejorar o a mejorar sus estándares de vida. Cada vez la vida de las personas de nosotros se va haciendo más fácil utilizando la tecnología. y la idea de crear una ciudad inteligente sería una ciudad en la que involucrara machine learning y tecnologías basadas en blockchain para crear un rol crucial. Por ejemplo, una casa inteligente, una smart home, que utilizará estas tecnologías para monitorizar fácilmente para la monitorización fácilmente de cada usuario y por ejemplo dar una experiencia personalizada a cada usuario en cada dispositivo. El trading, obviamente ya que el blockchain es la tecnología que está detrás de la mayoría de las criptomonedas, sobre todo las más grandes, como por ejemplo pueden ser Bitcoin o Ethereum. Estas criptomonedas cada vez son más populares entre los inversores de retail o las instituciones financiales grandes. Y hoy en día se utilizan ya bots de trading que están digamos basados en algoritmos de machine learning muy potentes. y digamos que se puede utilizar unas estrategias de Machine Learning más avanzadas. Una de ellas se llama Reinforcement Learning o aprendizaje por refuerzo, que es un método que se utiliza en el mundo de la tecnología. siempre ha sido muy popular entre sistemas que por ejemplo aprenden a jugar a videojuegos o juegos de mesa pero también últimamente se han usado para por ejemplo un folding protein de deshacer proteínas para saber cuáles son las proteínas que forman parte del cuerpo humano. También se ha conseguido muchos hitos muy recientes con esto, de hecho se ha nombrado uno en los inventos del 2022. Entonces, este tipo de aprendizaje no necesita una función definida para optimizar, simplemente saber lo que consigue un refuerzo y lo que consigue un castigo. Entonces por lo tanto automáticamente simplemente probando y jugando aprende al final a ser un máster de cualquier juego, de cualquier sistema. Entonces esta idea de digamos castigar o beneficiar al algoritmo para al final que aprenda se puede utilizar en el trading muy fácilmente. Bueno, muy fácilmente se puede utilizar directamente en el trading. y para hacer que pueda resolver el complicado juego, entre comillas, que es el trading. Entonces, por lo tanto, el uso de este aprendizaje por refuerzo podría ser un método viable para desarrollar una estrategia de trading de criptomonedas que sea, aparte de adaptativa, que sea también beneficiosa para los desarrolladores. optimización de estrategias de minado. Como ya hemos visto antes, el minado de las criptomonedas se puede ver, gasta muchos recursos, entonces por lo tanto ya hemos visto que algunas redes neuronales o sistemas de optimización han conseguido bajar el uso de esos recursos y ya que el minado al final, como ya sabréis, digamos que necesita adivinar unos valores para resolver funciones matemáticas en un blockchain utilizando muchos recursos computacionales. Entonces al final el minero es lo que consigue resolver la función y puede actualizar el blockchain con una transacción válida, pendiente. En este caso, lo que hemos visto anteriormente fue creado por investigadores que presentaban utilizando reinforcement learning puede ser optimizado para minar, optimizar este minado de criptomonedas como por ejemplo el Bitcoin y utiliza pues el reinforcement learning, que es una técnica que se llama Q-learning pero hay muchas otras. investigaciones científicas que también tratan este tema y digamos que no son los únicos, ya ha habido muchos avances y es una buena rama para tratar y está siendo muy popular. Y por último el cryptojacking, para poner en contexto un cryptojacking son ataques que recuestan recursos de minado de criptomonedas. cada vez son más comunes y necesitan de seguridad mucho más avanzada, algunos investigadores han encontrado un nuevo método para detectar la presencia de estos programas maliciosos que digamos que secuestran a los recursos computacionales. Es un método que se llama SCK-GCN. que, si lo queréis buscar, también está basado en redes neuronales. Y digamos que es un sistema creado que identifica las similitudes cogiendo dos parejas de código. y por lo tanto es capaz de detectar, utilizando estas similitudes, o mejor dicho, las diferencias que hay entre estas dos parejas de código, la presencia de los cryptojackers. Y aquí tenemos un ejemplo un poco más visual de lo que sería un algoritmo. de predicción de un precio de una criptomoneda, en este caso del Bitcoin. cómo sería el flujo de predicción utilizando los datos que puede obtener, predecir la tendencia y el precio del bitcoin utilizando muchas diferentes técnicas de Machine Learning. En este caso vemos como primero tenemos unas features del modelo que son combinadas con el precio o digamos los datos que se tienen del precio del Bitcoin y con ello se puede entrenar un sistema de Machine Learning. para obtener cuáles son las diferentes tendencias que pueden observarse en el Bitcoin o intentar predecir cuál va a ser la siguiente, hacia dónde va a ir en las siguientes etapas temporales el precio del Bitcoin utilizando pues estos varios algoritmos. Todos estos son de Machine Learning más clásico y este de aquí estaría basado en redes neuronales, el Long Short Term Memory, que es el que hemos visto del DSTM, y con ello se puede... predecir al menos las tendencias que va a seguir. Como ya sabéis, el trading es un sistema pseudocaótico y hoy en día no hay ninguna técnica para que pizca de forma exacta porque depende de muchas cosas, pero bueno, cada vez las redes neuronales son más potentes, se adaptan mucho más al comportamiento humano y pues seguramente se acabe entendiendo con que pueda ser capaz de entender información que venga de varios sitios pues acabar prediciendo de manera muy precisa el precio de estas criptomonedas o trading en general. Pues esto sería todo, como ya habéis visto un poco por encima hemos visto cuáles son las tecnologías del Machine Learning, la tecnología del Blockchain y cómo se puede usar estas dos tecnologías, los beneficios que puedan tener y al final las aplicaciones que tienen actualmente o para un futuro de aplicar ambas tecnologías. 


### 500.E1_Reconocimiento_Óptico_de_Caracteres-video

### Reconocimiento Óptico de Caracteres
![[500.E1_Reconocimiento_Óptico_de_Caracteres.mp4]]
[Reconocimiento Óptico de Caracteres](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres)
[Reconocimiento optico de caracteres (PDF)](501.E1.Reconocimiento_optico_de_caracteres-230120-152820.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/14kHTyVrfLQa2g2znTjS2Yiqkqy54O0Ci?usp=sharing)

Hola y bienvenidos a la práctica de reconocimiento óptico de caracteres. La idea que tenía yo para esta práctica era ver un poco lo que significa el reconocimiento óptico de caracteres o OCR, tanto casos un poquito también de poner en contexto qué significa esta tecnología que mover casos más en profundidad de... en qué tipo de industria se usa, un poco también cómo funciona internamente y luego saltar directamente a un caso práctico de una librería muy usada, actualmente si no me equivoco es la que más se usa, que es el Tesseract. También veremos qué funcionalidades tiene, qué tipo de herramientas dispone al usuario. Y luego veremos casos prácticos. En los casos prácticos, yo actualmente me encuentro en un proyecto con un banco español, en mi empresa Cognizant, donde estamos haciendo un caso en el que se utiliza el OCR como la principal columna donde se asienta el proyecto. Tanto mis compañeros como yo estamos usándolo en el día a día, y puedo daros de primera mano un caso práctico que se está usando hoy en día. Y luego, por último, veremos una práctica en la que nos iremos a Google Colab. Yo he creado unos cuantos celdas en Python y veremos cómo se ejecutan y las diferencias que tiene el OCR con diferentes imágenes, cómo funcionan. En este caso, como ya hemos comentado, el OCR de la lección ha sido Tesseract por lo mucho que se utiliza hoy en día y lo fácil que es usar. Bueno, para empezar y poner también en contexto, ¿qué es el reconocimiento óptico de caracteres? Pues es una tecnología que convierte, evidentemente, texto que procede de documentos que son digitales, que no tienen texto extraíble per se, como podría ser un Word o un PDF, pues los extrae el texto. En este caso, Deep Learning es la metodología más usada. No siempre ha sido Deep Learning, pero actualmente utilizan sistemas inteligentes para detectar el texto y extraerlo. De una imagen, puede ser también un documento escaneado, con un escáner, puede ser también un PDF que no tenga texto extraíble. Los usos principales... del reconocimiento óptico de caracteres es crear flujos de trabajo automatizados digitalizando documentos de diferentes unidades comerciales. Puede ser tanto. Pues podemos ver aquí un ejemplo donde se utiliza uno CR para identificar la matrícula de un coche pero también podemos ver que no sólo se queda. en la matrícula, sino que encima es capaz de detectar las entidades, tanto del estado en el que fue una parte donde está este vehículo, la matrícula, alguna información extra, la marca... Entonces digamos que también en el OCR no se queda solo en saber dónde está el texto y ser capaz de reconocerlo, sino además también hay un fuerte reconocimiento de entidades, también de tipo de reconstrucción de documentos, saber... solo el texto en el documento que papel forma ese texto, entonces por lo tanto luego todo esto se puede meter en un flujo de inteligencia artificial o en un flujo de información que da muchísima información. También tener en cuenta que todo este reconocimiento de entidades se hace de una forma anónima, no tiene que estar nadie detrás etiquetando estos documentos, entonces por lo tanto ya no solo es quitarse muchísimo trabajo y automatizarlo, sino también se vuelve más seguro ya que se quita el factor humano en este caso. Entonces, todo lo que viene a ser el escaneo automático de DNIs o pasaportes o documentos de extractos bancarios se convierte en algo completamente anónimo ya que no hay nadie detrás en este caso. También, pues eso, lo he dicho, el reconocimiento óptico de caracteres también puede ir apoyado con otros sistemas, como el reconocimiento de entidades, como estamos viendo en las imágenes que reconoce tanto el texto como qué parte forma, si es el título, si es el precio, si es una tabla. Reconstrucción. que sería el caso de poder reconstruir el orden lógico de los documentos o las partes de un documento y por último clasificación que sería por ejemplo este documento es una matrícula, este documento es un DNI, este documento es un pasaporte, todo lo que engloba esa parte. Más específicamente, un documento. un motor que se encarga de hacer este tipo de trabajo es el Tesseract, actualmente es el más usado ya que es súper fácil de usar y además es de código abierto y es gratis. Tesseract es de HP, fue creado en HP pero luego fue esponsoreado patrocinado por Google, como hemos dicho es open source. Actualmente soporta 116 idiomas y 37 alfabetos diferentes. Y ya no solo detecta el texto, sino que encima es capaz de devolver la jerarquía del texto. Esto significa que te puede decir que, por ejemplo, si un texto depende de otro anterior, si está dentro de una tabla, si uno es el título y el resto es algo que lo sigue. Esto es capaz de detectarlo. También es capaz, como detecta varios idiomas, pues no solo es capaz de leerlo de izquierda a derecha, sino también de derecha a izquierda y en diferentes formas de texto. Ya sea, por ejemplo, en texto tipo columnas, si es un texto que forma un círculo o si son tablas y, bueno, muchas diferentes otras parámetros que se le puede dar al Tesseract para que detecte tipos especiales. En este caso el Tessera desgraciadamente tiene un problema que es que está entrenado, bueno, digamos que es muy fuerte en documentos, como podemos ver en la imagen, pero no es capaz de detectar texto salvaje, que se le llama. Que es texto, por ejemplo, de hacer una foto a un... a un coche y que te identifique el texto de la matrícula le va a costar mucho. Necesita un documento ya recortado, rotado y un documento que se parezca mucho a un PDF para detectar el texto muy fiablemente. Entonces, por lo tanto, incluso los propios de Tesseract recomiendan aplicar un preprocesamiento a la imagen antes de pasarla por el motor Tesseract. En este caso puede ser mejora de contraste para que la diferencia entre el fondo del documento y las letras quede muy resaltada y se diferencie muy bien. También puede ser una corrección de perspectiva. Si por ejemplo hemos hecho una foto del documento, que en las cuatro esquinas del documento se agranden y se deshaga esa perspectiva y que quede como si lo hubiéramos escaneado. Algunas aplicaciones de móvil lo hacen automáticamente, pero el Tesseract si no se hace ese preprocesamiento no va a ser capaz de detectar casi nada de texto o le va a costar mucho. Y en el mismo caso va a pasar en la rotación. Si le damos un documento que está rotado... tampoco va a ser capaz de detectar ya que presupone que el texto está completamente recto una vez que le damos la imagen. Esto es un poco lo que hemos visto, lo que explica también otro preprocesamiento que se hace, reducción de ruido, si también es una técnica bastante fácil de utilizar. realizar es básicamente reducir el ruido de la imagen. Se puede hacer tipos de filtro que reducen el ruido de la imagen sin tocar mucho o intentando no quitar muchas partes de texto. Y esto es, en la imagen que vemos a continuación, es un flujo de, digamos, de procesar una imagen con un OCR. La imagen sería la entrada. y digamos que primero haces unos cuantos pre-procesamientos a la imagen que hemos comentado, pues en un filtro, puede pasar el quitar la rotación, convertir a blanco y negro o mejorar el contraste, quitar el ruido. y luego internamente, en este caso del tesseract, hace toda esta parte automáticamente. Simplemente tenemos que llamar el tesseract a partir de aquí. Entonces internamente el tesseract lee las líneas, detecta cuántas líneas hay, a partir de las líneas las palabras, utilizando los espacios, y luego los caracteres que tiene cada palabra, y ahí ya se detecta cada carácter, se hace un posprocesamiento, utilizando conocimiento interno de gramática. y te devuelve el texto. Luego el texto, también es posible que haya que hacer una etapa de posprocesamiento del texto porque evidentemente no se puede adaptar a todos los casos que existen y a lo mejor tienes que adaptarlo luego para cada caso y cada tarea que quieras realizar la salida a lo que estés tratando o a tu caso específicamente. Estos son algunos parámetros que le podemos dar al tesseract, como lo que hemos comentado, que puede detectar el texto en muchas formas, tanto digamos que si es de izquierda a derecha o si por ejemplo va por columnas, si es una parte de un círculo o por tablas, digamos que esto es lo que el Page Segmentation Mode, o PSM, es un parámetro que le podemos dar. Podemos decirle que sea una columna sola. que digamos que es un bloque uniforme de texto alineado verticalmente, que simplemente es una palabra, que simplemente sea un texto, una línea de texto que forma un círculo, que simplemente sea un carácter. Y luego el 11 y el 12 es un poco, digamos, en cuenta todo el texto posible sin que forme parte de ninguna jerarquía, o sea, ninguna segmentación de página. Eso, todo esto son más opciones que nos da el Tesseract. para poder adecuarnos a nuestro tipo de problema. Como he comentado antes, en mi empresa actual, en Cognizant, estamos llevando a cabo un proyecto muy grande de inteligencia artificial pura, de lectura, escaneo y reconocimiento de documentos para una empresa de banca española. Y... Y como hemos comentado, ahora mismo es el proyecto más grande de Inteligencia Artificial Sur de Europa y utilizamos las salidas del OCR para sacar información de cada documento, cómo aparecen las imágenes, evidentemente las imágenes están sacadas de internet, no son del proyecto actual. Podemos también unir... diferentes documentos que vienen de diferentes partes y unirlos a un mismo usuario o a una misma empresa. Podemos luego dirigir cada documento a donde va porque sabemos lo que contiene y sabemos exactamente cuáles son los pasos siguientes para ese documento. Y luego también es algo que se puede escalar, ya no solo, digamos, a la clase que estamos viendo ahora, también somos capaces de escalarlo a otro tipo de tareas. todo esto partiendo de un sistema que detecte los caracteres, o sea, que detecte el texto de los documentos y la jerarquía también de los documentos de manera eficaz. Efectivamente, pues es algo que interesa mucho actualmente, específicamente, pero no sólo limitado a las bancas porque lo que hemos dicho es algo que se puede automatizar muy fácilmente. que se quita muchísimo gasto humano de recursos y luego encima al quitar esa parte humana pues también se reduce mucho la latencia y también se aumenta mucho la seguridad. En este caso pues ya hemos visto que todo el reconocimiento de caracteres y tal son algoritmos entonces por lo tanto es completamente anónimo y no digamos que no se va a quedar con ninguna información que a lo mejor un humano sí que podría tener. Y el resto de la sesión vamos a ver el collab. Aquí tenéis el link que coloquiqueis cuando vayamos todos. En esta práctica lo que he preparado es, digamos, un par de formas de leer el texto de un documento. tanto si el documento es un PDF como si es una imagen. Veremos que si es un PDF a lo mejor tiene el texto extraíble. Entonces accederemos a la meta información del PDF con el VREA que se llama PDFminer. Entonces podremos leer el PDF y extraer el texto. Lo bueno de la librería es que si hay texto extraíble a lo mejor también es capaz de extraer las imágenes, por ejemplo. Entonces nos daría mucha información. Estaría también junto al texto te devuelve su posición, te devolvería también obviamente el texto y demás información. Lo dicho, es muy útil porque hay muchas veces que a lo mejor no necesitamos un OCR si los documentos que nos dan son en PDF. Sin embargo también hay que tener en cuenta que no todos los... PDFs tienen texto extraíble. Entonces, para el resto de casos, tanto imágenes como PDFs sin caso extraíble, cogeremos e instalaremos el Tesseract en Colab y veremos rápidamente unos cuantos ejemplos de casos en los que funciona muy bien, casos en los que no funciona tan bien, cómo corregir esos casos, a lo mejor aplica alguna técnica de mejora. y ver como afecta eso a la salida del modelo. Nos encontramos aquí en el collab, poco os lo he preparado ya por partes, está ya separado por los diferentes ejercicios que vamos a hacer, así que nada, todas las celdas se van a ejecutar desde arriba hacia abajo, entonces por lo tanto esto ya lo he ejecutado yo pero vosotros tendréis que instalar las dependencias, seguramente os pondrá un mensaje que tendréis que leer. hacer un restart de la rutina del collab así que le dais y ya estaría. Aquí es donde vamos a importar todas las librerías, vale, si no estáis relacionados con Python no pasa nada, simplemente todas las funciones que vayamos a usar las tenemos que tener dentro del programa y si no pues no vas a ver dónde buscarlo. todas las imágenes y los archivos y los pdfs que usemos en esta demo no van a estar en el colab ni os vais a tener que meter en ningún filesystem simplemente utilizando la función de wget podemos acceder a cualquier documento que esté en la web y descargarnoslo para meterlo automáticamente en el filesystem que viene con él en mi caso ya he ejecutado algunas de estas celdas este por ejemplo es el pdf si no recuerdo mal el de atención vale entonces nos lo descargamos vale que parezca todo correcto y aquí es donde como es un pdf que bueno podéis ver que yo me puedo meter en el pdf y puedo seleccionar el texto entonces por qué no intentar acceder a la meta información que tiene el propio pds y extraer la información que tiene ese pdf porque a lo mejor no tengo que tirar un OCR si puedo acceder directamente a esa información. Entonces aquí comentándolo línea por línea cargaremos el archivo, en este caso utilizaremos la función open de Python y le daremos el flag de read bytes para que lo lea como un buffer de bytes. y utilizaremos la librería pypdf2. Entonces le daremos el objeto que ha leído The Bytes, y se lo daremos a la función de FileReader. En este caso ya nos devolverá un objeto de la librería. Entonces en ese objeto podemos acceder a sus atributos, como por ejemplo el número de páginas. pero también podemos acceder al documento, a la información que tenga ese documento. Entonces en este caso lo vamos a guardar en una variable que se llama información, info, y luego lo mostraremos por pantalla. Entonces de tal forma que el número de páginas va a ser el número de páginas, el título va a ser el título, el autor el autor, ¿vale? Y así un poco iremos a ver qué información podemos extraer directamente del PDF y lo comprobaremos con el propio PDF a ver si es correcto. así que vamos a ejecutarlo. Ha sido muy rápido obviamente. Vemos que el número de páginas son 11 y podemos ver aquí que efectivamente tiene 11 páginas el documento. El título o title, attention is all you need, efectivamente así es el título. El autor, pues varios autores que podemos ver que están aquí todos referenciados. El subject, esto ya no lo pone en el PDF en sí, sino que es de la información que tiene el documento. El creador vemos que no está definido y el producer tampoco. Entonces creo que automáticamente se pone la librería que hemos usado. Pero bueno, el resto de cosas, aquí hay información muy útil, tanto el autor como el título del documento, como el número de páginas. Yo personalmente, la información que utilizo en el proyecto que estamos ahora, sobre todo, es el número de páginas para poder iterar sobre ellas. Antes de comentar que tanto hay librerías como pdf2 que te extran la meta de información del texto, también hay otras que te convierten los pdfs a imágenes para luego poder procesar los tugs. Por otra parte, si no hay suficiente texto extraíble en los documentos o no hay directamente. Aquí voy a comprobar antes que se esté grabando todo. Un momentito, vale, disculpad. Aquí es donde vamos a utilizar justo la variable de número de páginas, que como hemos visto nos devuelve un número entero, en este caso 11, y vamos a hacer un bucle for que significa para cada página vamos a iterar sobre ellas y leeremos el contenido. Disculpad el corte, he visto un pequeño fallo que había en el código y lo he corregido. Fallos del director. Entonces una vez creado el objeto de la página utilizando el objeto que nos ha devuelto antes la función y la función getPage le damos como argumento el número de la página. y nos devolverá un objeto que contiene información de esa página. En este caso nos interesa el texto entonces llamaremos sobre ese objeto a la función extractText. En mi caso vamos a imprimir, he decidido imprimir un salto de línea y unas cuantas barras para separar las páginas, que nos imprima también el número de la página y luego el contenido de la página y luego pues cerraremos el pdf cuando lo hayamos dejado de leer. Vamos a ejecutarlo. vale pues podéis ver que se ejecuta efectivamente todo entonces vamos a ir al principio vale pues tampoco vamos a leerlo aquí pero bueno página 0 vale empieza a contar desde 0 entonces acabará en la 10 pues eso attention is all you need los diferentes autores el abstract vale todo esto pues evidentemente se corresponde a esto de aquí y digamos que ya tenemos un texto que es modificable por un ordenador. Yo puedo a partir de aquí buscar por ejemplo cada, por palabras clave, como haremos a continuación y o estar diferente información que antes pues evidentemente no podía. Vamos a limpiar este output. Ahora vamos a ir con imágenes. En este caso evidentemente una imagen no contiene texto extraíble ni leíble por una máquina a priori. Entonces vamos a decodificar ese texto, vamos a leerlo utilizando Tesseract para poder acceder a luego ya a modificar ese texto o a ver qué tal lo ha hecho. Esto es una pequeña función que utiliza la librería Matplotlib. para mostrar imágenes por pantalla. No voy a ir muy en profundidad evidentemente de qué significa esto y todo eso pero bueno básicamente como estamos utilizando OpenCV que es una librería de computer vision para leer las imágenes como matrices de números pues digamos que OpenCV lo lee en formato BGR que es blue green and red pero luego para mostrarlo por imagen lo muestra en RGB por lo tanto pues hay que cambiarle los colores y esto es lo que significa. No mostramos el axis porque en este caso estamos mostrando imágenes y lo mostramos a la pantalla. Una vez ejecutada la función esta, para que la meta en memoria vamos a descargarnos la primera imagen que es esta de aquí. Es una imagen bastante, digamos, se lee todo bastante claro, no es un escaneo ni nada, es una imagen completamente digital. no tiene muy buena resolución pero bueno es más que suficiente entonces esto digamos que es un caso perfecto. Aquí cargaremos las imágenes, utilizaremos la función de OpenCV para leer la imagen, la mostraremos por pantalla y luego le daremos esta configuración al tesseract. La configuración, si lo recordáis de lo explicado en el pdf, es el page cementation 11 que significa que el pdf está saca el texto que pueda sin preocuparte de cómo esté repartido por la imagen, por así decirlo, y luego le vamos a especificar que el idioma es el inglés. Aquí ya podéis hacer modificaciones de qué pasaría si le quito el idioma inglés, si funciona peor o mejor. No siempre funciona mejor, no siempre funciona peor, es un poco dependiendo de cada caso. Y luego, lo que sí que es obvio es que si queréis detectar caracteres especiales que tenga un idioma, siempre es bueno decírselo porque si no va a ser muy difícil o directamente no os lo detectará, esos caracteres. y directamente llevamos a la función, es una función super fácil, de tesseract que le damos la imagen y es imageToString y como indica la función pues va a pasar de una imagen a un string de palabras o un texto ejecutamos, nos muestra la imagen y aquí nos muestra el texto que he extraído y pues lo podéis comprobar vosotros pero vamos está perfectamente extraído no se ha equivocado en ninguna parte efectivamente es un texto muy fácil y nos ha extraído sin mucha dificultad el texto, pero también no solo devuelve el texto, hay otra función que se llama imageToData que le volvemos a dar la imagen y la configuración nuestra como entrada y vamos a ver que devuelve este tipo de función. Aquí vemos que devuelve como un data frame en el que nos devuelve información de nivel, número de página, número de bloque, parNum, número de línea, número de párrafo, número de línea, número de palabra, left, top, width, height, const, text. Esto es una... un data frame o un diccionario que nos va a devolver mucha información por cada palabra. ¿Vale? Veis que la palabra this, que es la primera de todas, pues te dice que un poquito de izquierda, de derecha a izquierda la confianza es del 96%, o sea que está muy seguro de que pone esa imagen. Vamos a abrir todo lo que hay aquí porque esto ya son cosas internas de párrafos o de caracteres especiales. La siguiente información nos va a dar la posición que tiene ese texto en la imagen, los píxeles en los que se encuentra ese texto. Y luego todo esto nos va a decir que fue una parte de un nivel número 5, el número de página número 1, el bloque 1. el párrafo 1 y digamos que diferente información sobre jerarquía que contiene más de información sobre el texto que es en plan pues digamos que no sólo ha detectado this sino que te dice exactamente dónde está localizada y qué jerarquía forma con respecto al resto del texto. Vamos a también a limpiar un poco este output. Vamos a probar ahora con imagen de peor calidad. Vamos a ver qué imagen es. Ya nos lo he imprimido aquí por pantalla. Es una imagen que podemos ver que también es texto por texto de digital perdón, digital no, texto de documento pero la calidad es mucho. Pues lo primero es que está un poco rotado luego además no hay mucho contraste, no era como la imagen esta en la que el negro y el blanco se diferencia muchísimo, aquí hay unos cuantos grises en medio y luego además pues la calidad también es muy baja, entonces vamos a ver si es capaz de detectar. Bueno, aquí no lo he mencionado pero volvemos a llamar con la misma configuración a la misma función que antes, nada nuevo, imprimir el texto. Vemos que sí que es capaz de detectar las primeras líneas. pero luego la palabra dog se lo ha comido. Pues digamos que hacer un debut de esto es un poco complicado porque evidentemente todo esto es muy alto nivel pero por lo que sea no ha detectado la palabra dog en este caso si era una abstracción pues casi perfecta pero vemos que ha fallado en dos caracteres. Si también podemos ver la posición y la jerarquía que ha estudiado el texto y podemos ver también que si antes esta jerarquía era la misma osea digamos nos devolvía caracteres ups disculpad voy a cargar la imagen esta un segundo la confianza que nos daba del texto 95, 96, 96 96% osea digamos que una precision muy alta vemos que ahora esa nueva confianza se convierte pues también hay muchos 96 pero en la palabra D por ejemplo ya ha bajado a 80 y directamente el DOG la O y la G y el punto nos los ha cogido Entonces podemos ver también que es bastante sensible a cualquier deformación de la imagen. Vamos a probar ahora con la imagen manuscrita, que para nosotros es muy fácil leerla, pero vamos a ver si el TSRack es capaz de detectar texto manuscrito. Vale y vemos que falla bastante. Esto es porque digamos que todo lo que se vaya fuera de, como hemos dicho en el PDF, en la presentación anterior, todo lo que se salga de documentos, por así decirlo, de uso, no es capaz de detectar tanto ya cosas rotadas, de peor calidad, manuscrito, otro tipo de... de letra que no sea la típica de documento entonces pues todo eso no es capaz de detectarlo. Ha hecho lo que ha intentado su mejor pero no ha sido capaz. Vale ahora vamos a hacer un pequeño ejercicio por último ya para acabar de extracción de entidades en la que vamos a leer esta página, vamos a imprimirla. que es un documento legal, está extraído de internet, digamos que no significa nada, está en formato imagen, entonces por lo tanto no podemos extraer ningún texto a priori. pero digamos que yo quiero tener un sistema que automatice esto, que los usuarios me puedan enviar estos documentos en foto y yo pueda acceder al nombre, a yo que sé, por ejemplo, el número de teléfono y a dónde reside. Entonces vamos a crear un pequeño flujo de imagen en el que hagamos exactamente esto. ¿Cómo haré ese flujo? Pues cuando extraiga el texto a la imagen podremos ver que tanto la palabra resident name y fake person en este caso están muy juntos, están digamos seguidos en la extracción del texto. Entonces si yo busco la palabra clave resident name pues me dirigirá automáticamente al nombre de esa persona. Entonces lo primero como hemos dicho vamos a convertirlo a texto. Pues aquí nos lo ha convertido todo a texto. el texto que ha detectado no es muy bueno en este caso porque digamos que no tiene muy buena calidad el documento para empezar pero bueno lo que es necesario sí que lo ha extraído bien. Entonces vamos a hacer ahora unas cuantas funciones de Python. Voy a explicarlas un poco por encima. Esto lo que nos va a hacer es quitarnos caracteres especiales como saltos de línea o saltos de diferentes caracteres del propio string de Python. Luego lo separaremos por saltos de línea. Disculpad, el stream no quitaría saltos de línea, efectivamente. Porque aquí lo separaremos el texto por saltos de línea. Es decir, cada una de las entidades, cada una de las líneas va a formar parte de un índice diferente en una lista. Luego, cada uno de los índices de esa lista lo vamos a separar por espacios. por estos espacios de aquí. De tal forma que ya vamos a tener una lista que contiene todas las palabras del documento. Esto simplemente es para convertirlo a la raíz de palabras. Ahora vamos a filtrar, vamos a quitar todas las palabras que no tengan texto porque por lo que sea a lo mejor ha habido dos espacios seguidos entonces no hay ningún texto en esos espacios. vamos a ver cómo se limpia el texto. Como hemos dicho ya tenemos un array con todas las palabras, un poco limpio. Entonces yo quiero extraer, pues como hemos comentado antes que está el nombre, el número de teléfono, la ciudad y el código postal. Entonces podemos ver que en la imagen pues tenemos que buscar recién el nombre, tenemos que buscar teléfono número. y tenemos que buscar también el zip que estaría aquí. Entonces vamos a hacer eso mismo, buscarlo y la función que tiene para buscar es index, la función que tiene Python. Entonces yo buscaré nombre y me quedaré con qué índice de este array contiene la palabra nombre y lo mismo con el resto. Entonces ahora lo que voy a hacer es sabiendo que el nombre me espero dos palabras, que con el teléfono me espero a lo mejor dos palabras también, con la ciudad de residencia también me espero unas cuantas palabras predefinidas y el código postal es solo una palabra, pues entonces puedo acceder a esa información dentro de la lista. En este caso pues el idre también está un poco hecho a drede para que funcione bien porque lo he comprobado que funcionaba bien. pero no has extraído correctamente esa información de forma automática. O sea, digamos que yo solo le he dado una imagen de entrada, completamente que no tiene nada de texto extraíble, y tendría que estar una persona detrás accediendo a esa información y leyéndola. Pero se le ha dado al tesseract y con un poquito de programación básica en Python he sido capaz de extraer las entidades que yo quiero. ¿Vale? Esto es un ejemplo muy básico. Efectivamente, no es la mejor forma de hacerlo. pero digamos que es un ejemplo suficientemente que demuestra un poco la capacidad y la facilidad que tienen estos algoritmos de muy pocas líneas poder hacer un sistema automático automatizado de extracción de esas entidades. Esto sería el laboratorio y con esto concluye la práctica de OCR. cualquier duda que tengáis o cualquier sugerencia que tengáis me podéis escribir como hemos comentado a tanto al linkedin como a github y tanto también dentro del linkedin está el correo electrónico si os viene mejor y eso.


### 502.E1_Generar_Imágenes_con_Stable_Diffusion-video

### Generar Imágenes con Stable Diffusion
![[502.E1_Generar_Imágenes_con_Stable_Diffusion.mp4]]
[Generar Imágenes con Stable Diffusion](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion)
[Generar Imágenes con Stable Diffusion (PDF)](503.E1_StableDiffusion-230120-152902.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1KwcbPDiRL_RArlVdWvopIZ-_nEVICn1K?usp=sharing)

Hola y bienvenidos a un nuevo curso de W3. En este curso vamos a dar Stable Diffusion y diferentes modelos también de traducción de texto a imagen o de generación de imágenes a partir de texto. La idea que estaba pensada para esta clase era empezar con un poco la trayectoria que han seguido estos modelos, ver un poco sus inicios, cómo han empezado, las primeras redes que hacían este tipo de tarea y luego también pasar directamente a dónde está el state of the art ahora mismo, los modelos actuales, qué tipo de imágenes pueden generar, para qué se pueden usar. Como ya sabéis esto también interesa tanto a nivel de investigación, a nivel de deep learning y inteligencia artificial. Evidentemente es un tema extremadamente interesante y con mucho futuro y que está teniendo muchísimo desarrollo en estos últimos, sobre todo en estos últimos meses y en estos últimos dos años. Entonces veremos un poco por qué existe ese interés y luego también cómo se puede usar y qué significa para el arte sobre todo, porque claro ahora como se está poniendo de moda, por ejemplo, la generación de arte digital de NFTs, de arte digital y de compra y venta de estas piezas de arte, pues también un poco este tipo de modelos ayudarían o cambiarían un poco ese tipo de mercados, de tal forma que cualquiera que tenga conexión a internet y la mayoría de veces gratis o pagando muy poco dinero puede generar imágenes distinguibles de imágenes reales o de piezas de arte generadas por cualquier autor en cualquier estilo a lo largo de la historia, básicamente con este tipo de herramientas. Entonces por lo tanto son bastante útiles de conocer y evidentemente van a ser muy prácticas y muy usadas en el futuro. Por último vamos a ver una práctica, unos pequeños ejemplos hechos por mí para vosotros en Collab. Está todo bastante preparado, simplemente iremos ejecutando e iremos explicando cómo generar imágenes de forma gratis desde Collab con muy buena calidad. Eso lo veremos al final. Para empezar, como ya hemos comentado, vamos a ver un poco de dónde viene este tipo de modelos. La verdad es que son relativamente recientes. Por ejemplo, uno muy famoso fue Stack Gun, que fue del 2017 y era un hito, por así decirlo, para este tipo de modelos porque hasta ahora no se han generado imágenes de tanta calidad. Evidentemente para hoy en día no es muy buena calidad, pero bueno para aquel entonces la verdad es que era una pasada y como no sé si se puede ver por la imagen, pero digamos que tiene dos etapas el modelo. La primera etapa generaba un pequeño boceto muy borroso con mucho ruido del texto que se intentaba generar y luego una segunda etapa lo que hacía era con el boceto y con el texto intentar realizar una especie como de super resolución o de mejorar la resolución y la calidad de la imagen teniendo en cuenta también lo que se quiere generar. Digamos que pensando desde un punto de vista estadística es un problema que se llama de one to many porque digamos que un mismo texto puede generar una cantidad casi infinita o infinita de imágenes. Por lo tanto es el modelo el que no hay una respuesta correcta. Sí que existen incorrectas o mejor dicho lejanas a la idea que se quería generar, pero digamos que no existe solo una respuesta correcta. Esto hace que al entrenarlo digamos que sea muy complicado y muy complejo. En este caso se entrenó esta red neuronal la stack GAN en la metodología GAN que es una forma de generar imágenes en la que se contrasta la generación con un discriminador que te dice si lo que se ha generado es realista o no, por así decirlo. Intenta saber si lo que se ha generado es falso o verdadero y con la salida de este discriminador también hay un bucle en la que se se entrena al propio generador para que aprenda a engañar al discriminador. Entonces al final este bucle y este juego lo que hace es generar imágenes con mucha calidad y con no demasiados datos ya que es casi imposible o es muy difícil o teóricamente no se podría hacer overfitting de este tipo de modelos ya que no ven los datos originales. Esto sería en 2017. Vamos a dar un salto de cuatro años al 2021. En enero del 2021 la compañía que empezó un poco esta esta nueva moda de generación de imágenes a partir de texto fue OpenAI. OpenAI en enero del año pasado del 2021 sacó su primera versión de Dall-E que era un modelo que utilizaba un tipo de generación de imágenes que se llama VQ-VAE que es Vector Quantisized Variational Autoencoder que digamos que lo que utiliza es una representación discreta de los datos en lugar de continua como por ejemplo pueden ser las GANs o otro tipo de modelos más usados para este tipo de generación de imágenes. Esto lo que hace es que tenga muchísima más representación, ese espacio embedido y además que de capacidad a muchas más variedades diferentes. Entonces por tanto gracias a esta nueva representación digamos que era fue más entre comillas fácil o fue un poco el desencadenante de que la calidad de las imágenes fuera mucho mejor y en este caso se demuestra en la imagen de la izquierda es la del 2021 y la de la derecha se correspondería a su variante más actual en abril del 2022 creo que es un poquito anterior pero bueno el paper revisado la última revisión que se hizo fue en abril del 2022 en el que OpenAI sacó su segunda versión y bueno la calidad el salto de calidad fue increíble no solo tampoco de calidad sino también de tamaño y de entendimiento del texto y un poco lo que cambió de un modelo al siguiente es que el siguiente no utiliza la metodología que hemos hablado de los VQBAE sino que utiliza un proceso de generación de imágenes en este caso o de datos en general que se llama modelo de difusión. El modelo de difusión digamos que es una técnica en la que a una imagen conocida se le añade ruido poco a poco paso por paso en el que en cada paso se le va añadiendo más ruido y digamos que en cada paso que se realiza lo que lo que se crea es una renderonal que vea un poco cómo se ha añadido el ruido y que aprenda a deshacer ese ruido todo esto por pasos entonces al final pues por ejemplo tenemos 700 pasos en los que se ha añadido 700 veces un poquito de ruido qué pasa que al final después de añadir 700 veces ruido pues te queda una imagen muchísimo ruido tanto ruido que digamos que es indistinguible de una imagen simplemente de ruido que digamos que la imagen original queda completamente ocultada y eclipsada por el ruido pero como hemos tenido una renderonal que ha aprendido o le hemos enseñado a deshacer cada paso lo que vamos a hacer es empezar desde el paso 700 y deshacer y que cree de esta forma hasta que deshaga todo el ruido vaya quitando el ruido en estos 700 pasos entonces nos queda al final una imagen sin nada de ruido como las que vemos en la pantalla esto es lo que nos permite dado que después de todos esos 700 pasos te queda ruido básicamente porque la imagen ya ha sido completamente destruida pues lo que vamos a hacer es simplemente coger ruido aleatorio y pedir que deshaga los pasos entonces este proceso a partir de ruido aleatorio puede es capaz de generar imágenes que no existen en la en nuestra digamos en nuestra base de datos que no ha sido entrenado con ellas por así decirlo entonces lo que hacemos es al generar al realizar este proceso inverso de difusión que se llama el quitar ruido lo que hacemos es condicionarlo con contexto entonces si simplemente le ponemos una condición textual a este proceso de difusión inverso lo que hacemos es condicionar que digamos que genera me o quita el ruido de esta imagen sabiendo que en esta imagen hay pues por ejemplo lo que puede ser eso cerro en un campo en el estilo de moned por ejemplo entonces por lo tanto es capaz de generar datos con muchísima más calidad y con digamos que también el proceso de generación de imágenes mucho más estable que utilizando por ejemplo las guns como hemos visto en la diapositiva anterior y esto permite también con data sets mucho más grandes como por ejemplo la que fue entrenada Dall-E que son billones de imágenes pues también el tener información de mucha cantidad de diferentes objetos artistas animales situaciones y todo un poco sin embargo menos conocido en mayo del 2022 google por ejemplo también ha sacado un par de modelos de este estilo google lo que hizo es generar utilizar otro tipo de arquitectura que no vamos a entrar tampoco en él es un poco parecida a la anterior que hemos visto de uvecubae combinada con guns pero era muchísimo más robusto en algunos problemas que tenía open y el Dall-E de open y tiene un gran problema por así decirlo que es que no sabe escribir vale como se puede ver en la imagen de la izquierda estos serían textos generados por open y por Dall-E la que podéis usar todos pagando una pequeña cuota en su en su api digamos que si le pones un cartel que por ejemplo que genere un cartel en el que se escribe en el que se puede leer deep learning pues ya veis que genera un poco de todo menos lo que le estamos pidiendo mientras que por ejemplo si a esta a la imagen de google le pedimos que genere una una tienda en la que se puede leer en el que tiene un letrado de que ponga texto image pues es bastante elegible de forma humana mientras que en esta parte de aquí sería también la generada por Dall-E en la que no ha conseguido leer escribir el texto y hay veces que pues parece como una especie de combinación rara de las letras o que como que intenta hacerlo pero no lo consigue y esto es un poco de la potencia del modelo no permite no tiene tanta entendimiento de geometría sobre todo le cuesta bastante y es algo que también le cuesta a la mayoría modelos a la imagen también que es la geometría del entendimiento de una escena a nivel geométrico es algo extremadamente complicado que obviamente lo sabemos desde nacimiento pero digamos que al decir por ejemplo quiero una foto en la que haya 35 pelotas de ping-pong pues a este tipo de redes le cuesta muchísimo porque claro tiene que entender pues que por ejemplo que una pelota de ping-pong donde empieza donde acaba nivel de píxel o nivel de concepto que se vean todas por la imagen que no haya ningún tipo de contaminación entre conceptos entonces claro a partir de creo que eran de 7 o de 8 objetos se lía si le pedimos por ejemplo a partir de 10 pelotas de tenis o lo que sea pues en principio se va a empezar a liar pero bueno en texto pasa un poco algo parecido en el que digamos que en texto tienes que entender el orden de lo que se está escribiendo aparte de también entender que luego un humano lo pueda leer y es algo que OpenAI, Dall-E perdón, no lo entendía y por ejemplo el de imagen en este caso sí que se el imagen de google sí que lo consigue llegar a entender luego google sacó otra versión de un modelo de texto-imagen estos ya como os digo no están públicamente accesibles y no parece que los vayan a lanzar en ningún momento son modelos mucho más grandes que Dall-E pero aparte de eso también pues digamos que la calidad de las imágenes en principio es mejor y luego bueno ya podéis ver por las imágenes que ya no sólo conceptos un poco más abstractos como una cobra hecha de sushi o lo que sea que a lo mejor también Dall-E también lo podría llegar a generar o por ejemplo un wombat tomándose un un cóctel con camiseta hawaiana y todo eso también Dall-E lo podría llegar a generar pero en el texto como vemos en la en la de la derecha del todo el texto es de muy buena calidad y cumple con todas las los requerimientos de diseño y de sobre todo de estilo que se le pide entonces pues la verdad es que es un buen paso hacia esa dirección de entendimiento de la escena sobre todo se ve al final el texto el que escriba algo es una digamos que es una excusa para saber si o determinar si un modelo es capaz de entender cosas muy complicadas como por ejemplo en la escritura entonces pues digamos que si es capaz de escribir texto que nosotros podamos leer es una muy buena señal una cosa a tener en cuenta también que pues también ha sido bastante hablado a la vez que se comen que se iban saliendo este tipo de papers es que pues evidentemente muchos todo esto este research se está haciendo desde américa principalmente entonces por lo tanto pues los datos también de internet en general están un poquito o sea bueno tienden hacia la cultura americana europea entonces por lo tanto es un poco razonable o sea es razonable el decir que existe un baias una tendencia en este tipo de modelos dado que pues evidentemente hay un punto de referencia en el que en el que se crean estos modelos entonces por lo tanto pues evidentemente siempre que quieras yo que sé por ejemplo medir la palabra exótico pues siempre que pues en cualquier parte que te vayas de la tierra la palabra exótico se va a significar cosas completamente diferentes y en la por ejemplo la imagen que vemos a continuación en esta parte de aquí la palabra exótico pues evidentemente tiene una definición muy europea y muy digamos americana y una persona exótica pues tampoco te va a salir una persona como como las que estamos acostumbrados a ver y sale pues un punto de vista muy diferente al de por ejemplo cualquier otra persona de otro país también estas son este por ejemplo el punto de vista de una persona exótica pues puede ser una de las menos perjudiciales pero sí que pues otras sí que fomentan los estereotipos que tenemos en este tipo de países y lo cual es un poco evidente y lo que estaba comentando en la diapositiva anterior es que evidentemente el si los datos es garbachín garbachado si le das basura te da basura entonces si los datos que le hemos dado es internet entero y en internet por lo que sea pues está plagado de de este tipo de tendencia americana europea pues evidentemente lo que saca este modelo va a ser ese tipo de cosas que ha aprendido esto puede ser evidentemente perjudicial también evidentemente si a este tipo de modelos en vez de simplemente decirle quiero un terrorista pues yo que sé quiero un terrorista rubio con ojos azules pues evidentemente te va a sacar un terrorista rubio con ojos azules entonces también esto es muy fácil mitigable cualquiera cualquier persona lo puede evitar pero bueno esto simplemente es lo que saca el modelo de Dall-E en este caso con palabras o con conceptos muy muy muy generales hemos hablado mucho de Dall-E que es de pago hemos hablado también de los modelos de google que no son accesibles para el público en general y ahora lo que vamos a ver son modelos que son gratis o también otros modelos que son más baratos en algunos de ellos veremos que que son que tiene una prueba gratuita pero que luego habrá que que si se quiere generar más pues pues también el habrá que pagar un poco después otros son no dejan prueba gratuita y otros son 100% gratuitos de estos vamos a ver un poco el estilo de imágenes que generan los pros y contras de cada uno y un poco pues también para que si en algún momento alguien quiere hacer algún tipo de proyecto sobre esto pues también saber qué puede esperar de ejecutar un mismo texto en varios de estos sistemas el primero mencionar otra vez más Dall-E, Dall-E no tiene prueba gratuita pero pero digamos que puedes generar 115 imágenes por 15 dólares y permite hacer una gran variedad de de tiene una gran variedad de herramientas dentro de su de su página web digamos que la este ejemplo de aquí sería la de simplemente poner texto decir pues en este caso yo que sé un par de ositos de peluche químicos en óleo y digamos que te generaría una unas 4 o 5 imágenes pero también le podemos dar una imagen por aquí como por ejemplo la de vermeer este la de la mujer con él con el pendiente y te genera variaciones de la imagen que le le vayamos a dar esto se puede utilizar posiblemente para logos para retratos para otro tipo de arte para que te genere por ejemplo piezas similares a la que tú quieres o fotografías por ejemplo de un diseño de una habitación y todo eso y no sólo eso sino que también sobre la misma imagen otra herramienta que tiene Dall-E es que podemos estirar la imagen de forma coherente entonces al estirar la imagen esta es el cuadro original vale la pieza que está aquí en también en el recuadro es capaz de extender la imagen en todas las direcciones de forma arbitraria o sea no tiene final simplemente el número de dinero que os queréis gastar en la que digamos que se puede ir generando parches por parche viendo hacia dónde va la imaginación para decirlo entre muchas comillas del Dall-E entonces copiará el estilo copiará que sea una transición buena entre entre la imagen que le vayáis a dar y el resto la imagen que tenga sentido y todo eso lo dicho es una herramienta muy potente la calidad de imágenes que genera es muy buena pero es de pago y el modelo no es libre para que digamos que nosotros no podemos acceder al modelo y ver a ver qué pasa dentro del modelo y tan simplemente podemos acceder a las salidas lo cual evidentemente para la mayoría de personas es más que suficiente pero a la hora de crear en la comunidad por ejemplo no hay ninguna comunidad de esto otro otro modelo disponible es mi journey que es también muy famoso creo que es el bueno en mi opinión es el segundo y por lo que he visto el segundo más famoso después de Dall-E en este caso se utiliza solo por discord en el que tú es un bot de discord en el que bueno también es un canal de disco en el que tú te puedes meter y puedes pedirle que genere imágenes y en la forma gratuita de forma gratuita creo que se pueden generar unas cuantas unas 20 o 30 no recuerdo el número exacto y a partir de ahí son 10 dólares por 200 imágenes bastante más barato que Dall-E y al final pues por imagen son unas unas cuantas fracciones de centímetros por lo tanto pues en verdad por imagen sabiendo lo que costaría de normal con un artista y todo eso pues es bueno es interesante para la gente que lo necesite esto la diferencia con Dall-E es que ha sido entrenado con con artistas famosos de todo tipo tanto artistas clásicos como artistas actuales y artistas sobre todo muy famosos de arte digital y todo eso por eso también tiene una tendencia muy clara a sacar siempre una salida muy artística mientras que Dall-E tiende a sacar algo realista y por ejemplo el cuadro bueno el cuadro la pieza de la izquierda es un poco la una exageración de lo que saca de normal Dall-E mid journey que son como muchas fractales pero muy artistas muy artístico muy este estilo no sé pintura de cristal de este no sé muy bien cómo describirlo pero pero siempre tiende como muy a la fantasía y se le da muy bien las piezas fantásticas mientras que también es capaz como se ve en la pieza de la derecha de generar pues algo que es muy imaginativo por así decirlo y no existe en la vida real y pero de forma más o menos realista mid journey también dentro de la aplicación de discord dentro dentro del disco se le puede pedir que digamos que una vez que genera cuatro imágenes parecidas a lo que tú le dices o sea bueno cuatro imágenes a partir del texto que tú le dices puedes elegir una de ellas por ejemplo y aplicarle un algoritmo de súper resolución por eso es capaz de sacar este tipo de imágenes aparte de que Dall-E sólo puede generar imágenes de 1024 x 1024 píxeles y esto puede generar imágenes de una resolución arbitraria y luego además si se quiere más resolución se le puede aplicar algoritmo de súper resolución varias veces hasta poderlo creo que era multiplicar por 8 la resolución y tener una imagen creo que en 2k se puede llegar a tener luego a partir de ello hay otras otros algoritmos de y otras páginas web que escalan la imagen mucho más entonces por tanto pues digamos que se puede llegar a tener incluso una calidad suficiente como para hacer un póster o cualquier cosa que es ni te pucha calidad por último vamos a ver stable diffusion que es un modelo que es 100% gracis y además está públicamente accesible bueno se me olvidó comentar que mid journey no es público tampoco el acceso al modelo en este caso sí en este caso nos dan el modelo los pesos del modelo el código suficiente como para ejecutar el modelo y ya está nos dicen que que hagamos lo que queramos con eso no está capado de ninguna forma puede generar cualquier tipo de imágenes que le digamos no no tiene ningún filtro ya que lo estamos ejecutando desde local otra cosa es que elijamos ejecutarlo desde utilizando alguna api o utilizando hugging face o lo que sea que en ese caso sí que va a estar capado pero si nosotros somos los que nos descargamos los pesos no tiene ningún tipo de filtro de ninguna forma está entrenado con artistas modernos e imágenes de fotografía de imágenes normales y yo por la experiencia que tengo es muy bueno generando sobre todo piezas realistas de sobre todo imágenes de personas es como ya estáis viendo en las imágenes estas son increíbles y cuesta mucho saber que no han sido hechas por inteligencia artificial de hecho en la mayoría bueno en sobre todo en la de la izquierda a ser que se te diga es muy complicado y luego también pues conceptos como por ejemplo el del medio es evidentemente no es un retrato por así decirlo es un receto imaginativo y la verdad es que también lo defiende muy bien este es el que vamos a usar luego en la práctica entonces tampoco voy a entrar mucho en detalle de cómo funciona evidentemente se llama stable difusión es porque utiliza el modelo o la técnica de difusión que hemos comentado anteriormente que también utilizaba Dall-E y luego lo veremos generaremos unas cuantas imágenes utilizando este modelo de aquí y ya que este botifusión es gratis y está disponible el código lo que ha hecho la gente es crear una comunidad alrededor de esta este algoritmo o esta red entonces por lo tanto la gente friki como como como puede ser cualquier programador y muy interesado en en la inteligencia artificial y que sepa de esto pues coge ese modelo y por ejemplo ya que lo puede utilizar gratis desde donde yo quiera como yo quiera pues lo que voy a hacer es por ejemplo coger el todas las hacer como una especie de red social como es léxica es una red social de gente que le gusta mucho el hacer ingeniería de texto para ver qué tipo de imágenes se puede llegar a generar y en esta red social digamos que la gente sube textos y las imágenes que han llegado a generar con ese texto entonces la cosa es que yo puedo ir ahí y coger una imagen que me gusta muchísimo y decir en plan de a ver qué texto han utilizado para esto y me lo quedo pero esta imagen también me gusta entonces por ejemplo poco que haya ese texto y combinarlo de alguna forma con el que tengo llegar a uno nuevo genera una cosa súper chula y también subirla a esta red social para que otra persona lo pueda usar o también yo personalmente lo he usado para si tengo muy claro el estilo que quiero para una generación de una imagen pero pero por ejemplo no sé muy bien qué ponerle o no sé exactamente a qué artista se puede atribuir este tipo de estilo o no conozco exactamente qué ingeniería hay que aplicar para para escribir el texto exacto para la que salga algo que yo quiera algo muy decente o de muy buena calidad entonces pues me voy a esta red social y como el que pues se mete en instagram estás un rato dando scrolleando buscando exactamente qué es lo que es lo que más se parece a lo que tú quieres generar y digamos que también ayuda mucho pues a inspiración a coger ideas y todo eso luego está dream studio que creo que es la más famosa de todas que es la de la que nos permite utilizar este modelo de forma online y no sólo es la generación de imágenes a partir de texto pero también te permite hacer la edición de imagen dream studio si no recuerdo mal tiene una aplicación para poder utilizar el modelo de forma local es muy recomendable esto usarlo si se tiene una gpu muy potente sino pues también pues evidentemente no o no se va a poder generar imágenes o va a tardar muchísimo tiempo en generar cualquier imagen pero digamos que lo bueno de utilizarlo de utilizarlo en local es que no tenemos el filtro que sí que puede tener dream estudio en su en su en su forma online de esto creo que la aplicación para ser descargada si se quiere usar en local creo que se llama art room y creo que dream estudio no es la compañía es la página web pero la compañía si no me acuerdo se llama stability ai y eso entonces digamos que si se quiere utilizar online sí porque por lo que sea no me apetece descargarme un modelo y usarlo en local porque ya sea porque no tengo tiempo porque no tengo un ordenador suficientemente potente pues utiliza en su modalidad online con dream studio y si por lo que sea sí que me interesa el quitarme el filtro y el que no tenga ningún tipo de límite ni el tener que depender de la conexión de internet nada de eso pues art room nos permite el descargar el modelo y utilizarlo sin ningún de esa forma plant manía por tercera una tercera creación que he visto por ahí que es que plant manía permite crear textos para generar buenas imágenes digamos que tú le escribes un texto como lo podría escribir yo o otra persona que no sabe cómo escribir buenos textos para para para este tipo de de modelos pues yo que sé yo quiero una imagen de una fotografía de un gato entonces digamos que una vez que le das esta esta idea te escribe un texto muchísimo mejor y mucho más capacitado para o que permite a una vez que luego pones en stable difusión el que la imagen que genere tenga muchísima más calidad que si no le dices solo una cosa en general porque una cosa que vemos en estos modelos sobre todo en este tipo en este tipo el difusión es que cuanta más información le des es mucho mejor en cambio es muy poca información o algo muy general las imágenes que genera son de peor calidad entonces por tanto es aplicaciones como esta son muy útiles porque porque saben exactamente qué tipo de qué tipo de texto escribirle de qué en qué formato en qué orden porque son cosas que afectan bastante a la generación final de las imágenes y bueno esta herramienta habrá más seguramente yo sí que conozco alguna más pero bueno esta es la que la que la más famosa y una que yo he usado bastante y he tenido muy buenos resultados y digamos que yo la recomiendo y es una idea al final y por último bueno photoshop sí que ha habido ya implementaciones o ideas o conceptos no no de la no no sé si del propio la prima compañía de adobe de photoshop pero pero para implementar este tipo de herramientas en photoshop que sería una pues evidentemente los que hagáis algo de ya sea arte digital o edición de fotografía de alguna forma pues el poder por ejemplo quitar cualquier cosa de la imagen utilizando este tipo de modelos o con completar una imagen si falta un trozo o por ejemplo simplemente decirle a esta persona quiero que tenga el pelo rubio ahora o esta persona quiero que tenga un traje en vez de camiseta o lo que sea y todo eso lo genere de forma automática en segundos y que quede tan realista como si hubiera sido una foto de verdad pues es algo muy interesante y que pues está a punto de venir y también un poco gracias a y aquí para finalizar esta diapositiva explicar un poco el hecho de que estas ideas han sido gracias a que pues stable diffusion ha sido lanzado al público sin ningún tipo de restricción y claro al final mi journey o Dall-E no tienen este tipo de de ventajas ya que no han lanzado el código y claro el resto de personas que queremos crear algo con esas con esos modelos pues no nos estamos no se nos permite ya que este código cerrado pero en cambio los de código abierto pues sí que al final crea una comunidad explotan evidentemente es mucho más famoso ha convertido a este modelo en algo muy famoso y muy utilizado que la gente ha ido mejorando y eso y al final es en este caso es la comunidad lo que ha hecho el modelo este muy grande y por último simplemente por comentar como curiosidad que digamos que por el auge este de generación de imagen a partir de texto también ha habido otras otras ramas por así decirlo que surgen de esta de esta nueva línea investigación o de esta línea muy avanzada de investigación que que generan otro tipo de datos sintéticos a partir de texto o sea se ha visto un poco también a partir de imágenes pues digamos que también fuera del ámbito del arte y de la creación artística pues que también ha habido pues por ejemplo generación de imágenes de rayos x para aumentar data sets de reconocimiento de tumores o de otro tipo de problemas médicos también se ha utilizado en otro tipo de generación de imágenes pero que también se puede generar otro tipo de datos con metodologías muy similares a las que hemos visto en con imagen pero por ejemplo en audio en vídeo en imágenes sensoriales de tridimensionales de un líder y todos en este caso sólo vamos a verlas porque todo también más interesantes más curiosas pero también más más famosas en audio meta en la antigua facebook creo un modelo hace muy poco hace creo que un mes que se llama audio gen que como su nombre indica es un modelo que genera audio a partir de texto esto está muy bien porque porque claro imaginabas que si por ejemplo yo soy un productor de sonido y yo quiero exactamente quiero meter un sample en alguno de cualquier audio que yo esté creando y pues va a haber una página web en un futuro no muy lejano en el que yo puedo quiero escribir yo que sé una persona sonidos de tacones de alguien caminando sobre parque y yo que sé y un bebé llorando de fondo y que te genera un audio de una longitud que tú quieras sobre eso pero no sólo eso sino que también este tipo de modelos no sé si no recuerdo si era este modelo en particular pero que le podemos dar los primeros tres segundos de audio de una canción y te la continúa hasta el infinito lo cual es una pasada y evidentemente pues a nivel de composición ya no sólo es capaz de hacer lo mismo que hemos visto con imágenes que es prolongar una imagen hasta el infinito y pues además también con audio y evidentemente una combinación de ambas decir en plan vale pues simplemente contexto decirle quiero que me genere es una canción de cuatro minutos y medio que sea folk mezclado con electrónica y que él es en el estribillo entre un piano y que pues en un futuro esto no sé si existe ahora creo que no pero vamos no es nada no es ninguna tontería y es algo que vamos a ver si no el año que viene en el siguiente pero me extrañaría mucho que no lo veamos ya a principios del año que viene bueno por contexto esto se está grabando en en diciembre del 2022 entre otros en la gente que lo esté escuchando en el futuro pues ya me dirá si tengo razón y por último vídeo como también como curiosidad google creo fenaki que es un modelo de generación de vídeo a partir del texto lo cual ya es muy fuerte porque al principio crearon uno que podía generar unos gifts de una duración determinada tú lo escribías por ejemplo pues que sea un oso panda volando en el cielo y te creaba un gift que de unos creo que eran unos tres segundos o cinco segundos de eso pero luego crearon uno que se llama que es este el fenaki que lo que podía seguir un guión de que el texto fuera cambiando y digamos que el vídeo iba cambiando de y siguiendo ese guión de forma adaptativa todos los frames tenían sentido entre ellos y bueno yo quería entrar para enseñaros luego aquí luego podéis estar obviamente toqueteando todo lo que queráis pero un poco por encima pues podéis ver que el texto va cambiando en este ejemplo de aquí por ejemplo te dice un oso de panda fotorrealista que está para un oso panda un osito de peluche que está nadando en el océano en san francisco dice que luego se va debajo del agua luego que ve peces de colores y de repente que es un panda el que está debajo del agua entonces podemos ver cómo está por encima se sumerge aparece un pececillo por ahí y de repente se convierte en un panda pero podéis ver que la transición es perfecta o sea es extremadamente bueno lo mismo con otros como puede seguir en plan de que primero esté buceando en el océano que luego pase a la tierra a andar por la playa y luego que aparezca por último un campfire una hoguera mientras que la cámara se aleja de él hace un plano alejado y cómo sigue exactamente el guión y con esto también han por ejemplo han creado un vídeo de dos minutos que es el vídeo más largo de la historia y siguiendo todo este guión de aquí que podría ser por ejemplo la introducción a una película las imágenes como podéis ver no son extremadamente buenas así que se ven muchos efectos por ahí pero bueno yendo un poco a la diapositiva de las primeras que hemos visto ya habéis visto que Dall-E desde la primera versión hasta hasta la actual tardó un año menos de un año entonces por lo tanto como pues no tampoco me parece una locura pensar que todo esto dentro de menos de un año podremos darle el guión de una película y que pueda generar una película entera en hd o incluso en 2k en el que pues eso digamos que sólo falte poner la voz de los actores o la música y ya está esto es un poco las cosas curiosas y ahora sí si queréis nos vemos en él en el collab y ejecutamos unas cuantas ejemplos para poder ver las imágenes que podemos llegar a generar utilizando este google diffusion hola de nuevo aquí estamos en el collab aseguraos cuando entréis en el google collab que cogéis una una una sesión con gpu vale yo ya me he conectado de antemano lo de la gpu es importante ya que no es limitante pero sí que nos va a dejar nos va a permitir el el digamos el generar imágenes más rápidas y de mejor calidad que siempre vende utilizando la cpu una vez que nos instalemos las librerías vamos a loguearnos con nuestra cuenta de hugging face vale esto es importante porque vamos a utilizar la librería de stable diffusion de diffuser que es una que va a cargar un modelo de hugging face entonces por lo tanto vamos a tener que tener acceso a la API de hugging face que es súper fácil simplemente os tenéis que loguear ahora veremos cómo hacerlo vale simplemente vamos a ejecutar de momento las celdas de en orden y aquí nos va a pedir que escribamos el token vale cuando va a ser muy fácil para crear el token yo lo voy a tener aquí yo tengo mi cuenta de hugging face hecha simplemente clicando una vez que tengáis la cuenta hecha clicando en el link este que se aparece os va a salir directamente la la clave de la del access token yo simplemente con copiarlo lo tenéis aquí le dais a login y dicen que login sucesful vale y una vez creado el login sucesful vamos a cargar el modelo directamente desde hugging face aquí muy importante ver o una curiosidad es ver que estamos utilizando el modelo de fp16 o de float 16 normalmente el modelo es de float, float es un tipo de variable en python que representa números con decimales y en este caso pues utilizamos una precisión de 16 bits lo normal es utilizar el modelo de digamos que el modelo de base es de 32 bits pero no para quitarle un poquito de bueno quitarle bastante carga de computación a google quitando muy poca un poquito de precisión a la generación de imágenes pues nos va a permitir generar imágenes muy buenas con requisitos de software un poco más ligeros vamos a cargar la el modelo va a tardar un ratillo mientras que se cargan todos los pesos de la nube son 16 archivos vale un poco por hablaros de estos archivos pues es tanto el modelo de reconocimiento de texto el modelo de generación de imágenes y luego también hay un modelo que que es de detección de imágenes que no sean not safe for work de NSA for double esto lo que nos va a detectar son pues lo que hablábamos antes de cuando utilicemos stable diffusion a partir de otra compañía en este caso hagan face pues están obligados también a tener un filtro de digamos de cosas que generaría la gente si no le dieras ningún tipo de filtro y en este caso es un modelo que detecta si en algún caso generamos una imagen que tenga algún tipo de contenido explícito de cualquier cual de cualquier forma pues pues nos va nos va a sacar una imagen en vale disculpad que había habido un problema con la versión de la librería de difusers ya está solucionado vosotros vais a tener el repositorio perdón el collab como como toca yo tenía un pequeño fallo probad si si no si por lo que sea no os funcionara os pasa el mismo error que a mí simplemente ejecutando esta línea aquí lo que va a hacer es instalar una nueva versión de estas librerías entre es por tanto en principio el error a mí me ha desaparecido al menos lo dicho hemos cargado el modelo a partir de la librería de difusas que se conecta a hagan face introduciendo nuestras credenciales de hagan face y nos hemos descargado ya la pipeline que contiene una serie de modelos ya en el orden que toca entonces lo único que vamos a hacer es meter la pipeline en la gpu para que como hemos dicho tarde menos tiempo en realizar estos cálculos entonces por tanto esto simplemente diciéndole que la pipeline entera la pasea acuda vale que es un tipo de la gpu vamos entonces por lo tanto ahora simplemente escribiendo en un texto en el que queramos en este caso el clásico de este tipo de modelos es escribir este para empezar y simplemente lo vamos a llamar a la pipeline para que dado el texto y un número de pasos la altura y el ancho de la imagen y un número de aquí vale ahora vamos a ver lo que lo que significa que te saque la primera imagen y luego la mostraremos por imagen por perdón por pantalla lo que hace esto de aquí el número de steps es el número de cantidad de digamos de pasos que se hace para quitarle el ruido cuanto más pasos mejor calidad pero también va a tardar mucho más tiempo aunque ponerle más de ciento y pico es demasiado 50 es la verdad es que genera imágenes de muy buena calidad y tarda bastante poco entonces por tanto eso luego el tamaño 512 por 512 es un buen tamaño para una imagen y luego el guidance scale es la digamos cuánto quieres que se parezca la imagen al texto que tú le has dado si es muy alto por ejemplo 14 o 15 te va a generar imágenes muy raras si es si se pone alto tipo 12 va a tener menos capacidad de imaginación por así decirlo y evidentemente también va a pasar lo mismo por el otro lado pero al revés y digamos que 7 con 5 es la verdad es un buen número de decirle escríbeme lo que te he dicho pero también con un pelín de decir en plan de también imagínate el resto de cosas si le damos por ejemplo para esta imagen vamos a ver que va a tardar 50 que es el número de pasos que le hemos dicho va bastante rápido dentro de la generación como ya hemos visto como tiene que quitarle el ruido de forma paulatina es lo que tiene que tiene que ir poco a poco en este caso bueno bueno aparece el astronauta y el caballo pero no ha sido una buena vamos a intentarlo otra vez así genera una imagen de con un poquito más de sentido de esto como como hemos dicho cada vez que lo ejecutamos lo que hace es coger ruido aleatorio pues cada generación va a ser completamente diferente está es mucho mejor y luego a partir de ahora pues si se quiere por ejemplo decir en el estilo de moned vamos a ver si consigue por ejemplo coger el estilo de moned bueno es mucho estilo de moned pero bueno seguramente si le damos otra vez a esto generamos un texto mejor es capaz de entenderlo esto también lo que hemos comentado también hay un buen trabajo de saber exactamente qué tipo de texto preguntar esta imagen bueno es un poquito van gogh pero bueno no está mal pero eso que es muy importante saber también cómo escribirle este texto y para que genere exactamente lo que queramos pero bueno esto simplemente es una pequeña práctica un pequeña prueba una explicación de cómo se puede utilizar de forma gratuita completamente gratuita este tipo de modelos y a partir de ahí hacer cualquier proyecto que os apetezca cualquier proyecto la final la imaginación también en este caso ahora sí que no hay ninguna excusa que la imaginación es el límite ya que esto pues más fácil de implementar y más chulo no no puede ser bueno muchas gracias por asistir a esta sesión y nos vemos en otra.


### 504.E1_Estado_del_Arte-video

### titulo
![[504.E1_Estado_del_Arte.mp4]]
[Estado del Arte](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla)
[Estado del Arte (PDF)](505.E1_DeepLearningStateoftheArt-230120-153018.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1HBSP9yyfvaOqG3FebpSP7VrSFEuR7kUv?usp=sharing)

Bienvenidos a la sesión de State of the Art. En esta sesión lo que veremos es un poco por encima cuáles son las técnicas de State of the Art de las redes neuronales de hoy en día. Algunas de ellas tienen unas pocas semanas de antigüedad a la hora que se está grabando esta sesión, que es diciembre de 2022, y otras son muy conocidas y si vosotros habéis hecho algún tipo de tratamiento de imágenes con redes neuronales pues seguro que sonarán. Entonces partiremos de esas a algunas más novedosas que a lo mejor no suenan tanto. Iremos un poco viéndolo por orden de precisión. Empezaremos con las más antiguas y las que menos precisión tienen hoy en día el primero y luego pues iremos subiendo en esa escala. Notar que muchas de ellas pues tienen más precisión que la anterior a lo mejor en unas pocas décimas, no por ello son una revolución por así decirlo, pero sí que es verdad que la arquitectura cambia lo suficiente como para considerarlas una red completamente diferente o una arquitectura muy diferente a la anterior. Y por último saltaremos al Google Colab donde tengo preparadas algunas prácticas para poder ejecutar estos modelos de forma fácil utilizando la librería de PyTorch. La verdad es que facilita bastante el uso de estos modelos, pre-entrenados incluso, entonces simplemente cargaremos los modelos y ejecutaremos unas cuantas imágenes de internet para ver su precisión, para un poco comprobar a ver cuál puede funcionar mejor, cuál es más robusto y todo eso y que luego pues si lo vais a usar, vais a usar alguno de estos modelos internamente, bueno por vuestra parte digamos que ya lo tengáis más claro de cómo usarlo y que ésta os sirva también de referencia. Lo primero que vamos a necesitar para comparar cualquier tipo de cosa, vamos a necesitar digamos una base, en este caso la base es una dataset que se llama ImageNet, a lo mejor os suena, es muy popular, es la base más popular hoy en día, contiene más de 14 millones de imágenes que han sido anotadas a mano, digamos que ha habido personas anotando durante muchas horas esas imágenes y algunas de ellas contienen la clase que contiene la imagen y otras pues también contiene digamos la posición que contiene esa clase, o sea ya no solo es que aquí hay por ejemplo una persona sino que coordenadas de la imagen ocupa esa persona. En cuanto a categorías hay más de 20.000, las categorías son muy muy diversas tanto vehículos, animales, razas dentro de esos animales, comida, objetos cotidianos, de todo, o sea son 20.000 categorías al final. Digamos que esta dataset lo más normal es que los modelos que digamos que si un equipo de científicos quieren sacar un nuevo modelo o han sacado un nuevo modelo y quieren medir la precisión o cómo se compara con el resto pues entonces lo que van a hacer es coger mil imágenes, o sea las mil imágenes de mil categorías de esta dataset en vez de las 20.000 y medir cómo la precisión que tiene su modelo en esas mil imágenes y digamos que hay un benchmark, una tabla donde cada modelo tiene su precisión entonces pues es más fácil compararlos entre ellos. Aquí podemos ver una de estas tablas donde aquí pone la precisión en la tabla de la derecha del todo. En este caso como podemos ver lo hace con mil clases. Esta es la más reciente que he encontrado, no están todos los modelos que vamos a ver aquí desgraciadamente pero bueno están hechos de la mayoría y hay otros que no están. Un poco de izquierda a derecha podemos ver que normalmente se puede ver el modelo del modelo que se está tratando, el nombre del modelo. Normalmente cada tipo de modelo tiene diferentes variaciones de más pequeño a más grande en este caso y el número de parámetros en millones de parámetros y luego los flops es una unidad de computador digamos que que necesitas de cómputo para ejecutar esa red neuronal en gigaflops en este caso y luego pues las imágenes por segundo que saldría y luego por último el top 1 que significa que efectivamente la clase que sale con más precisión de tu modelo es la clase en la que está etiquetada esa imagen entonces por lo tanto pues digamos que eso sería un acierto y el número de aciertos el porcentaje de aciertos sale reflejado en la tabla de la izquierda de la derecha un poco toda esta tabla se divide en dos tipos de arquitecturas principalmente que son convolucionales y vision transformers en el caso es verdad que todas estas están se aplican bastante a visión pero digamos que la arquitectura transformers pues también es ahora mismo lo que más se aplica en temas de texto reconocimiento de texto pero también y las convolucionales también se aplican a temas de audio o otro tipo de datos y simplemente pues digamos que el tratarlo con imágenes es una forma de universalizarlo todo pero no está limitado solo a imágenes son muy versátiles la mayoría de estas arquitecturas en la tabla de aquí podemos ver como como él a lo largo de los años ha ido aumentando esta esta precisión en esta dataset efectivamente pues como se puede entender cuanto más avanza la investigación de estos campos y cuanto más avanza la la la capacidad de cómputo de los ordenadores y son más universalmente digamos que compras un ordenador potente es más universalizado que nunca pues también va aumentando la precisión de los modelos digamos que es directamente proporcional de momento y hoy en día pues nos situamos pues ahora mismo el último modelo que ha salido a día de que se está grabando esto es el focal net que lo veremos el último de todos de una precisión que ronda el 83,5 33 84 casi vale que sería el street of the art hoy en día vamos a ver para empezar como hemos visto en el índice una red neuronal que en su día fue una revolución que se llama resnet la resnet tiene ese nombre ya que net efectivamente pues evidentemente desde network y res es el tipo de conexiones especiales que va a que mete esta esta red que se llaman residuales las conexiones residuales digamos que si en una red neuronal normal que como ya sabéis esta forma por capas conectadas una detrás de otra pues seguiría un flujo que seguiría estas estas flechas de color un poco más oscuro y luego la residual lo que hace saltarse conexiones esto hace que la información fluya más fácil a través de la de las capas y mitiga muchos problemas que tiene una red neuronal como por ejemplo convertirla en que sea más fácil de entrenar que la puedas aumentar mucho de tamaño sin que sin que haya problemas en el entrenamiento digamos que la hace mucho más robusta en general y efectivamente una vez que se introdujo pues fue una revolución ya que aumentó mucho la precisión de las del reconocimiento de imágenes está aquí que se refleja en la imagen es resnet 50 que también la veremos luego en el laboratorio bueno la práctica perdón en la que pues cada dos capas hay una una conexión residual aclarar que las conexiones residuales una vez que se conectan digamos que cuando incidiendo se puede tanto sumar las activaciones como concatenar las activaciones lo más normal es que se sumen las activaciones la resnet 50 en la tabla que hemos visto antes ocuparía perdón la resnet en general ocuparía esta esta parte de la de la grafía que hemos visto digamos que es de las primeras porque siguen usándose bastante si son las más antiguas que hay en esta en esta tabla y tiene una precisión alrededor del 80% en imagenet vamos a pasar ahora buen salto en cuanto a tanto a arquitectura como de histórico a los vision transformers que fueron creados después de ver la precisión fueron creados inicialmente los transformers fueron creados para texto para procesamiento de texto y vista la precisión que tienen con el texto lo intentaron pasar a imagen y vieron muy buenos resultados y efectivamente pues se creó una un vision transformer que es la versión de imagen del transformer pero a partir de aquí a partir de esta parte de aquí del modelo es exactamente igual que los transformers de que se aplican a texto entonces digamos que aquí es donde un poco se da la idea de que tal y como funciona nuestro cerebro pues la información se trata de forma muy similar aunque venga de diferentes dominios o digamos que si tu cerebro es suficiente o digamos la capacidad que tengas de cómputo es suficientemente grande digamos que tiene una fuerte inteligencia como quieran como queréis llamarlo sería capaz de adaptarse a cualquier tipo de datos que te entran mientras que sean representativos de algún tipo de información en este caso en digamos que en procesamiento de texto era un poco más directo separarlas por ejemplo una frase por palabras aunque en verdad se separan por unas entidades más más básicas que palabras llamadas tokens pero bueno se separa por palabras es muy fácil digamos el pasarlo a partes pero en imagen no digamos que no había ninguna forma de hacerlo hasta que se les ocurrió en el paper de vision transformer separarlo por parches de 16 por 16 píxeles de esta forma tratar digamos la imagen como diferentes palabras que forman una frase entera que sería la imagen entera si lo queréis entender por esa parte y eso reduce bastante el nivel a la computación de por ejemplo pasarle un píxel como un token de la imagen que se ha intentado y efectivamente era incomputable sobre todo porque el transformer y ahora vamos un poco a lo que significa el transformer es una arquitectura que lo que hace es por cada parche que se extrae digamos que intenta prestar atención al resto de parches intenta ver cómo se parece o qué relación tiene con qué parche entonces digamos que se crea una atención global donde cada parte de la imagen un poco presta atención a la parte que interesa de la imagen entonces esto crea un mapa muy global de las features y digamos que pues es mucho más potente por así decirlo que las convolucionales que en principio pues irían por partes de la imagen entonces el obtener ese mapa global o no se llega a hacer o se hacen etapas muy muy finales de la red neuronal digamos que también por poner un poco los vision transformers comparados con las convolucionales los convolucionales son mucho más fáciles de optimizar pero una vez que optimizas los vision transformers de los cuales pues aparte de que cuesta mucho entrenarlos pues también editas muchos datos pero una vez que cumplas eso sí que es verdad que los vision transformers han demostrado mucha más precisión y mucha más digamos de robustez que las convolucionales también mencionar que los vision transformers y tanto los vision transformers como los convolucionales con las redes convolucionales tiene una digamos una capacidad llamada inductive bias que lo que hace es que cada red tal y como está formada tiene digamos una capacidad intrínseca y está demostrado que pues la por ejemplo los vision transformers prestan más atención a la forma de los objetos y las convolucionales prestan más atención a la textura de esos objetos entonces digamos que también tenerlo en cuenta y sí que es verdad también que las convolucionales tienen más de más vallas de estos que las que los vision transformers entonces por lo tanto los vision transformers por defecto ya son más robustos a las distorsiones de imágenes y a las permutaciones que pueden tener estas los parches de las imágenes y digamos que en verdad tampoco por mucho que haya una universalización de estos digamos de los beneficios de una con respecto de otra al final depende de cada tarea aplicar arquitectura o la otra. Vamos a ver ahora los swing transformers, los swing transformers una vez visto los vision transformers son muy fáciles de entender lo que hacen es digamos que la parte que hemos visto de los vision transformers es separar la imagen en parches de 16 por 16 y digamos que en ningún momento siempre la atención se computa con los digamos entre los parches de la imagen de forma global en el swing transformer por otro lado de esos parches se vuelve a dividir en otros pequeños parches entonces digamos que la atención primero se computa a nivel local dentro de estos parches y luego estos parches se van acumulando y luego se va calculando a nivel global esto hace que la arquitectura sea más densa que la atención sea más fina entonces por tanto se atiende a parches mucho más pequeños que anteriormente con una arquitectura de vision transformer que acabamos de ver pero no sólo no sólo eso sino que además tiene otra complejidad computacional diferente la de vision transformer que la hace más eficiente. Gracias a esto se utiliza tanto en clasificación de imagen pero también se utiliza en detección de objetos que sería un poco dibujar una cajita alrededor de los objetos que diferentes objetos que forman una imagen y también se utiliza para segmentación que sería ya a nivel de píxel decir a qué clase forma cada píxel entonces pues digamos de una imagen en la que te aparece una persona y un coche saber qué píxeles forman parte de la persona y qué píxeles forman del coche y esto se ve muy fácilmente si habéis hecho alguna llamada vídeo llamada últimamente pues lo que te segmenta a la persona del fondo de pantalla utilizaría este la nuestra arquitectura necesariamente pero si la tarea de segmentación en la que sabe qué píxeles forman parte de la persona ya no sólo de la piel sino el pelo o la ropa y el resto digamos que lo puede sabe que no es no forma parte de la persona entonces luego lo puede cambiar a otra foto en dentro de la tabla que hemos visto anteriormente destacar el papel la parte que donde están estos transformers los transformers vemos que tenemos el tiny el small y el base vale y como hemos visto pues cada uno de estos tiene incremento el doble de parámetros que el anterior casi y entonces por tanto su complejidad de ejecución aumenta mucho la velocidad en la que en la que se procesan las imágenes disminuye bastante y por último también pues efectivamente cuando más grande es la arquitectura por orden un poco más normal y lógico pues también aumenta la precisión general entonces también es verdad que como hemos visto parece poco porque al final en resnet es una arquitectura más simple entre comillas pero tiene un par de puntos sólo por debajo pero ese par de puntos la verdad es que significa mucho en digamos en estas en estas tareas entonces por lo tanto pues aunque parezca poco a nivel de números es un buen salto luego vamos a ver las resnext que su nombre evidentemente se aplica a las resnet que hemos visto anteriormente pero digamos que una nueva generación en este caso es una digamos una arquitectura un poco curiosa porque no cambia nada de la ninguna capa en general de las de las convolucionarias que hemos visto antes son siguen siendo redes convolucionales lo único que hace diferente es un poco hacer unos stacks de bloques que tengan misma topología entonces por tanto digamos misma topología como como hiperparámetros de digamos que los filtros que se aplican y todo eso son compartidos entonces por tanto de una capa residual que de las que hemos visto anteriormente que pueden ser que pueden ser esto de aquí vale donde simplemente pasamos la rendering al normal pero luego tenemos una conexión que se salta a todas las conexiones y se suma a lo que se procesa por otra parte pero en este caso digamos que se divide mucho más incluso en 32 pads como podemos ver con misma topología y luego se suman cada uno de ellos y luego además tenemos otra conexión residual que se suma a todos esto pues digamos que la presión que puede llegar a tener lo hace más robusto y como podemos ver en esta tabla que es diferente a las que hemos visto anteriormente porque efectivamente no estaba en las que en las que hemos visto también es verdad es una arquitectura muy bastante reciente a la hora que se está procesando esta clase y podemos ver que tiene unos parámetros que no son muchos la verdad y unos flops que tampoco son demasiados pero sin embargo tiene una precisión muy alta en este caso según esta tabla no sería mejor que la swing transformer pero en general está demostrado ya en algunas otras que sí que sí que supera los swing transformers pero no consigo encontrar estas tablas seguramente en el paper de la resnext si queréis buscarlos estará reflejado la resnext además es mucho más robusta que el swing transformer en principio según explican en su paper y obviamente pues supone también un buen salto desde la resnext anterior sin cambiar la arquitectura y la verdad es que también facilita mucho a la gente que ya hemos trabajado con mucho más con convolucionales que con los transformers que tienen menos tiempo de antigüedad pues también nos facilita mucho el salto a decir que a lo mejor no hace falta cambiarse a transformers y simplemente utilizando las capas que conocemos de toda la vida pues podemos digamos el superar o hacer tener resultados similares a los transformers por último y más por una curiosidad que por otra que digamos como algo que vayamos a explicar muy en profundidad aunque tampoco es muy complicado sería explicar las focal nets digo como curiosidad porque a la hora que se está grabando esta clase que recuerdo que es diciembre del 2022 pues tiene unas pocas semanas de antigüedad es una red desarrollada por microsoft digamos que lo que hace es cambiar el mecanismo de atención de las de los transformers por uno basado en convolucionales digamos que ya estáis viendo que hay una pequeña guerra entre transformers y convolucionales donde las transformers pues fácilmente superan a las convolucionales pero luego a lo mejor si se aplican esas mismas técnicas a las convolucionales pues vemos que vuelven a resurgir las convolucionales al final no hay mucha diferencia entre ellas y menos y si un poco se copia en la arquitectura simplemente pues cambia algunos detalles de cómo se perciben los datos de las imágenes y ya está en este caso self attention sin entrar mucho en detalle lo que crean son matrices de key query y value y aplica una atención entre el query y el key utilizan la función de atención que luego se multiplica por el value para crear el output en este caso lo que hacen es digamos una arquitectura un poco similar si podéis ver un poco una similitud entre los gráficos pero digamos que la la agregación de contexto se hace mediante unas redes convolucionales que se hacen que cada vez van cogiendo un kernel más más grande y esto aunque parezca complicado de entender son unas pocas líneas de código para hacer esta convolucional no es para nada algo complicado y luego pues también se concatena esto con el query para sacar el output como se hace un poco similarmente en la en el self attention esto simplemente para para comprobar esto pues evidentemente la tabla está la he sacado del propio paper donde se puede ver que focal net está resaltado en negrita tiene diferentes arquitecturas como hemos visto para saber cuáles son más potentes simplemente fijaos en el número de parámetros 28 millones de parámetros es la más pequeña y llega hasta 88 millones de parámetros casi 89 millones de parámetros sería la más grande y efectivamente pues también la más grande es la que más precisión tiene que hoy en día es el state of the art en clasificación de imágenes reconocimiento de imágenes pero como ya hemos comentado el ser capaz de reconocer imágenes también explica un poco y digamos que también da índice de que esta red también se va a poder clasificar otro tipo de o digamos adaptarse a otro tipo de datos también resulta que la focal net igual que igual que el swing transformer tiene ese state of the art en segmentación de píxeles ya no sólo en clasificación de una imagen entera así que si queréis un poco resumen de lo que hemos visto ahora mismo el focal net es la red más usada si bien es cierto que no hay ninguna la implementación está en github la podéis buscar súper fácil buscando por el nombre de focal net está hecha por microsoft y es de código abierto y es pues como ya comentaba el modelo es muy fácil de entender porque son unas pocas líneas pero no la veremos en el laboratorio porque en la práctica porque porque es tan reciente que no tiene una implementación fácil digamos como las que yo estaba buscando en esta práctica y el state of the art hoy en día también como otra conclusión es del 83,9 en image net 1k como ya hemos comentado vamos a ir ahora a pasar al collab donde veremos cómo cargar unas cuantas imágenes utilizando la librería pillow para cargar las imágenes como un objeto de python y utilizaremos la librería de torch de pytorch una librería que tiene internamente que se llama torch vision que está dedicado para temas de pues como indicas un nombre de imágenes de visión y usaremos los modelos que han sido entrenados con image net 1k eso significa que vamos a simplemente cargando los modelos pesos nos lo descargará automáticamente y en una en una línea solo no vamos a tener que programar nada y automáticamente le podemos dar una imagen y si en esa imagen hay alguno de las mil clases de las que de las que hemos comentado pues o sea de las que está entrenado pues nos pondrá la salida junto con la precisión que tiene o la confianza mejor dicho que cree que tiene esa clase vale pues muchas gracias de momento por la teoría la teórica está y ahora nos vemos en la en la práctica hola otra vez estamos ahora en él en el collab donde vamos a ejecutar lo que hemos comentado una vez más vamos a seguirlo de arriba abajo vale no haber que saltar ninguna celda están todas por orden y lo vamos a intentar ejecutar todos juntos y así os voy explicando un poco lo que hace cada línea por si acaso no estaba bien comentado si no se entiende vamos a importar los modelos automáticamente el google collab por defecto viene con la librería torch vision instalada eso es por lo tanto simplemente del torch vision vamos a importar los modelos y vamos a imprimir por pantalla qué modelos vienen incluidos con con esta librería vale vemos que no son pocos estos son modelos que vienen de código abierto podéis acceder al código en la página oficial en el repositorio de de pytorch y digamos que parten están ordenados alfabéticamente en este caso al parecer entonces no nos fíéis tampoco de esta de este orden de mayor a menor precisión y en de forma histórica sí que es verdad que alex net en este caso creo que es la más antigua que hay aquí o dense net pero aquí vemos las con next que las hemos visto ahora hace poco algunas más conocidas como por ejemplo la resnet que también las hemos visto swing transformer vision transformer aquí están vision transformers pero bueno también hay muchísimas más que no he mencionado porque evidentemente pues estaríamos un curso entero para verlas todas pero pero están aquí para probar todas ellas y todas ellas están pre entrenadas vale vamos a limpiar este output en este caso he decidido simplemente coger cuatro clases una la dos un poco básicas en este caso de un gato y de un perro podemos ver si queréis el link a las imágenes este gato de aquí y este perro de aquí vale muy bien bastante de digamos modelos de sus clases pero también he cogido si no me equivoco esto era un esto es una mangosta he tenido que buscar porque no me acuerdaba una mangosta que es bueno yo no la verdad es que la conocía pero no me sonaba mucho pero bueno como una clase un poco más rara y luego un fagot en este caso y estas son clases efectivamente que están dentro del 1k este las clases que hemos visto entonces podéis ver que pues cada una es muy diferente el anterior entonces son animales muy conocidos un animal un poco menos conocido y un objeto la verdad es que poco conocido entonces nos vamos a descargar para poder bueno primero nos descargamos estas imágenes vale el hecho de descargarlas es para que no os tengáis que conectar a ningún a ninguna base de datos ni a ningún drive ni nada de eso simplemente la descargamos automáticamente nos lo dejará en nuestro file system de la ejecución de este colab vale utilizando la función web le podemos decir un poco el nombre que queramos y simplemente el link a la imagen con éxito nos lo han descargado todas y también nos vamos a descargar esto de aquí esto aquí es un documento que contiene las mil imágenes a la clase el nombre de las mil clases que hemos visto vale pues eso podemos ver que tiene muchas clases muy diferentes de ellas yo la verdad es que no sé lo que son pero bueno alguna vez así que son un poco más raras pero sí que no nos suenan pero nosotros digamos que tampoco es que sean todas una de animales también hay bueno ahora tampoco me voy a poner a buscarlas pero digamos que también hay muchos objetos dentro de las dentro de las de estas clases vale ya las podéis ver vosotros si queréis pero no son muy variadas al final son mil clases y claro hay de todo unidad por ejemplo aquí hay un canon un cañón que sería una un objeto o una rueda de coche todo esto es capaz de reconocerlo entonces simplemente porque las cargamos porque al final el modelo lo que nos va a devolver simplemente es un índice por ejemplo 8 entonces claro si el índice pues nos tenemos que venir a para poderlo entender nosotros nos tendremos que venir a esta lista de aquí y buscar en plan 1 2 3 4 5 6 7 y 8 y ver que es un gallo entonces digamos que que si cargamos todo el nombre de estas clases es mucho más fácil luego evidentemente pues el el mirar a ver si ha sido correcta la identificación o no esto tampoco sirve lo que vamos a hacer no sirve tampoco para evaluar estos modelos porque lo único que hemos hecho es cargar cuatro clases y ya está pues tampoco es una buena evaluación de un modelo simplemente con cuatro clases pero simplemente que sea como demostración de cómo utilizar estos modelos como cargar imágenes en estos modelos y cargar los pesos pre-entrenados utilizando esta librería y ver que es extremadamente fácil bueno me he descargado de aquí la la el el archivo que hemos visto antes con las clases y lo que voy a hacer es simplemente abrirlo utilizo la función de open que viene con él con python y simplemente lo que voy a hacer es abrirlo y a la vez que lo abro también lo voy a separar por líneas utilizando la función de readlines esto nos devuelve pues es una lista con las diferentes líneas entonces simplemente nos voy a guardar en una lista que se llama labels entonces por lo tanto el labels va a tener una lista con todas estas imágenes vale de hecho vamos a echarle un ojo vale y vemos que todas las clases que hemos visto anteriormente están aquí las mil clases de hecho mira podemos son muchas clases si vemos el length of labels esto nos va a devolver el tamaño que tiene vale vemos que son mil clases como en otras prácticas que hemos hecho vamos a importar vamos a utilizar la librería de matplotlib para dibujar las imágenes que vayamos viendo entonces me creo yo mi función de show image para mostrar las imágenes en la resolución que yo quiera y entonces vamos a utilizar la librería pillow que le hemos importado anteriormente bueno no hace falta importarla perdón porque viene con bueno está importada aquí de hecho vamos a subirla hacia arriba donde aquí ya vamos a importar la librería pillow en este caso vamos a utilizar dentro de librería la clase de image en torch vision vamos a importar transforms que nos vamos a ver qué significa eso y luego pues pytorch per se en image lo que vamos a hacer en todas va a ser cargarlas a partir de las que nos hemos descargado anteriormente y convertirlas en formato rgb por si acaso y la vamos a guardar simplemente en variables y las mostraremos por pantalla cosa que ya hemos hecho antes pero simplemente para ver que se han cargado todas efectivamente a los que hemos visto antes vamos a limpiar esto aquí es donde vamos a crear el flujo de pre procesamiento de las imágenes esto es porque las imágenes que les vamos a dar o sea que le tenemos que dar los modelos son diferentes o sea tienen un tamaño fijo en este caso el tamaño universal que se utiliza es de 224 por 224 luego las convertiremos en tensores para poderse las dar a pytorch y luego las normalizaremos esto es digamos que son datos de normalización estándar de media y de desviación estándar de la de la librería de imagenet entonces ahora nos vamos a crear una función que sea para clasificar una imagen para simplemente como vamos a clasificar muchas imágenes con muchos modelos pues que no tener que escribir todo esto todo el rato pues nos queremos una función para simplemente llamarle y ya está en esta función le vamos a dar la imagen y le vamos a dar el modelo simplemente para que lo que significa esta función es para digamos que sacarlas las labels de una imagen utilizando un modelo el modelo lo pasamos a modo evaluación esto digamos que internamente pytorch lo que va a hacer es decirle que no vamos a entrenar el modelo simplemente lo vamos a evaluar entonces hacemos algunas modificaciones en las capas necesarias y no se pasa ningún gradiente o digamos que no deja almacenado ningún gradiente preprocesaremos la imagen con la con el flujo que hemos visto anteriormente una y entonces ya la tendremos en tensor con esto la convertiremos en un patch en un batch perdón de una imagen y la pasaremos al modelo el modelo te va a devolver un vector de mil dimensiones vale por cada según las probabilidades de que tiene esa imagen de una parte en cada clase entonces obviamente como queremos la que más probabilidades tenga la clase que más probabilidades tenga entonces cogeremos el índice de la de la clase con más probabilidades y lo que vamos a hacer es convertirlo en porcentaje utilizando la función softmax que tampoco vamos a entrar en detalle lo que hace pero que hace bueno digamos que todas las probabilidades sumen uno entonces por tanto te sacará la te convertirá la que más probabilidades o sea la que más energía tenga a probabilidades y las mostraremos por pantalla según la clase el nombre que tenga dentro de la lista que hemos sacado de nombres de clases junto con el porcentaje que tiene o que ha sacado el modelo de seguridad de que está seguro de que en qué porcentaje de probabilidades una parte de esa clase entonces simplemente vamos a crear esta función y la metemos en memoria y vamos a probar primero con resnet dentro de resnet ya hemos visto que había diferente estaba resnet 50, resnet 101, resnet más grandes y nosotros vamos a probar de momento simplemente con la resnet 101 como un test y ahora es muy importante ver que le estamos dando el flag de pretrain como true, eso significa que va a descargarse los pesos de image net 1k y los va a meter dentro de la red neuronal, también se puede poner como false si por ejemplo queremos entrenar la red nosotros desde cero o una vez entrenada o sea cargada entrenada podemos quitar algunas capas y hacer las nuestras para pues digamos hacer transfer learning todo esto es muy fácil con la librería está de PyTorch, ahora nos va a descargar los pesos del modelo y ya tenemos el modelo cargado en memoria entonces lo que vamos a hacer es utilizar la función que hemos creado antes para imprimir para pasarle las diferentes imágenes el modelo que queremos usar y que nos imprima a ver cada una lo que cree que la primera es gato vale y fijaos ahora que no sólo nos dice que es gato sino que encima te dice en la la raza de gato que es que en este caso es un gato tigre con el con el perro lo mismo te dice que es un golden retriever con cierta porcentaje de seguridad aquí pues efectivamente te dice que es una mangosta creo que se llamaba con un 99 como 87 por ciento y luego pues el basún que es el fagot en inglés que tiene muy mala traducción efectivamente pero también nos lo ha sacado con muy buena probabilidad ahora vamos a ver muy rápidamente el resto de modelos aquí veis que cambia un poco el la forma de escribir aquí era pretrain igual a true aquí es weights igual a true también veis que aquí los los pesos son ocupan 171 megas aquí ocupan 1,14 gigas entonces por lo tanto ya es una demostración de que este modelo va a ser mucho más grande que la resnet y vamos a ver si tiene más precisión pero bueno tampoco como lo he dicho aunque falle en algunas de estas imágenes no es un modelo digamos una medida de la robustez de este de estos modelos aquí por ejemplo saca otra otra raza de perro del gato es lo mismo y los últimos dos digamos que baja un poco la seguridad que tiene de que son esa clase pero digamos que también las categoriza correctamente si queréis luego vosotros con la imagen del perro y la raza que pone aquí podéis buscar en internet a ver cuál es la que creéis que se parece más por último sin transformer bueno por último perdón luego van las estas por penúltimo sin transformer mucho más pequeño aquí también entended que aquí estamos utilizando el vt 32 que es un modelo medio grande y luego el sin transformer tiny que es el más pequeño que aquí las weights ya tienen que ser especificadas que quiere es el image net 1k y versión 1 por lo tanto son 108 megas que es más pequeño incluso que el resnet entonces por lo tanto también vamos a ver que va a tardar muchísimo menos que el resnet tampoco es que aquí estamos midiendo tiempos pero iba a sacar una pues aquí parece que tienen mucho que ver con el visión transformer ha sacado la misma clase con unas probabilidades bastante parecidas siendo una red muy diferente pero bueno eso estamos viendo cómo clasificar estas imágenes la ahora sí por último vamos a cargar el el com next que estaría un poco entre resnet y y visión transformer en cuanto a también a tamaño del modelo y vamos a clasificar estas imágenes parece que están la mayoría están de acuerdo a que es esta raza de perro pero no todas en general han clasificado pues obviamente aunque sea diferentes razas de perro el que es un perro tiene súper seguro lo que quiere jugársela más hacia la raza con el gato no lo ha dudado en ningún momento pero bueno también ha detectado efectivamente que el objeto que existe en esa imagen es un gato y luego pues algunos que son menos comunes también ha sido capaz de entenderlo por último focal net como hemos comentado no está la librería aún metida en la en perdón no está el modelo metido en esta librería pero sí que pues en el repositorio oficial de microsoft está todo el código abierto os lo podéis descargar los pesos pre-entrenados también con imagenet y para diferentes otras tareas incluso nada esto sería un poco la práctica de esta de esta sesión la hemos visto cómo coger una imagen reprocesarla para para cargar para hacerle un paso en un modelo pre-entrenado y sacarlos las salidas de ese modelo convertirlas a porcentajes de seguridad de la red y sacar el porcentaje más alto la clase con porcentaje más alto pues en muy pocas líneas de código.


### 506.E1_El_futuro_del_Machine_y_Deep_Learning-video

### El futuro del Machine y Deep Learning
![[506.E1_El_futuro_del_Machine_y_Deep_Learning.mp4]]
[El futuro del Machine y Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris)

Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no hablar de un concepto que se llama Automachine Learning. Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolla inteligencia artificial. A priori puede parecer una hoja. una amenaza para la humanidad, pero no tiene nada que ver con todo esto. Simplemente, estamos hablando que hay, como dijimos, la razón de ser de este campo de la IA y del Machine Learning, es el eliminar tareas repetitivas de bajo valor. Por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías. De una forma muy rápida, tenemos... plataformas que nos permiten hacer tareas que en otras ocasiones duraban meses. Estamos hablando, debemos ponernos un poco en la piel de un científico de datos, tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. La primera sería con lo que se llaman desarrollo a bajo nivel, estaríamos hablando de código, estaríamos hablando de... estamos hablando de Python, que es el lenguaje oficial, por decirlo de alguna forma, de esta comunidad, aunque están apareciendo otros, como Go, que también pretenden ganar terreno. También tenemos R, que se utiliza más en el ámbito de la bioinformática. En definitiva, Python es el que mayor comunidad tiene. Pero también tenemos otro grupo, que sería de los... y esta es una pregunta muy... común de los estudiantes que es el de los programas de alto nivel. Los programas de alto nivel son aquellos que son user friendly, es decir, son interfaces de usuarios que son amables, que son fáciles de entender. En pocas palabras estamos hablando de picar código versus tener herramientas que son prácticamente drag and drop, que son ya más familiares y que trabajan con el concepto de nodos. ¿Qué herramientas tenemos que usar? en este segundo grupo del drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial del aprendizaje. Tendríamos herramientas como KNIME, como ORANGE, como BICML. ¿Qué os podría comentar de estas herramientas? Pues bueno, básicamente KNIME. está muy bien para el aprendizaje en fases tempranas y para poder prototipar, para poder defenderte, para poder entender de una forma esquemática lo que son los flujos de un algoritmo respecto en la ETL y en la fase de prueba, de ensayo y error con diferentes algoritmos. Y estamos hablando de un concepto de nodos. Obviamente no son programas profesionales, sino que nos sirven de puente para poder llegar a pensar. y aplicar todos los conceptos tanto teóricos como prácticos. Después tendríamos programas como Orange, que es del pack Anaconda, que Anaconda es el grupo donde se ubica Python, donde han hecho una interfaz mucho más amable, mucho más agradecida y también nos permite prototipar muy rápido y de una forma muy intuitiva, nos permite en definitiva acortar tiempos y sobre todo ponernos bien. barreras al aprendizaje. Y BKML sería un concepto que ya se sale de lo que son los nodos y realmente es un concepto más por ventanas. Estas serían las tres herramientas, las dos primeras son gratuitas, BKML es de pago, pero en definitiva este concepto, sobre todo el de nodos, es el que están utilizando ya plataformas como Azure o como IBM. Watson, en este caso azules de Microsoft y VM Watson, que son ya directamente plataformas drag and drop. ¿Qué significa esto que estamos hablando de conectar nodos? Conectar nodos lo que implica es que nos evitamos muchos errores de sintaxis de código, pero lo que implica también es que no tenemos tanta capacidad de edición como tenemos en el código, es decir, al final el código va a ser la tecnología que siempre va a ir más rápido, la más profesional y la más escalable de cara a ponerlo en producción. Pero sí que es cierto que con el nacimiento de Azure, o IBM Watson, se abrió una puerta a trabajar drag and drop que nos puede dar muchísima agilidad a la hora de trabajar. Desde este punto, es cuando empezaron a salir nuevas tecnologías como Data Robot o H2O, o Data IQ, Data Bricks, Rapid Miner, donde se centraron ya otra vez en los nodos, pero yendo un paso más allá, aplicando Automation Learning. Automation Learning, al final, como hemos visto, para que os hagáis una idea muy sencilla, como hemos visto, tenemos que hacer una serie de comprobaciones, de conceptos claves, como los missing values, los outliers, qué correlaciones hay entre diferentes variables, que esto en código lleva bastante tiempo, siempre teniendo en cuenta que tenemos que visualizar los datos y tomar decisiones, y ver qué factores hay que corregir, que es el proceso de TLE. Esto lleva mucho tiempo en código dependiendo el nivel que tengas a la hora de saber escribir código y de depurarlo. Por tanto, aquí surge una solución que es el Automation Learning. Podríamos decir que el líder es DataRobot. a nivel mundial, pero ya existe un grupo bastante nutrido, RapidMiner, por ejemplo, también está haciendo bastante sombra. El AutoML lo que hace es que todos estos insights, porque al final estas plataformas lo que te hacen es desvelarte insights que están revelando a los datos, ya te están marcando, te están sugiriendo qué columnas habría que eliminar potencialmente, qué tipo de filas... qué tipo de variables, qué correlaciones tienen entre ellas, todo esto de forma automatizada. Esto, al final, lo podemos traducir en tiempo, y al final el tiempo es dinero. Claro, ¿qué problema presenta esto para el usuario? Básicamente, el problema es que las licencias aún son bastante caras. Estamos hablando de que una licencia de AutoML, de estos softwares de pagos, pueden oscilar desde los 50.000 euros por usuario hasta los 150.000 euros anuales. esto es un coste que no todas las empresas se pueden permitir, pero sí que es cierto que lo que a priori era un mundo bastante cerrado, del auto M&L, que aquí la ventaja, vuelvo a repetir, es que te permite realizar de forma automatizada el EDA, lo que es el Exploratory Data Analysis, y la ETL, junto con la selección de algoritmos, Lo que ha sucedido es que han aparecido ya versiones mediante las cuales con código y de forma gratuita podemos utilizar AutoML. En esta sesión de hoy, vengo a exponeros las principales herramientas de código gratuitas que podemos utilizar para trabajar con AutoML. Cuando hablamos de AutoML tenemos que pensar que, como os he dicho siempre, la primera parte es entender el contexto, el negocio, la naturaleza del problema. pero también debemos de visualizar los datos, debemos pintarlos, esta parte es muy importante porque si no visualizamos los datos no llegaremos a entender si hay outliers, si hay correlaciones, hablábamos también de matrices de correlaciones, por ejemplo, de relaciones entre variables. ¿Qué ocurre? Que este proceso es bastante lento también porque a nivel de código hay que ejecutar Muchas sentencias y a día de hoy ya tenemos herramientas que nos permiten hacer autoeda. Autoeda sería automachine learning aplicado al exploratory data analysis. ¿Qué herramientas tenemos? En Python tenemos la librería Pandas Profiling, que desde mi punto de vista es la más potente y la mejor resuelta. ¿Y por qué? Porque estamos hablando que básicamente en 10 líneas de código, cargando nuestro dataset, nos va a ejecutar, nos va a... pintar. todos los datos referentes nos van a visualizar todas las correlaciones, todas las estadísticas de este conjunto de datos y además nos lo va a generar en HTML. Con lo cual estamos hablando que en cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos. No solo tenemos Pandas Profiling, también tenemos SuiteBits que de una forma diferente, con otro planteamiento nos arroja. prácticamente la misma cantidad de datos. El objetivo de la autoeda, así como del automachine learning, será siempre ahorrar tiempos. Os dejamos aquí dos notebooks. para ejecutar con Google Colab, donde podéis probar estas herramientas de forma muy sencilla, simplemente subiendo vuestros datasets. Y veréis que es una forma muy ágil de comenzar a trabajar con una base sólida. Respecto al Automation Learning, creo que cualquier persona que haya trabajado en este sector como científico de datos, este es el sueño de cualquier científico. el poder utilizar con código Automation Learning. ¿Por qué? Porque básicamente, realmente diríamos que es la máxima expresión de la practicidad. ¿Por qué? Porque al final en código tenemos acceso, podemos utilizar todas las herramientas que queramos y van evolucionando de forma muy rápida, pero si podemos utilizar Automation Learning, coger las ventas. de otro tipo de software, que en este caso son corporaciones muy potentes, con mucha inversión, poder utilizar características similares a las que ellos proponen, esto realmente es algo muy práctico. Con Automation Learning, consideraciones a tener en cuenta, al final Automation Learning nos va a hacer una... En el caso del código no tenemos tantas herramientas para... para utilizar el proceso de ETL, pero en el momento que tenemos que probar diferentes algoritmos, tenemos que pensar que esto, hacerlo prácticamente uno a uno, pues nos llevaría, es un proceso muy lento porque hay que invocar el algoritmo, probarlo, comprobar resultados. Aquí la ve, la ventaja que nos da Automachine Learning, que simplemente le subimos los datos, realizamos la ETL trabajamos las variables y automáticamente decimos pruébanos el mejor... algoritmo, o sea, pruébanos, por ejemplo, le podemos decir que nos pruebe 100 algoritmos y que, y ordenanos en un ranking los que mejor performance tienen sobre los datos de test, ordenarlos por accuracy, por predicción. Esto estamos hablando que es un avance total, aparte que ya te ofrece y te proponen los hiperparámetros, es decir, nos soluciona dos partes muy pesadas. al final probar diferentes algoritmos, lleva un tiempo bastante elevado, en tiempos de programación, pero ya una vez hemos seleccionado uno, seleccionar los hiperparámetros es la parte, diríamos, más abstracta, más técnica y más compleja, al final estamos hablando de hilar fino. Cuando hablamos de ajustar hiperparámetros, simplemente, para que os hagáis una idea, es cuando tenemos una precisión del 90%, cómo subir hasta el 95%. Este tramo, pues... es el más complejo de todos, porque parece que ya estamos, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando de que estas herramientas nos pueden resolver muchísimos problemas. Entre las más comunes, y hay que saber que el Machine Learning cambia constantemente, es decir, es un campo que está totalmente... que está totalmente vivo. Aquí os presentamos herramientas de AutoML en código gratuitas como puede ser, por ejemplo, TPOD. También te diríamos H2O o H20, depende de cómo lo queramos leer. AutoKeras. AutoKeras lo que está haciendo es auto-deep learning. Por decirlo de alguna forma es decir, Keras es un framework de deep learning. Normalmente. cuando nos enfrontamos a crear una red neuronal, es decir, y por un poco retomar los conceptos anteriores, cuando tenemos muchos datos, cuando tenemos un dataset muy elevado y podemos utilizar deep learning. Nuestro principal reto va a ser en decidir cuál va a ser la arquitectura de esta red neuronal vía capas o vía neuronas. Lo que nos ofrece es Autoqueras, que realiza AutoML y nos revela cuál es la mejor arquitectura posible para nuestro problema. También tenemos otro Scikit Learn, Scikit Learn es una librería de Machine Learning de Python muy potente. Tenemos otro PyTorch y seguramente por el momento que veáis estas clases habrá salido alguna más. Nosotros por nuestro lado os adjuntamos la documentación para que podáis trabajar con ellos. A mí, personalmente, AutoH2 es la que más me gusta. Y el motivo es porque, aparte, ofrece una serie de librerías que te muestran la explicabilidad del modelo, como los Shape Values, por ejemplo. La explicabilidad del modelo es muy importante, porque es no sólo decir, este modelo es el que mejor predice, sino nos va a explicar en nuestro caso de negocio, cada modelo nos va a explicar también cuáles son las palancas o cuáles son las variables claves y en qué porcentaje para poder pasar a la acción, poder mejorar desde el punto de vista de negocio. Por tanto, os animamos a que probéis los notebooks que os dejamos aquí y que disfrutéis del concepto de Automation Learning.


### 507.E1.U7.1.1_Auto_ML-El_futuro_de_la_IA

### El futuro de la IA
**Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no hablar de un concepto que se llama Automated Machine Learning.**

Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolle inteligencia artificial. 

A priori, puede parecer una amenaza para la humanidad, pero no tiene nada que ver con todo esto. 

Simplemente, estamos hablando que la razón de ser de este campo de la AI y del Machine Learning es eliminar tareas repetitivas de bajo valor, por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías. 

¿Qué significa esto? Significa que de una forma muy rápida, tendremos plataformas que nos permiten hacer tareas que en otras ocasiones duraban meses. Debemos ponernos un poco en la piel de un científico de datos. Tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. 
1. Desarrollo a Bajo Nivel
2. Programas de Alto Nivel (Drag & Drop)

#### 1 | Desarrollo a Bajo Nivel.
Estaríamos hablando de código, de Python, que es el lenguaje oficial, por decir alguna forma, de esta comunidad, aunque están apareciendo otros como GO que también pretenden ganar terreno.

También tenemos R, que se utiliza más en el ámbito de la bioinformática.

#### 2 | Programas de Alto Nivel
Aquellos que son user friendly, interfaces de usuarios que son amables, fáciles de entender. 

En pocas palabras, estamos hablando de picar código versus tener herramientas que son, prácticamente, Drag&Drop, que son ya más familiares y que trabajan con el concepto de nodos. 

![[507.E1_Auto_ML_-_El_futuro_de_la_IA_1.png]]

##### Herramientas Drag & Drop
¿Qué herramientas tenemos en este segundo grupo del drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial de aprendizaje? Tenemos herramientas como:
- Knime (Gratuita)
- Orange (Gratuita)
- Big ML (De pago)

Knime está muy bien para el aprendizaje en fases tempranas y para prototipar, poder defenderte y comprender, de una forma esquemática, lo que son los flujos de un algoritmo respecto en la ETL en la fase de prueba, de ensayo y error con diferentes algoritmos.

> Obviamente, no son programas profesionales, pero nos sirven de puente para poder llegar a aplicar todos los conceptos, tanto teóricos como prácticos.

Después tendríamos programas como Orange, que es del pack Anaconda y, a su vez, Anaconda es el grupo donde se ubica Python, donde han hecho una interfaz mucho más amable que también nos permite prototipar muy rápido y de una forma muy intuitiva. 

Nos permite, en definitiva, acortar tiempos y sobre todo no ponernos barreras al aprendizaje. 

Big ML sería un concepto que ya se sale de lo que son los nodos, realmente es un concepto por ventanas. 

Estas serían las tres herramientas. Bueno, las dos primeras son gratuitas, Big ML es de pago, pero, en definitiva, este concepto de nodos, es el que están utilizando ya plataformas como Azure (Microsoft) o IBM Watson, que son ya directamente plataformas Drag & Drop. 

**¿A qué nos referimos cuando estamos hablando de conectar nodos? **
- Conectar nodos lo que implica es que nos evitamos muchos errores de sintaxis de código. 
- Pero lo que implica también es que no tenemos tanta capacidad de edición como tenemos en el código.
- El código va a ser la tecnología que siempre va a ir más rápido, la más profesional y la más escalable de cara a ponerlo en producción. 
- Con el nacimiento de Azure o IBM Watson, se ha abierto una puerta a trabajar Drag & Drop que nos puede dar muchísima agilidad a la hora de trabajar. 
- A partir este punto es cuando empezaron a salir nuevas tecnologías como Data Robot o H2O o DataIQ, DataBricks, RapidMiner, donde se centraron ya otra vez en los nodos, pero yendo un paso más allá, aplicando a Automated Machine Learning.

#### Automated Machine Learning
Como hemos visto, es necesario hacer una serie de comprobaciones de conceptos claves, como los Missing Values, los Outliers, qué correlaciones hay entre diferentes variables.

![[507.E1_Auto_ML_-_El_futuro_de_la_IA_2.png]]

Esto, en código, lleva bastante tiempo, siempre también teniendo en cuenta que tenemos que visualizar siempre los datos y tomar decisiones y ver qué factores hay que corregir y depurar, qué es el proceso de TLE.

Por tanto, aquí surge Automated Machine Learning como una solución. 

[DataRobot](https://www.datarobot.com/) es el líder a nivel mundial, pero ya existe un grupo bastante nutrido de [Rapidminer](https://rapidminer.com/platform/?product_marketing_c=studio&source_marketing=ppc&campaign_marketing_c=branded) que le está comiendo terreno.

Lo que hace el Automated Machine Learning es sugerir, de forma automatizada, qué columnas, qué tipo de filas y qué tipo de variables, hay que eliminar y las correlaciones entre ellas.

Esto lo podemos traducir en tiempo y el tiempo es dinero. El problema es que las licencias son bastante caras, desde 50000 € por usuario hasta los 150000 € anuales, un coste que no todas las empresas se pueden permitir.

Pero lo que, a priori, era un mundo bastante cerrado de la Automated Machine Learning, ya te permite realizar, de forma automatizada, el EDA, el Exploratory Data Analysis y la ETL, junto con la selección de algoritmos.

También han aparecido ya versiones mediante las cuales con código y de forma gratuita podemos utilizar Auto ML. 

**Cuando hablamos de Auto ML, tenemos que recodar la primera fase**

Entender el contexto, el negocio, la naturaleza del problema. Pero también visualizar los datos, pintarlos. 

Esta parte es muy importante, porque si no visualizamos los datos, no llegaremos a entender si hay Ouliers o correlaciones. 

Hemos visto también matrices de correlaciones, por ejemplo, de relaciones entre variables. ¿Qué ocurre? Que este proceso es bastante lento también, porque, con respecto al código, hay que ejecutar muchas sentencias y a día de hoy ya tenemos herramientas que nos permiten hacer Auto EDA. Auto EDA sería auto Machine Learning, es aplicado al Exploratory Data Analysis.


### 508.E1.U7.1.2_Auto_ML-Herramientas

### Herramientas de Auto Machine Learning
Un buen ejemplo puede ser [Pandas Profiling](https://pandas-profiling.ydata.ai/docs/master/index.html)

![[508.E1_Auto_ML_-_Herramientas_1.png]]

#### Pandas Profiling
**El objetivo principal de esta herramienta, es proporcionar una experiencia de análisis de datos exploratorios (EDA) de una línea en una solución uniforme y rápida.**

- En diez líneas de código, tras cargar nuestro Dataset, nos va a a pintar todos los datos.
- Vamos a visualizar todas las correlaciones, todas las estadísticas de este conjunto de datos.
- Además, nos lo va a generar en HTML.
- En cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos.

> Auto ML es el sueño de cualquier persona que haya trabajado en este sector como científico de datos. Poder utilizar con código Auto Machine Learning es la máxima expresión de la practicidad.

Tenemos a nuestro alcance todas las herramientas que queramos mientras estas van evolucionando de forma muy rápida. 

Usando Auto ML podemos aprovechar las ventajas de otros tipos de software, detrás de los cuales hay corporaciones muy potentes y con mucha inversión, o sea, que podemos utilizar soluciones similares a las que ellos proponen.

  ![[508.E1_Auto_ML_-_Herramientas_2.png]]  

#### Consideraciones a tener en cuenta en Auto ML
**Hacer, manualmente, el trabajo que realiza Auto Machine Learning nos llevaría mucho tiempo, sería un proceso muy lento.**

Con Auto ML solo tenemos que:
1. Subir el Dataset.
2. Realizar la ETL.
3. Trabajar las variables.
4. Decirle que nos pruebe diferentes algoritmos y que seleccione, en un ranking, el que mejor performance tenga sobre los datos de Test, ordenándolos por precisión.

Esto supone un avance total. Probar diferentes algoritmos conlleva un tiempo bastante elevado en tiempos de programación, pero, una vez hemos seleccionado uno, seleccionar los hiperparámetros es la parte más abstracta, técnica y compleja, pues trata de hilar fino. 

Cuando hablamos de ajustar hiperparámetros, simplemente para que os hagáis una idea, es cuando tenemos una precisión del 90%, cómo subir hasta el 95%. 

Este tramo puede ser el más complejo de todos, porque parece que ya estamos, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando que estas herramientas nos pueden resolver muchísimos problemas. Entre las más comunes, sí hay que saber que el machine learning cambia constantemente. Es decir, es un campo que está totalmente, que está totalmente vivo. 

Aquí os presentamos herramientas de Auto ML en código gratuitas:
- Teapot
- H2O
- AutoKeras
- AutoPylearn
- AutoPyTorch

Cuando vamos a crear una red neuronal, tenemos un dataset muy elevado y podemos utilizar deep learning, nuestro principal reto va a ser en decidir cuál va a ser la arquitectura de esta red neuronal, vía capas o vía neuronas. Lo que nos ofrece AutoKeras, que realiza AutoML, y nos revela cuál es la mejor arquitectura posible para nuestro problema. 

A destacar Auto H2O, que gracias una serie de librerías que te muestran perfectamente el concepto de explicabilidad del modelo. 

La explicabilidad del modelo es muy importante porque no nos dice “este modelo es el que mejor predice”, sino nos va a explicar, para nuestro caso de negocio, cuáles son las palancas adecuadas, las variables clave y en qué porcentaje para poder pasar a la acción y poder mejorar desde el punto de vista de negocio.

Por tanto, os animamos a que probéis todos las herramientas y que disfrutéis del concepto del Auto Machine Learning.


### 509.E1.U8_Graphext-video

### Graphext
En el siguiente vídeo os presentamos [Graphext](https://www.graphext.com/), una empresa dedicada al Análisis exploratorio de datos.

Gracias a su herramienta no-code, permite a sus clientes ser más autónomos y menos dependientes de otros Data Scientist, permitiéndoles así explorar datos de sus propios clientes y hacer preguntas ad-hoc, de forma independiente y con la facilidad de uso de una hoja de cálculo más el poder de un Notebook.


### 510.E1_Implementación_de_la_IA_a_Negocio-video

### Implementación de la IA a Negocio
![[510.E1_Implementación_de_la_IA_a_Negocio.mp4]]
[Implementación de la IA a Negocio](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866919-u8-victoriano-izquierdo-implementacion-de-la-ia-a-negocio)

En este máster ya os han estado enseñando bastantes cosas de datos, inteligencia artificial, ¿no? Y la realidad es que hoy día sacarle provecho a todas estas técnicas es verdaderamente difícil. si no sabes programar. Y creo que se da bastante la circunstancia de que hay gente que tiene una capacidad muy analítica de las cosas y sabe hacer buenas preguntas con datos, pero eso no quiere decir que sepa hacer esas buenas preguntas, traducirlas a un lenguaje como Python o R. Creo tanto en esto que llevo cinco años con un equipo de 20 personas montando una herramienta de análisis de datos en la que intentamos que toda esta gente en la que se le queda corto el Excel y quieren ir más allá y quieren hacer ciencia de datos. datos, Data Science, más allá de agregar y pivotar tablas y hacer gráficos sencillos, poder dotarles de unas herramientas que permita ir y explotar esa creatividad y esas intuiciones a través de los datos que empieces a recoger de lo que ocurre dentro de una plataforma que hagas, de tus clientes, de tus productos, de tus proveedores o del mercado. Hay una cantidad de datos pública y privada que de qué sirven los datos. Y también tengo la se puede mejorar lo que no se mide. Y en eso es lo que os voy a enseñar, lo que hemos estado trabajando este tiempo. Todo lo que hemos aprendido de Data Science, lo hemos intentado llevar a una herramienta no-code, que es ahora cómo se le llama a esta corriente de herramientas que intentan hacer cosas que hasta ahora no se podían sin código. Y de eso vamos a hablar. Entonces, bueno, os voy a empezar hablando y os voy a empezar enseñando. contra lo que competimos, para que quede bien claro exactamente dónde se posiciona una herramienta como la que hacemos, qué no somos y qué sí somos. Entonces, aquí en esta diapositiva lo que enseño es un dashboard. Un dashboard o un cuadro de mando, como le decimos en español, viene a ser el lugar donde se ponen esas métricas de negocio que a menudo son como la estrella polar, de las empresas, de hacia dónde quieren ir, cuál es ese número mágico de clientes de facturación en el que llegas a Breakeven, pero de qué nacionalidad son esos clientes, cuánto te usan por mes... Bueno, hay un montón de variables, literalmente, hoy día. una empresa mide cientos de variables distintas y al final se seleccionan unas pocas, a las que se suelen llamar KPIs, como las métricas más importantes. Y eso está bien, y para eso se usan los dashboards. El problema es que los dashboards normalmente te dicen el qué ha pasado, pero no el por qué. ni tampoco te ayudan a hacer predicciones de qué es lo que va a pasar. Entonces, obviamente, te dan intuiciones sobre esas dos cosas, sobre el por qué y el qué va a pasar, pero a menudo, a veces, llevan a equivocaciones también. todos sabemos esto de que correlación no implica causalidad, pero es que además la mayoría de cosas complejas que ocurren nunca son porque una variable sola ha cambiado, sino son por un conjunto de factores que se dan en el tiempo. Entonces, ante ese tipo de preguntas y respuestas, ha empezado este ámbito que se llama el Data Science, que de nuevo es un poco contraposición de lo tradicional del Business Intelligence, que estaba más basado en esta idea de crear dashboards para todo. Yo lo vería más como una especie de evolución complementaria, es decir, es interesante seguir teniendo dashboards porque hay métricas básicas que son las que hay que monitorizar a diario, que son va bien, va mal, pero en cuanto a entender el por qué cambia, ahí es donde creo que la ciencia de datos puede ayudar bastante. Voy a ilustrar todo esto con varios ejemplos. Y además también hay otra cosa que no hacen los dashboards, que es trabajar con datos desestructurados. Una gran cantidad de información que capturan hoy día las empresas no solo vienen en formato de dato tradicional, que sería como filas y columnas, sino que también vienen formadas de texto, imágenes o incluso vídeos. Muchísima de la comunicación que tenemos con los clientes, por ejemplo, a través de un chat, viene a través de un texto. Y ahí, por ejemplo, se expresa un montón de deseos y cosas que nos pueden dar bastantes pistas sobre qué es lo que quiere la gente o por qué dejan de ser clientes y cosas así. Entonces, voy a saltar directamente aquí a la aplicación para que entendáis. Lo primero que hacemos cuando creamos un proyecto es conectarnos a una fuente de datos. Tradicionalmente en una empresa lo ideal es que acabes montando lo que se llama un data warehouse, que es una base de datos en donde centralizas un montón de tablas que pueden proceder de muchas bases de datos distintas. Desde bases de datos internas en las que tú registres cosas básicas de la aplicación, a CRM como Salesforce o Haspot, donde tienes información sobre los contratos de los clientes, la comunicación que tiene la gente de ventas, a datos que tienen que ver con los eventos, con las cosas que están haciendo, con cada clic que hacen, a datos de comunicaciones, a través de chat y demás. Entonces, lo normal es que cojas esos datos, los pongas en un data warehouse y entonces intentes crear un modelo de tu cliente. o de tus productos, ¿vale? Y en función de ese modelo de datos de esa tabla, como la que vemos aquí, tengo aquí varias que voy a enseñar, ¿vale? Pues bueno, atacaremos un problema u otro, ¿no? Tengo dos datasets con los que quiero empezar. Uno creo que es muy intuitivo e interesante para todo el mundo. Es una encuesta de la encuesta de estructura salarial en un dataset público en el que nos puede servir para entender los salarios de los españoles, por qué ganan más o menos, qué características predice que alguien gane más o menos. Es verdad que nos... Podemos pensar que el cliente es España, o sea, los clientes son los españoles y el producto es el Estado. y queremos ver cómo optimizar esos salarios. Pero voy a también usar este, de hecho, es por el que voy a empezar, que es de una empresa de telecomunicaciones canadiense que liberó unos cuantos datos de sus clientes, en el que vamos a entender por qué estos clientes de esta compañía de telecomunicaciones se van. Y, de nuevo, creo que la metáfora es bastante equivalente a cualquier otro tipo de datos. Además de estos dos, luego vamos a ver otros de otra naturaleza. Por ejemplo, vamos a ver... ventas de productos, por ejemplo, de Mercadona, y vamos a ver qué productos se compran juntos, más habitualmente, y vamos a ver también, incluso, transacciones en Ethereum de blockchain, y vamos a entender, por ejemplo, qué conexiones, qué patrones podemos revelar de dinero que se mueve entre exchanges distintos y direcciones. O sea, como veis, un montón de casos de uso distintos, que vamos a intentar cubrir los próximos 10, 12 minutos. Empiezo por este. Lo que estamos viendo aquí en la pantalla es una base de datos de clientes de una empresa de telecomunicaciones de Canadá que tienen más de 7.000 clientes, en este caso, lo que han liberado al público para poder usarlo para gente que quiera aprender cosas de ciencia de datos. Entonces, como vemos, tenemos más de treinta y tantas variables asociadas a cada una de estas personas y automáticamente nada más cargar el dataset, lo que va a hacer es vemos que se crean estos histogramas y distribuciones que representan cada variable como está distribuida. En concreto, yo me voy a fijar aquí en cuánto pagan al mes. Si pinchamos aquí, vemos, por ejemplo, que la mediana son 70 dólares al mes. Pero también podemos ver los diferentes cuartiles, que son como el 25% de los que más pagan, empiezan en 90 dólares al mes y los que menos pagan son menos de 35, o sea, entre 18 y 35 al mes. Entonces, claro, algo que inmediatamente yo me puedo preguntar es, oye, esta gente que paga poco, o sea, estos clientes que pagan poco y son muchos, ¿qué es lo que tienen en común? Entonces, al hacer eso, no sé si veis que conforme cambio esta selección, todo lo demás es reactivo. Y aquí a la izquierda me dice qué es lo que corre la más con esa selección. Y como vemos, son estas cosas, por ejemplo, internet service, que esta gente que paga poco es porque no paga por internet. Y como vemos, la mayoría de clientes sí que pagan por fibra óptica y ADSL. Pero este grupo del 23% de clientes no paga por ello. Entonces, es una manera rápidamente de entender relaciones entre variables uno a uno. Pero lo que queremos entender es el churn, que es la variable que codifica si un cliente se va o no. O sea, si se ha ido o no de la empresa, ha dejado de pagar. Entonces, si pinchamos en los que churnean, aquí vemos automáticamente que me está diciendo que lo que más corre es el tipo de contrato. Vemos que los clientes que pagan contratos mes a mes, renovan mensualmente. suelen irse mucho más. Esto me viene a decir que son el 55% de los clientes, pero el 88% de los que se van. Mientras que los que tienen contratos anuales o de dos años es más infrecuente que se acaben yendo. Luego vemos que esto está seguido por otras variables. Por ejemplo, si tienen contratado online security, si tienen soporte técnico o los meses que llevan siendo clientes. Aquí vemos que a partir del mes 20, la probabilidad de que un cliente se vaya es mucho más baja. Entonces, rápidamente nos hacemos una idea variable a variable de qué es lo que está más relacionado con el churn. Y lo bueno es que además podemos... rápidamente encontrar otra relación. Por ejemplo, aquí vemos que los clientes que churnan en general suelen pagar más de 70 dólares al mes. Pero si me enfoco en los que se van y pagan menos de 30, pues vemos que eso el total charges ha sido poco. O sea, son clientes que han durado muy poquito. Entonces, el poder hacer estas preguntas así de manera tan dinámica y sugiriéndote cosas con una interfaz visual, creemos que cambia bastante las cosas respecto a la manera tradicional de tener hipótesis. muy buenas, excelentes, y usar los datos solo para validar una hipótesis. Aquí sería más bien que tengas una pregunta muy básica y con la interfaz, con las cosas que te vamos sugeriendo visualmente, haciendo esa navegación, tú puedas ir teniendo mejores preguntas y refinarlas. Esto lo hacemos gracias a técnicas de Machine Learning que aplicamos para que hagas buen uso del Machine Learning. Es muy meta todo. ¿Vale? Veréis que tenemos otras secciones distintas. Por supuesto, esto es una tabla del centro que se va filtrando. A veces, sin más, vas a querer hacer un segmento y decir, ¿qué clientes tengo que paguen mes a mes, que paguen más de esto? ¿Y qué paguen al mes esto? Y entonces, a base de filtrar estas cosas, pues tendrás una tabla u otra. Pero como veis, aquí tenemos esta sección del compare, que nos permite también hacer, antes hemos visto simplemente un segmento contra una selección. ¿Y qué pasa si saltamos aquí y comparamos los que pagan mes a mes, versus los que pagan de dos años y los que pagan de un mes? de un año. Pues automáticamente aquí en la pantalla vemos cómo me sugiere que otras variables distinguen más esos tres segmentos. Podríamos definir de nuevo cualquier segmento que quisiéramos de clientes, pero vemos que lo que más distingue el tipo de contrato es cuánto tiempo llevan siendo clientes. Es lógico, es difícil vender un contrato bianual a un cliente que lleva poco tiempo contigo, pero un cliente que ya lleva mucho tiempo y está contento es más fácil vendérselo. Y aquí vemos, en el tenis se ve muy bien es en el mes 25, o sea, a los dos años, cuando la probabilidad de que un cliente tenga un contrato que no sea mes a mes es más alta que de mes a mes. Parece que fidelizar a estos clientes para un contrato más largo ocurre a partir del mes 24. Es interesante, porque entonces podríamos ver qué distingue a un cliente que empieza el mes 24 solamente, por ejemplo, en variables socio-demográficas. Podríamos hacer esa pregunta de una manera muy rápida. Entonces, el análisis de datos es... todo el rato así, o sea, tenemos algunas intuiciones, observaciones, validamos o no que tengan razón y entonces con las sugerencias que tenemos nos hacemos preguntas nuevas que probablemente sean mejores. Como ésta hay otras interfaces como Plot, que permiten hacer cualquier relación entre variables, por ejemplo, si yo digo oye pues ahora quiero ver el tipo de contrato Escribo aquí Contract y pongo, por ejemplo, el Monthly Charges. Quiero saber de media cuánto pagan según tipo de contrato. Y entonces, automáticamente me dice que tiene sentido que mire estos tres tipos de gráficos. Si le doy a este Osplot, automáticamente me sale ordenado de más a menos cuál sería la distribución de lo que pagan. Aquí vemos que los de mes a mes son los que pagan contratos más altos, después los de un año y después los de dos años. Aquí vemos que la diferencia es de 73 al mes. a 64, o sea, me parece que los clientes más, los que tienen contratos más largos están ahorrando una media de 10 al mes, 10 euros al mes, pero también vemos que la dispersión es más alta, también vemos que los que tienen contratos de un año, también el 25% más alto paga más, y este gráfico de aquí además es filtrable también por cualquiera de estos filtros que tenemos aquí, entonces a lo mejor yo digo, oye, solo me quiero fijar en los clientes que además paguen por fibra óptica, entonces hago clic aquí, lo veis, y automáticamente... se ve muy bien cómo cambia y ahora se ve que no, son los de dos años los que pagan más mes a mes, sin embargo, si cambiaba DSL, pues se vería cómo cambia otra vez, y los que no, pues cambia. Entonces, tener esa capacidad de tardar décimas de segundo en ver la hipótesis que tiene, creemos que es fundamental. Hay algo que a mí me obsesiona bastante, que es esta idea del flow, que desarrolló un psicólogo ungaro, que tiene un apellido bastante impronunciable, que viene a decir que hay un estado mental en el que entramos de flow, un estado mágico, en el que tenemos un reto por delante que todavía no sabemos resolver, pero no es tan grande el reto como para desmotivarnos y ser un poco desmotivador, overwhelming, como dirían en inglés, ni tampoco es tan fácil el reto como que nos aburrimos. Si lo pensáis, los videojuegos, son precisamente el tipo de cosas que se diseñan para mantenerte en ese estado de flow. O sea, tienes un reto por delante y estás ahí como enganchado porque crees que lo puedes superar, pero tampoco es tan fácil como para aburrirte. Entonces, yo creo que este concepto se puede llevar al software y hacer software que nos meta en ese estado de flow, que haga que te metas en un estado creativo, que yo creo que la gente que se dedica a hacer fotografía o edición de vídeo también puede llegar a lograr. entra y se pueden tirar horas sin sentir ni padecer, sin sentir hambre ni nada, si se están muy enganchados. Y queremos que este software sea eso, pero para dado al mundo de análisis de datos. Entonces, bueno, esta sección de plot, como decía, es así. Entonces, cuando vamos a algo interesante, podemos exportarlo. Podemos dar exportar, exportarlo en un fondo claro, oscuro e incluso customizarlo mucho más. Pero también podemos guardarlo dentro de Graphics y guardarlo como un insight. Porque al final aquí lo que se trata es que encontremos insights y un insight al final es como una pieza de conocimiento que nos permite sacar conclusiones de algo, que es el fin último de esto de los datos. Entonces imaginaos que yo, por ejemplo, aquí veo que los clientes mes a mes pagan de media 10 euros más que los de dos años. Eso puede ser un insight interesante. Entonces la vamos aquí a guardar y digo... los clientes, contratos mensuales, pagan de media 10 euros más al mes. Entonces le vamos a guardar Insights y automáticamente se crea esta diapositiva en esta sección de Insights como si fuera PowerPoint y entonces podríamos aquí añadir más notas para que alguien de nuestro equipo venga aquí luego. y pueda reproducir este insight tal cual. Eso es una cosa clave de los datos, poder reproducir los resultados. A menudo se puede llegar a una conclusión equivocada de nuevo porque se ha hecho una mala interpretación de las variables. Algo que estamos muy obsesionados también en gráfics, a diferencia de que un científico de datos te venga y escriba algo en Python y no se dé cuenta, pero lo estaba interpretando mal porque no tenía en cuenta que tal variable se medía de una manera o la otra. Es muy importante poder reproducir esos resultados rápido, tener acceso a cómo se llegó a esa conclusión. Para eso hemos hecho este botón de Play, en el que pinchas, y automáticamente, tachán, se ve ahí el resultado. Y queda muy, muy bien. O sea, tú ahora puedes llegar aquí y saber muy bien exactamente todo el estado de cómo se encontró esa cosa. ¿Vale? Entonces, bueno. Hemos hablado mucho de exploración básica de datos, que es la parte más importante antes de construir un modelo predictivo. Pero ahora vamos a construir un modelo predictivo, porque queremos predecir qué clientes se van a ir, sobre todo los que todavía no se han ido. Hay una manera rápida de construir el modelo predictivo, que es seleccionando cuál es la variable que queremos predecir y luego las variables que están involucradas. Y este es el resultado del modelo. En el resultado del modelo podemos ver que el 76% de las veces, 0,76, se ha clasificado bien si un cliente se iba a ir o no antes de que se fuera. Y aquí podemos ver cuáles son las variables que han sido más predictivas al construir este modelo. Cómo cuánto dinero en total se le había cobrado al cliente, cuántos meses llevaba siendo, cuánto es el pago mensual, qué tipo de contrato, el método de pago y demás. Entonces, bueno, esto está muy bien y esto además lo podemos poner en producción y conectar con nuestra base de datos y así automáticamente todos los días computar y ver un modelo o cualquier cosa. En este caso estamos prediciendo qué cliente se va a ir, pero podría ser alguien que rellena un formulario y queremos saber la probabilidad de que se convierta en un cliente por las preguntas que ha contestado ese formulario. O podría ser, por ejemplo, también cuánto dinero va a gastar este cliente en total en nuestra plataforma. O sea, se pueden hacer modelos predictivos de muchas cosas. y los modelos predictivos al final es tener una target, una variable que quieres predecir y variables que quieres usar como factores. Pero lo que tiene muy de especial GraphX es que podamos entender estos modelos, podemos hacer modelos predictivos que se puedan explicar. Entonces... Lo que veis aquí en Graph, esto de aquí es un clustering de clientes, basándome en las variables que usaba en el modelo. Entonces, cada uno de estos puntitos es uno de estos 7000 clientes y está conectado a aquellos clientes que son más similares teniendo en cuenta estas variables. Entonces, si yo coloreo por estas variables, veremos que los clientes se distribuyen de múltiples formas a través de este clustering, esta reducción de mensualidad que se le llama. Creo que algo se toca en el máster de ese tema. Entonces, lo bonito que tiene GraphX es que automáticamente te crea estos clústeres que son grupos homogéneos de clientes, teniendo en cuenta todas estas variables. De tal manera que gente que lleve menos de 10 meses, menos de 10 meses, saldrá de manera muy oscura, púrpura oscuro, morado oscuro, saldrán por aquí. Y que además paguen mucho, pues, por ejemplo, gente que lleva poco y paga mucho está como por aquí, por el clúster 13. O gente que, por ejemplo, tiene contrato mes a mes, pues los que pagan mucho y tal y eso, están aquí. Pero hay una manera más rápida de definir el clúster. Y sobre todo, lo que quiero es ver dónde están los clientes que se van a ir. Entonces, si coloreo por esa variable, la del churn, vemos en rojo los clientes que se han ido. Entonces, a ojo ya, viendo esto visualmente, podemos ver que los clientes que se van suelen estar concentrados sobre todo por esta zona de aquí. Por ejemplo, estos clústeres de aquí tienen muy poca prevalencia de gente en rojo. Entonces, si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir lo vemos con mayor detalle. Y si esto lo ponemos aquí en relativo y lo ordenamos por esta variable de adlib, yo puedo ver, por ejemplo, que este clúster 13, esta gente de aquí, tiene una prevalencia de irse altísima. O sea, en concreto, el 74% de los clientes que tienen estas características se acaban yendo, siendo normalmente el 26% de los clientes los que se van. Podemos asumir que la combinación de variables de esta gente, es muy predictiva de por qué se va un cliente. Entonces, podemos ver aquí qué definen estos clientes y lo podemos ver con mucha precisión. O sea, estos clientes, por ejemplo, suelen pagar entre 95 y 100 euros al mes, ¿vale? Y suelen llevar menos de 20 meses, ¿vale? Y además están pagando por el streaming movies, ¿no? Por películas en streaming. También pagan por figura óptica y también pagan por streaming TV, ¿vale? Y tienen además un contrato mes a mes, ¿vale? Entonces, esa combinación de factores para esta Teleco es lo peor, ¿vale? Entonces, esto quiere decir que hay todavía muchos de estos clientes que no se han ido, ¿vale? Hay unos poquitos, un 1% de clientes que podríamos retener, que todavía no se han ido y tienen exactamente esas características. Entonces, el dato del otro no lo tengo aquí, pero por lo que he visto, la clave de esto era precisamente que están pagando por streaming movies y streaming TV. O sea, están pagando por un servicio extra a la teleco que luego en realidad estos usuarios no estaban usando. O sea, esto es un buen ejemplo de que te da una pista de por qué esta gente que paga por estas cosas y paga esta cantidad se va. En este caso, esta telecom no estaba midiendo eso, luego lo midió y se dio cuenta, pero si tú haces que pagues mucho por unos servicios que no usas, es fácil que te llamen por teléfono y te digan oiga, ¿quiere internet a 100 megas? Yo tengo 50, 50 euros en lugar de 100, y dicen, bueno, no tiene películas por internet streaming, pero no lo uso de todas maneras. Adelantarte al competidor que va a llamar a tu cliente y le va a ofrecer una oferta mejor, que es verdad que luego te va a pagar menos si tú le bajas lo que paga al mes, pero si le bajas lo que paga al mes y deja de irse, es mejor que se vaya, sobre todo teniendo en cuenta los costes de adquisición de los clientes que cada vez son más caros. El otro ejemplo sería este que os voy a enseñar, datos de en este caso son tuits que escriben ing. O sea, imaginaos que sois ING y queréis saber de qué se quejan vuestros clientes. Entonces, bueno, hemos capturado un montón de tweets en un último año que le han inscrito a ING, bueno, en 2019 que le inscribieron a ING. Y entonces, automáticamente, este clustering no está basado en la similaridad a los tweets por otro, o sea, está basado simplemente en la similaridad de los tweets por el texto. Entonces, bueno, si hacemos zoom por aquí, por ejemplo, veréis que, si os fijáis, aquí hay un poquito de texto. Todas estas cosas son gente diciendo cosas del estilo, probando en Firefox y Safari, borrando el historial y caché, dirección escrita a mano y sigo sin poder acceder. No es un problema puntual, pesa muy a menudo. Y veis que este otro tuit cerca. Viene a decir como, he probado en dos navegadores, o he probado en dos navegadores limpiando cookies y tampoco funciona, brillante lanzamiento de no sé qué. Se ve muy bien que es gente diciendo que tras hacer algo que probablemente le hayan dado en soporte, como limpiar las cookies por un navegador, siguen sin poder acceder a la web. Entonces, todos esos tweets similares conforman un clúster que emerge verde de una temática, que básicamente es gente que tiene problemas de acceso. Si nos vamos aquí, veréis que aquí hay gente que está quejándose de Apple Pay, Apple Pay para cuándo. Es gente que está pidiendo Apple Pay. Y mi Apple Pay, ¿para cuándo? Fijaos que aquí lo predicen. ¿Cómo va Apple Pay? ¿Os acordáis de ellos? Fijaos cómo estos modelos del lenguaje, que están basados en cosas que habéis visto en el máster, como cosas como BERT y otros modelos del lenguaje que usan transformers por debajo, son capaces de detectar que dos tweets son de lo mismo, aunque no repitan ni una sola palabra, porque entienden la semántica muy bien. Entonces, fijaos cómo este clúster rosa emerge, que lo tengo aquí, y además nos dice que está muy correlado con otra variable, que está muy correlado con la fecha. Entonces, podéis ver que casi todos estos tweets de la gente pidiendo Apple Pay fueron muy fuertes hasta marzo de 2019. ¿Aguantar en qué fecha? la posibilidad de estar en Apple Pay. Se ve fenomenal. Entonces, en este caso, esto es de un cliente, pero podríamos hacerlo de cualquier empresa, porque son datos públicos que están ahí, o de cualquier temática. O sea, podríamos entender muy bien grandes mercados de gente, un montón de, en este caso, tweets, de información que está libre y disponible, de qué va, y entender ahí patrones muy interesantes que están ahí sueltos. Bueno, además de esto, Hay cantidad de otro tipo de datos, por ejemplo, esto de aquí es... A partir de un millón de órdenes de compra en Mercadona, sacar qué productos habitualmente van más juntos en la cesta de la compra. Pues yo qué sé, fijaos que, por ejemplo, si pincho aquí en el hummus, pues se ve muy bien que la gente suele comprar el hummus con regañás, con aperitivos de triángulo de maíz, con picos artesanos, o sea, muy bien, ¿qué hace falta para untar ese hummus? ¿Vale? Pero también, bueno, es curioso, con garbanzos cocidos, esto es gente ya que es muy fan de los garbanzos, con pan de molde. Incluso podemos ver aquí que hay otro clúster, que, fijaos, si busco aquí, por ejemplo, merluza, Podríamos ver que los que compran merluza congelada la suelen comprar con calabacín verde, endivia y otros tipos de comida sana. Y esto a lo lavel podríamos correlarlo con otras variables sociodemográficas que tuvieramos los clientes, como el código postal en el que viven, saber la renta per cápita de ese código postal. Y con todo eso adquiriríamos una inteligencia que verdaderamente nos inspiraría un montón para decir dónde abrir una tienda, qué productos poner juntos en la web, o físicamente si es un local, etc. Y por último, dejaros esta visualización que hice el otro día, con todas las transacciones de Ethereum o de cualquier blockchain son públicas, es una de las propiedades que tiene el mundo cripto, que tenemos esta pseudo-anonimicidad. Entonces aquí me he sacado todas las transacciones en un día de Cilium, ¿vale? Y entonces podemos ver todas estas direcciones cómo están conectadas, ¿vale? A través de... cuántas transacciones que hay entre ellas. Entonces, sobre esto aplicamos estos algoritmos de clustering, que habréis visto un poco en el curso. Y entonces podéis ver cómo emergen determinadas comunidades donde transita mucho dinero entre ellos. Entonces, por ejemplo, veis esta cluster 2. Claro, aquí habría que ir y sacar y revelar estas direcciones con que se corresponden. Es muy probable que muchas de estas nodos tan grandes, que tienen tantas transacciones, fijaos ahí, este está más dividido. se correspondan con exchange. Podría ser Bit2Me, podría ser Coinbase. Entonces, claro, aquí sería muy interesante, esto es un ejercicio por hacer, de hay más sitios, más sitios disponibles donde hay más información asociada a ciertas direcciones públicas de blockchain. Entonces podríamos entender ciertos patrones. Incluso si cruzáramos series temporales de datos, de crecimiento y bajada del valor de algo con el movimiento y la formación de estos clústeres, a lo mejor... podríamos anticipar algo. Probablemente habrá direcciones de cripto que hagan transacciones mucho antes de que se desencadene un efecto. Entonces, yo creo que esto es un buen repaso de lo que se puede hacer con una herramienta con grafis. Como veis, no hemos escrito una sola línea de código y hemos hecho cosas impensables de hacer en una hoja de Excel. y que incluso escribiendo código nos llevaría semanas, a veces incluso meses. Y creo mucho en este tipo de herramientas, creo que es el futuro de la ciencia de datos. Y nada, tenemos cuentas gratis para usuarios y para estudiantes, así que si vais a nuestra página web podéis verlo en grafis.com y aplicar en esas cuentas de estudiantes.


