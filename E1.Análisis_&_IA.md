---
title: Análisis e Inteligencia Artificial | Especialización 1
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41429870-u1-introduccion-a-la-inteligencia-artificial-jose-peris
lang: es-AR
---
# Especialización 1. Análisis e Inteligencia Artificial
## U1. Introducción a la Inteligencia Artificial
### Introducción a la Inteligencia Artificial (Video)
![[470.E1_Introducción_a_la_Inteligencia_Artificial.mp4]]
[Introducción a la Inteligencia Artificial](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41429870-u1-introduccion-a-la-inteligencia-artificial-jose-peris)

Cuando hablamos de inteligencia artificial, en nuestro imaginario colectivo, vemos representado casi un cerebro artificial, lleno de cables, una gran maquinaria que prácticamente puede alcanzar el nivel de conciencia de una persona. Todo esto no es más que una representación y está bastante alejado de la realidad. La inteligencia artificial es un conjunto de funcionalidades que se desarrollan básicamente para realizar predicciones. El nacimiento de la inteligencia artificial, o sobre todo este amplio desarrollo que ha sufrido en los últimos años, está vinculado a la aparición de dispositivos como los smartphones, relojes inteligentes y todo tipo de dispositivos con IoT.

¿Por qué? Porque generamos cada día millones de datos. Simplemente al utilizar nuestras aplicaciones favoritas o al movernos, estamos generando una cantidad infinita de datos. Estos datos son ingeridos por las corporaciones que son los propios proveedores de estas aplicaciones que utilizamos. En este momento, lo que ocurre es que los datos pasan a un sistema de ingestión en la nube, se procesan y se diseñan algoritmos de machine learning para poder predecir acontecimientos. Y esto es lo importante, porque pasamos a la acción.

Imaginad que si tienes un reloj inteligente, siguiendo este esquema, sería capaz de predecir si vas a tener una enfermedad, si vas a sufrir un ataque al corazón, o incluso predecir cuál sería tu nutrición más adecuada. Junto a la inteligencia artificial, es importante tener en cuenta el concepto de "data driven". Al final, las organizaciones diariamente consumen y trabajan con una cantidad de datos que sirven para determinar si hay una enfermedad o no. Si somos capaces de sacarle partido a nuestro favor, podemos obtener ventajas competitivas importantes.

Quizás la representación más común que tenemos asociada a los datos son los dashboards, que son elementos de puntos de control donde, con un simple golpe de vista, podemos interpretar qué está sucediendo. Al final, es todo bastante más simple de lo que parece y hay que tener en cuenta solo un hecho: los datos cuentan historias de personas, de eventos, de sucesos. La base de todo este módulo de machine learning se trata de comprender primero el storytelling de los datos para después poder desarrollar algoritmos que jueguen a nuestro favor para resolver problemas. Sobre todo, lo que resuelve es la optimización de la eliminación de tareas repetitivas.

Para comprender bien este campo, porque escucharéis muy a menudo los conceptos "machine learning" y "deep learning", vamos a intentar establecer la diferencia de una forma sencilla. El "machine learning" lo podríamos explicar de una forma sencilla como funciones matemáticas que explican un conjunto de datos. Al desarrollar un algoritmo, lo que estamos haciendo es obtener la fórmula que explica este conjunto de datos; por tanto, no será muy fácil hacer predicciones.

El "deep learning", por contrapartida, lo que hace es copiar un poco el funcionamiento de nuestro cerebro. El "deep learning" es una evolución del "machine learning"; de hecho, el "deep learning" funciona con algoritmos de "machine learning". Lo que hace es copiar un sistema de capas y neuronas a través de las cuales se va haciendo un aprendizaje. Pero en estas neuronas tenemos que saber qué es lo que está operando el "machine learning". Para entender todo este campo de una forma muy sencilla, el ejemplo más simple es pensar en cómo enseñamos a los niños, cómo aprenden los niños.

Cuando queremos que un niño aprenda qué es un vaso, lo que hacemos es mostrarle muchas veces diferentes vasos de diferentes tamaños, de diferentes colores y de diferentes materiales. Al final, el niño lo que aprende es un patrón en el cual sabe que hay un continente y un contenido, una forma cilíndrica en la cual se puede verter un líquido. El "machine learning" y el "deep learning" se basan, esencialmente, en todo esto, en la repetición de patrones, que es lo que llamamos entrenamiento, para después poder predecir y reconocer estos patrones.

Hay otro concepto que se llama algoritmo, que normalmente la gente confunde con logaritmo, y es bastante divertido. Para entender lo que es un algoritmo, es muy fácil visualizar el ejemplo de una receta de cocina. ¿Qué tenemos en una receta de cocina? Nosotros tenemos ingredientes y establecemos una secuencia. El "machine learning" y el "deep learning", al final, no son más que líneas de código que invocan a unas librerías que nos están trayendo unas funciones que vamos a ejecutar, y simplemente las ordenamos y las ensamblamos. Por tanto, cuando pensemos en algoritmos, simplemente tenemos que pensar en una batería de código que ejecuta unas instrucciones de forma ordenada.

Abordando este problema de una forma muy simplificada, realmente todos los algoritmos de "machine learning" y de "deep learning" solo saben hacer tres cosas: predecir, clasificar y agrupar. ¿A qué llamamos predecir? Imaginad que trabajáis en una empresa del sector inmobiliario y estáis constantemente recibiendo viviendas, donde estas viviendas tienen una serie de características: número de baños, número de dormitorios, ubicación en una zona céntrica o en la periferia, o cualquier otro tipo de características. Hay atributos que van a hacer que el precio de esa vivienda sea mayor o menor.

Imaginad que durante años el equipo que trabaja en esta empresa inmobiliaria se ha dedicado a analizar cada vivienda con todas sus características y asignarles un precio. Esta tarea repetitiva y manual, si la trasladamos a "machine learning" o inteligencia artificial, se va a convertir en algo muy sencillo, porque lo que va a hacer el algoritmo es, básicamente, analizar y entender todas esas variables y, en base a esas variables, como ha aprendido de toda la gente que ha estado estableciendo un precio, lo que va a hacer es aprender a predecir este precio. Por tanto, el resultado, de una forma práctica, simplemente sería que solo tendríamos que introducir las características y el algoritmo nos haría una predicción del precio que debería tener.

¿Qué es clasificar? Imaginad también que cuando vais al médico y os hacen un chequeo, os realizan una serie de pruebas donde os miden diversos parámetros: la presión sanguínea, la respiración, etc. Imaginad que nos miden 10 parámetros. Con estos 10 parámetros, lo que nosotros queremos predecir, por ejemplo, es saber si este paciente va a tener un ataque al corazón o no, o si va a tener cualquier tipo de enfermedad o no.

¿Qué ocurre? Que aquí ha habido, imaginad, una cantidad de médicos que tienen que analizar muchísima información, 15-20 parámetros, que vamos a llamar variables de ahora en adelante, que van a tener que analizar. ¿Qué ocurre? Que aquí hay un parámetro que nos dice si este paciente va a tener un ataque al corazón o no. Analizando todos de una forma visual, tiene que predecir si esta persona va a ser propensa a padecer una enfermedad o no. Aquí tenéis que pensar que la mente humana no es capaz de procesar más de 150 líneas en un archivo, imaginad un Excel o un CSV. Por tanto, lo que haría el algoritmo es aprender a clasificar si va a padecer una enfermedad o no. De esta forma, simplemente, una forma automatizada, ya haría su propia clasificación y, en este caso, el doctor simplemente tendría que fijarse en los casos críticos en lugar de ir uno por uno.

Y el tercer caso, la tercera casuística que resuelve es la agrupación. En este caso, imaginaros, esto es muy utilizado en la segmentación de clientes. Por ejemplo, si tú tienes un comercio que es un e-commerce, tienes registrados a una serie de clientes que tienen unas características sociodemográficas y que ejecutan ciertos patrones de navegación en tu página web. Podemos entrar a los sistemas de inteligencia artificial para que hagan una clasterización, es decir, una agrupación, de modo que cada vez que tú entras y ejecutas tus patrones de compra o de búsqueda, te va a clasificar en un grupo.

¿Esto qué ventaja aporta? Aporta una ventaja que se llama personalización. Es decir, imaginaos el departamento de marketing; no es lo mismo que haga una oferta a un perfil de "Buyer Persona" generalista que pueda dirigirse a ti, con tus características, con tu clasterización, con tu segmentación concreta, donde posiblemente te vas a sentir mucho más atraído por esa oferta porque se ajusta más a tus necesidades.

Para entender dónde estamos en el campo de la inteligencia artificial, hay que entender muy bien qué pasó en el año 2021, porque realmente hubo un salto muy grande. Ha cambiado realmente el paradigma del avance de esta tecnología. Existen tres tipos de inteligencia artificial. La primera sería la "narrow intelligence" o inteligencia estrecha, que es la que vivimos cada día. Todos en vuestro smartphone habéis intentado escribir una palabra y os hace una predicción de texto. Esto sería, estamos hablando de predicción, pero sería un texto predictivo. Se están utilizando algoritmos de "machine learning" por detrás. Todos también hemos intentado hacer una foto con nuestro smartphone y nos marca la cara. Esto es una "bounding box". Ahí también estaremos viendo la inteligencia artificial, o todos hemos utilizado el traductor de Google.

Estos serían casos de inteligencia artificial estrecha, donde la principal característica es que se entrenan a los algoritmos para resolver funciones concretas y específicas. El siguiente estadio de la inteligencia artificial es la general. Aquí estaríamos hablando de que la propia inteligencia artificial ya tiene un número tan elevado de funciones implementadas y se relacionan de forma tan eficiente entre ellas que ya diríamos que puede tener conciencia propia. Realmente, nadie sabe en qué punto estamos. Sí que sabemos que estamos entre la "narrow" y la "general", pero no sabemos exactamente en qué punto justo estamos.

¿Por qué? Porque este es un mundo que se apoya muchísimo en la comunidad, en la investigación, y continuamente se van publicando papers a nivel mundial. La gente, sí que es verdad, que los desarrolladores e investigadores comparten muchísimo conocimiento; por tanto, es un poco difícil especificar el punto exacto. Y ya, por último, la tercera categoría sería la superinteligencia artificial, donde aquí ya hablaríamos de que se conectarían todas estas inteligencias artificiales generales. Aquí se ha escrito mucho y se ha hablado mucho de este punto, cómo puede afectar a la humanidad. A día de hoy, nosotros nos centramos en el estado del arte, que es donde estamos ahora, y nos ubicaríamos entre la "narrow" y la "general".

### La Inteligencia Artificial
Cuando hablamos de Inteligencia Artificial, en nuestro imaginario colectivo, vemos representado casi un cerebro artificial lleno de cables. Una gran maquinaria que prácticamente puede alcanzar el nivel de consciencia de una persona.

Todo esto no es más que una representación y está bastante alejado de la realidad. 
- La inteligencia artificial es un conjunto de funcionalidades que se desarrollan básicamente para realizar predicciones. 
- El nacimiento de la inteligencia artificial o sobre todo, este amplio desarrollo que ha sufrido en los últimos años, está vinculado a la aparición de dispositivos como smartphones, relojes inteligentes…
- Cada día generamos millones de datos, simplemente, bajando nuestras apps favoritas o simplemente moviéndonos a través de ellas. 
- Dichos datos son gestionados por propios proveedores de estas apps que utilizamos, que pasan a un sistema de gestión a la nube. Se procesan, diseñan y se preparan algoritmos de **Machine Learning** para poder predecir acontecimientos. 
- Esto es importante porque pasamos a la acción. Imaginaros que si tienes un reloj inteligente siguiendo este esquema sería capaz de predecir:
1. Si vas a tener una enfermedad 
2. Si vas a tener un ataque al corazón
3. Cuál sería tu nutrición más adecuada

![[471.E1_La_Inteligencia_Artificial_1.png]]

#### Data-Driven
Junto a Inteligencia artificial, es fundamental tener en cuenta el concepto de Data-Driven. Al final, las organizaciones diariamente consumen también y trabajan con una cantidad de datos que si somos capaces de sacarle partido a nuestro favor, podemos obtener ventajas competitivas cruciales.

Quizás, la representación más común que tenemos asociada a los datos son los Dashboards, elementos de puntos de control donde simplemente, de un golpe de vista, podemos interpretar qué está sucediendo.

> Es todo bastante más simple de lo que parece y hay que tener en cuenta solo un hecho; los datos cuentan historias.

**Cuentan historias de personas, de eventos, de sucesos…** Y la base de todo este módulo de Machine Learning se trata de comprender primero el storytelling de los datos para, después, poder ser capaces de desarrollar algoritmos que jueguen a nuestro favor para resolver problemas. Sobre todo, lo que aportan es la optimización para eliminar tareas repetitivas.

#### Machine Learning vs Deep Learning
![[471.E1_La_Inteligencia_Artificial_2.png]]

A menudo, estos dos conceptos son confundidos. 

**El Machine Learning es un conjunto funciones matemáticas que explican un conjunto de datos.** 
- Al desarrollar un algoritmo, lo que estamos haciendo es obtener la fórmula que explica este conjunto de datos.
- Por tanto, nos será muy fácil poder hacer predicciones.

**El Deep Learning trata de copiar el funcionamiento de nuestro cerebro.** 
- Deep Learning es una evolución de Machine Learning. De hecho, Deep learning funciona con algoritmos de Machine Learning.
- La diferencia es que es un sistema de capas y de neuronas a través de las cuales se va haciendo un aprendizaje.
- Pero en estas neuronas tenemos que saber que es Machine Learning lo que está operando para entender todo este campo. Un ejemplo simple es pensar cómo aprenden los niños.
- Cuando le enseñamos a un niño lo que es un vaso, lo que hacemos es mostrarle muchas veces diferentes vasos, de diferente tamaño, de diferente color, de diferentes materiales.
- Al final, el niño lo que aprende es un patrón en el cual sabe que hay un continente y un contenido, una forma cilíndrica, en la cual se puede verter un líquido.
- Machine Learning y Deep Learning se basan esencialmente, en la repetición de patrones, que es lo que llamamos entrenamiento, para después poder predecir y poder reconocer estos patrones.

#### Algoritmo
A nivel conceptual, un algoritmo es una receta de cocina. 

Tenemos ingredientes y establecemos una secuencia de Machine Learning que al final no son más que líneas de código que invocan unas librerías que aplican unas funciones que se ejecutan. Por tanto, cuando pensemos en algoritmos, simplemente tenemos que pensar en una batería de código que ejecuta; unas instrucciones de forma ordenada, abordando este problema de una forma muy simplificada.  
  
Realmente todos los algoritmos de Machine Learning y de Deep Learning solo saben hacer 3 cosas:
1. Predicción
2. Clasificación
3. Agrupación

##### 1 | Predicción
Imaginad que trabajáis en una empresa del sector inmobiliario y estáis constantemente recibiendo viviendas con ciertas características. 

Tienen un número de baños, un número de dormitorios, están situadas en una zona céntrica o una zona de la periferia entre otras. Estos atributos van a hacer que el precio de esa vivienda sea mayor o sea menor.

Imaginaros que durante años, el equipo que trabaja en esta, en esta empresa inmobiliaria, se ha dedicado a analizar cada vivienda con todas sus características y a asignarles un precio. Esta tarea repetitiva o manual, si la trasladamos a Machine Learning o Inteligencia artificial es muy sencilla, se va a convertir en algo muy sencillo, porque lo que va a hacer el algoritmo.

Básicamente, va a analizar y a entender todas esas variables, y con base en esas variables como ha aprendido de toda la gente que ha estado estableciendo un precio, lo que va a hacer es a aprender a predecir este precio. Por tanto, el resultado, de una forma práctica, sería que solo tendríamos que introducir las características y el algoritmo nos hará una predicción del precio que tiene que tener.

##### 2 | Clasificación
Imaginemos una visita al médico para un chequeo, donde nos hacen una serie de pruebas para medir diversos parámetros: La presión sanguínea, arterial, la respiración… Imaginaros que nos miden diez parámetros.

Con estos diez parámetros, lo que queremos predecir, por ejemplo, es saber si este paciente va a tener un ataque al corazón o no, o si va a sufrir cualquier tipo de enfermedad o no.

Normalmente, un doctor tendría que considerar decenas de variables y sus posibles interacciones con una probabilidad de fallo u olvido significativa. Sin embargo, al entrenar al algoritmo, automatizamos ese proceso y aseguramos una probabilidad de acierto mayor

El resultado de aplicar esta tecnología a este tipo de casos es que un doctor podría centrarse en casos más críticos o en la experiencia del paciente, en lugar de utilizar su tiempo en procesos de menor valor.

##### 3 | Agrupación
La agrupación es muy común para la segmentación de clientes. 

Por ejemplo, si tienes un comercio, e-commerce, por ejemplo, tienes registrados a una serie de clientes con unas características socio demográficas y que ejecutan ciertos patrones de navegación en tu página web. Podemos entrarlos a los sistemas de inteligencia artificial para que hagan una “pasteurización”, es decir, una agrupación, de modo que cada vez que un usuario entra y ejecuta unos patrones de compra o de búsqueda, el sistema lo clasificará en un grupo establecido.

¿Esto qué ventaja aporta? La personalización. Es decir, imaginaos el departamento de marketing. No es lo mismo que haga una oferta a un perfil de buyer persona generalista que pueda dirigirse a alguien con unas características y preferencias más concretas lo que hará que se sienta mucho más cómodo, identificado y atraído por esa oferta.

### Introducción a la Inteligencia Artificial (Video)
![[472.E1_Introducción_a_la_Inteligencia_Artificial.mp4]]
[Introducción a la Inteligencia Artificial](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41865984-u1-1-2-introduccion-a-la-inteligencia-artificial-jose-peris)

Para entender otro concepto importante en Machine Learning y poder avanzar, debemos saber que este campo se centra mucho en el estado del arte. ¿Qué es el estado del arte? El estado del arte lo definiríamos como el punto en el que nos encontramos, cuál es la última tendencia, cuál es el último avance y cuál es el último hito tecnológico que se ha alcanzado. Para que os hagáis una idea, todo esto empezó con la programación tradicional, basada en bucles, como por ejemplo, las condiciones if-else, que son muy simples. Pero posteriormente apareció el machine learning, que fue el primer gran salto. Machine learning lo que hizo fue desarrollar algoritmos que tienen una característica importante: pueden trabajar con big data, pero también pueden funcionar con menos datos. Por otro lado, machine learning es muy útil porque permite una gran explicabilidad de las características del algoritmo. Más adelante veremos un poco qué es todo esto.

Posteriormente llegó el deep learning, que, como hemos comentado, imita el funcionamiento del cerebro humano. Es cierto que deep learning es mucho más atractivo y avanzado a nivel de estado del arte, pero tenemos un handicap: necesitamos trabajar con cantidades de datos mucho más elevadas para obtener resultados óptimos. Después del deep learning, vino el automation learning. Esto ya es prácticamente llevar las cosas al extremo, porque estamos hablando de inteligencia artificial que genera inteligencia artificial. Esta ventaja ha beneficiado o perjudicado, dependiendo de cómo lo miremos, a los desarrolladores. ¿Por qué? Porque lo que hace es aplicar inteligencia artificial a procesos de elaboración de algoritmos que son muy tediosos. En pocas palabras, lo que hace es optimizar el tiempo y ser mucho más ágiles, aunque es cierto que no porque se optimice todo podamos utilizarla sin tener conocimientos en este campo, que sería el data science.

Y ya por último, lo que sería el estado del arte de hoy, tendríamos los transformers. Los transformers han sido la disrupción más fuerte que hemos tenido en 2021, y que veremos a continuación. Cuando hablamos de transformers, hablamos de GPT-3. GPT-3, en una frase, es el chatbot más inteligente del mundo. Estamos hablando de hitos que eran impensables alcanzar hace unos años. ¿Por qué? Porque GPT-3 ha sido entrenado con millones de librerías de Wikipedia y de casi todos los libros disponibles en línea. Y claro, todos tenemos la imagen del chatbot que, al hacerle cuatro preguntas, se equivoca en la tercera o no tiene coherencia en sus respuestas. GPT-3 es un producto de OpenAI, y detrás de OpenAI está Elon Musk. Sabemos que es el chatbot más inteligente del mundo, ¿por qué? Porque es capaz de adoptar la personalidad, por ejemplo, de un personaje histórico. Es decir, en GPT-3 puedes hablar con Albert Einstein y tener una conversación coherente, un diálogo totalmente lógico con él, pero él siempre hablará con su lenguaje y con sus palabras. En el mismo chat puedes añadir a Elon Musk, a Gandhi o a otros personajes públicos sobre los que hay mucha literatura en la red, lo que proporciona datos para el entrenamiento.

Pero la ventaja de los transformers, y es algo que se utiliza mucho en este campo, es que es un campo muy líquido. Es decir, el conocimiento que se descubre en el ámbito del habla o de la escritura se puede trasladar a la imagen. GPT-3 nació en 2021, cuando se liberó la versión beta. Y, bueno, estamos hablando también de que, al estar entrenado de esta forma tan eficiente, llega el punto en el que puedes pedirle, por ejemplo, a GPT-3 que te diseñe una página web que tenga dos botones en el centro, uno de color rojo y otro de color negro. Y no solo te la va a generar, sino que también te va a generar el código. Es capaz de programar en casi cualquier lenguaje de programación. Simplemente tienes que pedirle que te haga una función matemática que cumpla una serie de condiciones y lo que hará es generarte el código. Después, con este código, puedes ejecutar el programa y te va a funcionar. Estamos hablando de un salto realmente muy potente. Esta es la potencia de GPT-3, que la podéis comprobar en el Playground; os dejamos aquí adjunto el enlace para que podáis jugar. Simplemente tenéis que registraros con vuestra cuenta de Google. Podéis comprobar que incluso le podéis pedir que os escriba un ensayo entre Platón y Aristóteles hablando sobre el concepto del amor, con una longitud de caracteres que vosotros deseéis. Y os va a crear un ensayo coherente. Incluso le podéis pedir que coja los ingredientes que tenéis en la nevera y que os haga una receta. Con esos ingredientes, os la va a hacer. Por supuesto, no es perfecto al 100%. Estamos hablando de una versión beta, pero realmente ha llegado a unas cotas que eran impensables hace unos años.

Como os decía, este campo es muy líquido. Y cuando llegan sorpresas, suelen llegar juntas, porque hay mucha capilaridad entre proyectos. También apareció DALL-E, obra de OpenAI, que rompió el paradigma de lo que es la visión artificial, porque aquí se ha juntado el conocimiento avanzado en el tema del habla y la escritura con el conocimiento de la imagen. ¿Qué significa todo esto? Que simplemente mediante palabras podemos crear, por ejemplo, objetos o conceptos gráficos que nunca han existido y que no están en ninguna base de datos. Esta combinación de estos dos algoritmos es un ejemplo de la narrow intelligence; cuando se combinan, pueden llegar a algo mucho más amplio o elevado. La realidad es que los resultados también son fascinantes, pero sí que es cierto que con DALL-E hubo un impasse donde se frenó el desarrollo. Y bueno, vino un desarrollo por otro lado con Glide. Glide lo que ha hecho es mejorar todo esto y estamos hablando de que si tú quieres a Glide, le puedes pedir que te dibuje una ilustración de Albert Einstein con un vestido de Superman y te la va a hacer. O que le pidas que te dibuje a un gato jugando al ajedrez con el estilo de Dalí, y te lo va a ejecutar. Es decir, imágenes imposibles, imágenes que nunca han existido, porque al final se combina la semántica con la imagen. Pero bueno, esto estamos hablando del estado del arte. Esto es el punto máximo.

También quiero comentaros que algunos de estos algoritmos sí que están disponibles para probar la versión beta o ya tenemos el código. Pero, ¿cómo se hace? Otros, pues aún no, simplemente se han utilizado para mostrar el potencial de dónde podemos llegar. Por ejemplo, ¿qué puede hacer por nosotros Glide? Para mí, Glide puede hacer algo totalmente fascinante, como es el uncrapping. El uncrapping no es más que, imagínate que le tomas una foto a tu amigo o amiga, y el uncrapping lo que hace es, mediante algoritmos de visión artificial, reconstruir y ampliar la escena. Él mismo se va inventando la escena que continuaría, con lo cual nos amplía el campo. El punto en el que nos encontramos ahora es muy excitante, simplemente por el hecho de que comienzan a confluir las tecnologías. Por un lado, tenemos las tecnologías descentralizadas y, por otro lado, la inteligencia artificial. Esto nos lleva a que podemos generar NFTs simplemente mediante la tecnología, utilizando código de inteligencia artificial. Es por ello que algoritmos como VQGAN más CLIP nos están permitiendo generar lo que nosotros queramos simplemente incluyendo tres o cuatro palabras. Es decir, podemos pedirle que nos genere un animal en un estilo modernista, en una escena del universo. Estamos hablando de imaginación, de creatividad y, si os fijáis, casi siempre os hablo de que es VQGAN más CLIP; estamos hablando de la suma de dos algoritmos, del ensamblaje. ¿Por qué? Porque lo que hacemos es unir características de diferentes algoritmos. En este caso, lo que estamos uniendo es CLIP, que es un clasificador que describe de forma muy exacta la imagen que está viendo a un nivel de detalle muy elevado. Es decir, no es lo mismo detectar que la imagen que está viendo un algoritmo de visión artificial es un coche o una bicicleta que detectar que, por ejemplo, está viendo una imagen de un husky siberiano sobre una vidriera. Estamos hablando de niveles de detalle muy diferentes. Este avance en la semántica ha permitido, al fusionarlo con la imagen, llegar a cotas muy elevadas. Lo mismo ocurre con VQGAN, que viene de las GANs, Generative Adversarial Networks, que a nivel de imagen comprende la profundidad de la imagen que está viendo. Imaginaros que estamos viendo una imagen de un pájaro o de un perro; lo que hace es calcular este vector de profundidad y entiende que esto es un animal que se llama pájaro, que puede estar representado con plumas de diferentes colores o con picos de diferentes formas. Lo mismo para un perro, que puede tener diferentes texturas y tipos de arrugas. Claro, todo esto nos lleva a que realmente lo que vamos a ver, y de hecho lo vais a practicar, os vamos a dejar aquí una pieza del código donde podéis crear vuestro NFT a partir de las palabras que vosotros queráis y os lo convierte a vídeo, también os lo anima. Claro, todo esto realmente ha pasado a partir de 2021, aunque venía con mucho recorrido detrás. Por tanto, este es el estado del arte de hoy.

Como os comentaba también, el automachine learning se basa en automatizar las tareas del data scientist. ¿Y qué significa eso? Básicamente, de una forma muy genérica, lo que hace un científico de datos es, primero, recopilar datos, una gran cantidad de big data para resolver un problema. Estos datos se tienen que limpiar y preparar, porque muchas veces están rellenados por personas que han completado una base de datos y pueden haber insertado un valor nulo, por ejemplo. Esto hay que limpiarlo; es un proceso que se llama ETL, que significa Extract, Transform, Load, y AutoML ya permite agilizar este proceso. Estamos hablando de que un algoritmo se puede desarrollar o diseñar en horas, semanas, meses o años, dependiendo de la complejidad a la que nos enfrentemos. Por tanto, si cogemos una métrica estándar, como pueden ser dos o tres meses, el proceso de ETL representa el 80%, y el proceso de selección del algoritmo puede representar el 20 o 30%, dependiendo del problema que estemos afrontando. ¿Qué hace el AutoML? En estas dos partes más tediosas, nos automatiza los procesos y, al final, lo que nos va a hacer es limpiar de forma más o menos automatizada y nos va a hacer una clasificación de qué algoritmos son los mejores que encajan para tu problema. Con lo cual, al final, esto se trata de inputs y outputs. Entonces, el input son los datos y el output que te va a dar el AutoML es la limpieza de los datos y un ranking con los mejores algoritmos para resolver tu problema.

El tercer punto es el deep learning. Deep learning, como os he comentado, se trata de trabajar con capas y con números de neuronas. Al final, imaginaros un clasificador. Hemos dicho que la inteligencia artificial predice, clasifica y agrupa. Imaginaros que, mediante visión artificial, queremos hacer un clasificador que sea capaz de distinguir entre un perro y un gato. ¿Cómo funciona el deep learning? Pues bien, lo que hace es establecer una serie de capas con un número de neuronas, donde cada capa de neuronas se encarga de realizar una función específica. Imaginaros que las primeras capas lo que van a hacer es analizar de lo general a lo concreto. Es decir, las primeras van a buscar la forma de la cabeza de un perro o de un gato. Aquí se utiliza una técnica llamada embeddings, que traduce los píxeles a vectores, a unos y ceros, porque los algoritmos entienden unos y ceros. Entonces, en una primera instancia, reconoce la forma del animal de forma genérica, después pasa a analizar la cabeza, la estructura de ojos, nariz y boca, luego analiza diferentes narices y finalmente analiza las curvas, que son los vectores que marcan el contorno, y al final decidiría si es un gato o un perro.

Todo esto, ¿qué ventaja tiene? Que funciona con backpropagation. Para que os hagáis una idea, backpropagation significa que la red neuronal va aprendiendo. Va hacia adelante y en las neuronas hay lo que llamamos pesos, que es una función matemática que se relaciona con el machine learning. Lo que hace es hacer una iteración hacia adelante, comprueba el resultado; aquí, cuando hablamos de resultado, hablamos de precisión. Siempre medimos la precisión del algoritmo; una precisión máxima sería de un 95-97%, ya que el 100% no existe. Lo que hace es, imaginaros, obtener una precisión del 80%; cuando vuelve a hacer otra pasada hacia atrás, actualiza los pesos y los optimiza hasta ir subiendo la precisión de forma iterativa mediante lo que llamamos epochs. Este es un poco el concepto general de deep learning. Quiero que os quede claro que en deep learning necesitamos trabajar con miles de datos; estaríamos hablando, a lo mejor, de 50.000 filas si estuviésemos hablando de datos tabulares. Pero sí que es cierto que el deep learning a nivel de imagen, cuando estáis viendo, por ejemplo, el caso de los deepfakes, es un caso de visión artificial muy común. Y lo mismo, cuando tú estás viendo un deepfake, hay un algoritmo que está reconociendo todo lo que es el contorno de los ojos, la nariz y la boca, y lo que está haciendo es mapear otra imagen, analizando los dos patrones y fusionándolos. Esto es un poco la tecnología que hay detrás; debo explicarlo de una forma sencilla, pero claro, el deep learning también lo estaréis viendo en el tema, por ejemplo, trasladándolo otra vez a la descentralización. Estaréis viendo NFTs creados con estilos similares a Van Gogh, a Dalí, incluso a Picasso. Otra característica que tiene el deep learning es el transfer style, es decir, que puede transferir un estilo. Todo esto, en fases primarias, es lo que habéis visto también en filtros de diferentes aplicaciones o software de fotografía.

Como hemos dicho, machine learning, yo casi diría que es mi favorita, y la realidad es que es mi favorita por un factor: la explicabilidad. Aquí tendremos que entender una cosa: los algoritmos se miden con la precisión. ¿Qué significa esto? Que, por ejemplo, si yo quiero saber cuánto va a facturar mi cliente de aquí a dos semanas, y hacemos un algoritmo predictivo que nos emita esta predicción de si va a facturar ese día 2.500 euros, y la realidad nos dice que facturó 2.400 euros, estaríamos hablando de una precisión elevada, ¿no? 90, 92%. Bien, esto es interesante, ¿no? Porque nos permite anticiparnos. Si yo sé que mi negocio de aquí a dos semanas va a facturar 2.400 euros, me está indicando que necesitaré más materias primas, más trabajadores, etcétera. Esto se traduce en acción y en ventaja competitiva. Pero, ¿qué pasa si yo tengo una previsión? Machine learning tiene otra característica, que es la explicabilidad. Y esto es muy importante. Porque, por ejemplo, para hacer este algoritmo de predicción de cuánto voy a vender de aquí a dos semanas, le hemos insertado a nivel de datos de big data, que es nuestra gasolina, todas las variables que afectan a nuestro negocio. Una variable podría ser si se hizo una promoción o no. Otra variable podría ser el número de trabajadores que tenía ese día, otra variable podría ser el número de clientes, etcétera. Entonces, ¿qué ocurre? Que, aparte de haceros una predicción, os va a decir el feature importance. Y el feature importance significa que voy a saber cuánto afecta cada variable a ese resultado. Es decir, puedo llegar a saber que no solo sé que voy a facturar 2.400 euros ese día, sino que sé que las variables que más afectan son el que haya una promoción o no, al 82%, el que haya 10 trabajadores, al 72%, y así sucesivamente. Es decir, nos habla a nivel de negocio de las palancas que tenemos que activar para mejorar nuestras métricas.

Con todo esto, ya hemos hecho un recorrido del estado del arte, de la historia de cómo pasamos de la programación básica a los transformers. Muchos alumnos me suelen preguntar qué es mejor: si machine learning, si deep learning o si, en un momento dado, se puede utilizar programación tradicional o si hay que utilizar un transformer. Y la respuesta es que siempre es la misma: depende de la naturaleza de tu problema, del caso de uso y, sobre todo, de tus datos. Como he dicho al principio, los datos cuentan historias; por tanto, una vez entendida la historia, seremos capaces de decidir qué herramienta es la que mejor encaja para nuestro caso de uso.

### Tipos de IA
Para entender dónde estamos en el campo de la inteligencia artificial, hay que entender muy bien qué pasó en el año 2021, porque realmente hubo un salto muy grande que cambió realmente el paradigma del avance de esta tecnología. 

Existen tres tipos de inteligencia artificial: 
1. Narrow Intelligence
2. IA General
3. Super IA

![[473.E1_Tipos_de_IA_1.png]]

#### 1 | Narrow Intelligence
Narrow Intelligence (inteligencia estrecha), es la que experimentamos cada día todos en nuestro Smartphone. 
- ¿Habéis intentado escribir una palabra y os hace una predicción de texto? Esto sería predicción, de ahí **Texto Predictivo**.
- Todos también hemos intentado hacer una foto con nuestro smartphone y nos marca la cara. Esto es una **Bounding Box.** 
- Todos hemos utilizado, alguna vez, el **Traductor de Google.** 

Esto serían tres casos, muy conocidos, de inteligencia artificial estrecha, donde la principal característica es que se entrenan a los algoritmos para resolver funciones concretas específicas.

#### 2 | IA General
El siguiente estadio de la inteligencia artificial es la general. Aquí estaríamos hablando de que la propia inteligencia artificial ya tiene un número tan elevado de funciones implementado y se relacionan de forma tan eficiente entre ellas que ya diríamos que puede tener consciencia propia.

Realmente nadie sabe en qué punto estamos, sabemos que estamos en los comienzos, pero no sabemos exactamente en qué punto preciso. ¿Por qué? Porque este es un mundo que se apoya muchísimo en la comunidad, en la investigación y continuamente se van publicando papers a nivel mundial. 

Ha habido mucha innovación en este campo y se está acelerando mucho su desarrollo pero, por el momento, no podemos afirmar que hayamos alcanzado la inteligencia artificial general.

#### 3 | Súper IA
La tercera categoría sería la Súper Inteligencia Artificial, que se conectaría en todas estas inteligencias artificiales generales. 

Se ha escrito y hablado mucho sobre este punto, sobre cómo puede afectar a la humanidad. No obstante, siempre desde un punto de vista de ciencia ficción con matices post-apocalípticos. 

![[473.E1_Tipos_de_IA_2.png]]

#### El Estado del Arte
El estado del arte lo definiríamos como el punto en el que nos encontramos, ¿cuál es la última tendencia, cuál es el último avance, cuál es el último hito que se ha alcanzado?

Para que os hagáis una idea, todo esto empezó con la programación tradicional basada en los bucles. 
- Por ejemplo, “if-then” son condiciones y bucles simples con lógica aplicada, pero posteriormente apareció el Machine Learning, que fue el primer gran salto. 
- Machine Learning permitió desarrollar algoritmos capaces de trabajar con Big Data, pudiendo también adaptarse a menores volúmenes más tradicionales. 
- Unos años más tarde llegó el Deep Learning, que al final, como hemos comentado, lo que hace es copiar el funcionamiento del cerebro humano. 
- Sí que es cierto que Deep Learning es mucho más atractivo y es más avanzado a nivel de estado del arte, pero tenemos un handicap donde tenemos que trabajar con cantidades de datos mucho más elevadas para obtener resultados óptimos.

**Después del Deep Learning vino el Auto-Machine Learning.** Esto ya prácticamente es rizar el rizo porque estamos hablando de inteligencia artificial que hace inteligencia artificial. Al final, en pocas palabras, nos permite optimizar el tiempo y ser mucho más ágiles. Aunque es cierto que no porque se optimice todo podamos utilizarlo sin tener conocimientos de este campo que sería el Data Science.

##### Transformers - ChatGPT-3 y Dall-e
Por último, lo que sería el estado de arte del arte de hoy, tendríamos los Transformers. 
- Estos, han supuesto la disrupción más fuerte que hemos tenido en los últimos años. Cuando hablamos de Transformers hablamos de **ChatGPT-3,** el chat más inteligente del mundo. 
- Estamos hablando de hitos que eran impensables alcanzar hace unos años. GPT-3 se ha entrenado con millones de librerías de Wikipedia de todos los libros que existen casi disponibles online. Y claro, todos tenemos la imagen del chatbot que le haces cuatro preguntas y se equivoca a la tercera o no tiene coherencia en sus respuestas. GPT-3 es un producto de OpenAI y detrás de OpenAI está Elon Musk. 
- Es el chat más inteligente del mundo porque es capaz de tomar la personalidad de un personaje histórico, es decir, gracias a GPT-3, puedes hablar con Albert Einstein y tener una conversación coherente. Pero él va a hablar siempre con su lenguaje y con sus palabras, pero en el mismo chat también puedes añadir a Elon Musk, o a Gandhi, o a cualquier personaje público sobre el que haya mucha literatura y referencias. 
- La versatilidad de GPT-3 es increíble y esto es solo el comienzo. Puedes pedirle, por ejemplo, que te diseñe una página web que tenga dos botones en el centro de uno de color rojo y otro de color negro. Y no solo la va a diseñar sino que la va a generar en código. El código es capaz de programar en casi cualquier lenguaje de programación, simplemente tienes que pedirle que te haga una función matemática, que cumpla una serie de condiciones y lo que va a hacer, lo que vas a hacer es generarte el código y después con este código puedes ejecutar este programa y te va a funcionar. Estamos hablando de un salto realmente potente.

Podéis experimentar con GPT-3 a través de [este enlace](https://chat.openai.com/chat).
- Simplemente, tenéis que registrarnos con vuestra cuenta de Google. 
- Podéis comprobar que incluso le podéis pedir que os escriba un ensayo entre Platón y Aristóteles hablando sobre el concepto del amor con la longitud de caracteres que vosotros deseéis y os va a crear un ensayo coherente. 
- Incluso le podéis pedir que coja los ingredientes que tenéis en la nevera y pedir que os haga una receta con esos ingredientes y os la va a hacer. Por supuesto, no es perfecto al 100%. 
- Estamos usando una versión beta, pero realmente ha llegado a unas cuotas que eran impensables hace unos años. Como os decía, este campo es muy líquido y cuando llegan sorpresas suelen llegar juntas porque lo que hay es mucha capilaridad entre proyectos.

###### Dall-e
También apareció Dall-e, obra también de OpenAI, y ahí es donde rompió el paradigma también de lo que es la visión artificial, porque ya estamos hablando que aquí se ha juntado el conocimiento, que se ha avanzado en el tema del habla y la escritura con el conocimiento de la imagen.

![[473.E1_Tipos_de_IA_3.png]]

¿Qué significa todo esto? Que simplemente mediante palabras podemos crear, por ejemplo, objetos o conceptos gráficos que nunca han existido, que no están en ninguna base de datos. Y la realidad es que los resultados también son fascinantes. 

Actualmente, Dall-e se mejoró dando pie a Dall-e 2, mucho más potente y con capacidades mucho mayores tanto creativas como semánticas.

En paralelo y para mejorar el modelo de Dall-e se desarrolló GLIDE (Guided Language-to-Image Diffusion for Generation and Editing). Este algoritmo no solamente puede crear imágenes totalmente nuevas a partir de texto, sino alterar imágenes añadiendo objetos o texturas. A pesar de un entrenamiento más modesto (GLIDE está entrenado con 3.5 mil millones de parámetros en comparación a Dall-e que cuenta con 12 mil millones), es un algoritmo más eficiente y el cual necesita menos muestras para entender el objetivo.

### Algoritmos
Algunos de los algoritmos anteriormente mencionados están disponibles para testear en su versión beta o conocemos su código.

Simplemente ha sido una pequeña demostración de su potencial para ver hasta dónde podemos llegar.
1. GLIDE
2. AutoML & ETL
3. Deep Learning
4. Backpropagation & Precisión
5. Transfer Style

#### 1 | GLIDE
¿Por ejemplo, qué puede hacer por nosotros GLIDE?.
- Imagínate que tomas una foto a tu amigo en un contexto. Lo que hace GLIDE es ampliar la escena mediante “unwrapping”. 
- Él mismo se va inventando la escena que continuaría. El punto en el que nos encontramos ahora es muy excitante, simplemente por el hecho de que comienzan a confluir las tecnologías. 
- Por un lado, tenemos las tecnologías descentralizadas y, por otro, lado tenemos la inteligencia artificial. Actualmente, ya se están generando NFTs por ejemplo con inteligencia artificial.
- Es por ello que algoritmos como VQGAN+CLIP nos están permitiendo generar aquello que nosotros queramos, simplemente incluyendo tres o cuatro palabras. Es decir, podemos pedirle que nos genere un animal en un estilo modernista en una escena del universo. 

![[474.E1_Algoritmos_1.png]]

- Estamos hablando de imaginación, de creatividad. Y si os fijáis, casi siempre hablamos de que es “VQGAN+CLIP”, la suma de los algoritmos del ensamblaje. 
- Lo que hacemos es unir, unir características de diferentes algoritmos. En este caso lo que estamos uniendo es un CLIP (Contrastive Language–Image Pre-training), por ejemplo, que lo que hace es un clasificador que describe de forma muy exacta la imagen que está viendo a un nivel de detalle muy elevado. 
- No es lo mismo detectar que la imagen que está viendo un algoritmo de visión artificial es un coche o una bicicleta que detecte que, por ejemplo, está viendo una imagen de un husky siberiano sobre una vidriera. Estamos hablando de niveles de detalle muy diferentes. 
- Este avance en la semántica ha permitido, al fusionarlo con la imagen, llegar a cotas muy elevadas, lo mismo que VQGAN, que viene de las GAN (Generative Adversarial Networks), comprende la profundidad de la imagen que está viendo. 
- Imaginemos que estamos viendo una imagen de un pájaro o de un perro. Lo que hace es calcular este vector de profundidad y entiende que esto es un animal que se llama pájaro, que puede estar representado con plumas de diferentes colores o con picos de diferentes formas. Y lo mismo para un perro. Puede tener diferentes texturas, diferentes tipos de arrugas.

Os dejamos aquí una plataforma donde podéis crear vuestro NFT a partir de las palabras que vosotros creáis y os lo convierte o si es vídeo también os lo anima. Claro, todo esto realmente ha pasado a partir del 2021. 

![[474.E1_Algoritmos_2.png]]

#### 2 | AutoML & ETL
El Auto-Machine Learning, como comentábamos antes, automatiza las tareas del Data Scientist.
- La mayoría del tiempo, lo que hace un científico de datos es primero recopilar gran cantidad de Big Data para resolver un problema. 
- Estos datos se tienen que limpiar, se tienen que preparar, porque al final muchas veces están rellenados por personas que tenían una base de datos y pueden haber insertado un valor nulo por ejemplo. 
- Este proceso se llama ETL, que significa “Extract”, “Transform” and “Load” y el AutoML ya permite agilizar este proceso. 
- El proceso de ETL representa el 80% y el proceso de selección del algoritmo podría representar el 20%-30%. 
- El AutoML limpiará de forma más o menos automatizada y hará una clasificación de qué algoritmos son los mejores que encajan para el problema en cuestión.

#### 3 | Deep Learning
El tercer punto es el Deep Learning. 

Se trata de trabajar con capas y con números de neuronas. Podéis imaginaros un clasificador. Hemos dicho que la inteligencia artificial predice, clasifica y agrupa. Imaginaros que mediante visión artificial queremos hacer un clasificador que sea capaz de clasificar entre lo que es un perro y un gato. A través de Deep Learning, establecemos una serie de capas con un número de neuronas donde cada capa de neuronas es encargada de realizar una función específica. Imaginaros que las primeras capas lo que van a hacer es analizar de lo general a lo concreto, es decir, las primeras van a buscar una forma o una cabeza de un perro o una forma, una cabeza de un gato.

Al final aquí lo que se hace es una técnica que se llaman “embeddings”, que es traducir los píxeles a vectores, a 1 s y 0 s, porque los algoritmos sólo entienden de 1 s y 0 s. Entonces lo que hace es, en una primera instancia, reconocer la forma del animal de forma genérica. Después, pasaría a analizar la cabeza, una estructura de ojos, nariz y boca. Posteriormente, pasaría a analizar ya diferentes narices y después pasaría a analizar las curvas, los vectores que están marcando el contorno y al final decidiría si es un gato o es un perro.

![[474.E1_Algoritmos_3.png]]

#### 4 | Backpropagation & Precisión
Todo esto funciona gracias a la “backpropagation”,que es que la red neuronal va aprendiendo hacia delante y en las neuronas hay lo que llamamos los pesos, que es una función matemática.

Una vez establecidos los pesos, comprueba el resultado y este proceso se repite un determinado número de veces o “epochs”. Aquí cuando hablamos de resultados hablamos de precisión. Siempre medimos la precisión del algoritmo para saber su desempeño. Una precisión máxima sería un 95%-97%, el 100% no existe. Lo que va haciendo el algoritmo es ir iterando y actualizando los pesos optimizándolos hasta ir subiendo la precisión de forma iterativa mediante las “epochs”. Así, si por ejemplo, en la primera iteración la precisión del algoritmo es de un 80%, esta podrá ir mejorando a medida que sigamos entrenando y optimizando.

Esto es un poco el concepto general de Deep Learning. Hay que recalcar que necesitamos trabajar con miles de datos. Estaríamos hablando de, por ejemplo, 50.000 filas si estuviésemos hablando de datos tabulares. 

El Deep Learning se utiliza sobre todo a nivel visión artificial como en el popular caso de los Deep Fakes. En estos casos, un algoritmo está reconociendo todo lo que es el contorno de los ojos, la nariz y la boca, y lo que está haciendo es mapear otra imagen, está analizando los dos patrones y fusionándolos.

![[474.E1_Algoritmos_4.png]]

#### 5 | Transfer Style 
En cuanto al mundo de la descentralización, por ejemplo, estaréis viendo NFTs creados con estilos similares a Van Gogh, Dalí o Picasso. 

Otra característica que tiene el Deep Learning, que es el “transfer style”, es decir, que puede transferir un estilo.

![[474.E1_Algoritmos_5.png]]

### Machine Learning y el concepto de Explicabilidad
#### ¿Qué es la Explicabilidad?
Imaginemos que queremos saber cuánto va a facturar el negocio de nuestro cliente durante las próximas dos semanas. 
- Creamos un algoritmo predictivo que nos emita esta predicción que calcula que va a facturar 2.500 € y la realidad nos dice que facturó 2.400 €.
- Estaríamos hablando de una precisión elevada, 90%-92%.

Esto es interesante, porque nos permite anticiparnos.  
Si yo sé que mi negocio de aquí a dos semanas va a facturar 2.400 €, me está indicando que necesitaré:
1. Más materias primas. 
2. Más trabajadores.
3. Etc.

Esto se traduce en acción y en ventaja competitiva.

#### Machine Learning tiene otra característica: La explicabilidad
Para hacer este algoritmo de predicción de venta a dos semanas vista, le hemos insertado a nivel de datos de Big Data, que es nuestra gasolina. Todas las variables que afectan a nuestro negocio. ¿Cuántas?. 
- **Variable 1:** ¿Se hizo una campaña promocional?.
- **Variable 2:** Número de trabajadores que tenía disponibles.
- **Variable 3:** Número de clientes, etc.

¿Qué ocurre entonces? Que aparte de hacer una predicción, os va a decir el **Future Important.** 
- Vamos a saber cómo afecta cada variable a ese resultado. 
- Ya no solo sabremos que hemos vendido 2.400 € ese día, también las variables que más afectan son el que haya una promoción o no, al 82%, o el que tenga diez trabajadores, al 72%, y así sucesivamente.
- Nos dice, a nivel de negocio, de las palancas que tenemos que activar para mejorar nuestras métricas. 

Con todo esto ya hemos hecho un recorrido del estado del arte, de la historia, de cómo pasamos de la programación básica a los Transformers.

#### ¿Machine Learning, Deep Learning, programación tradicional o Transformers?
La respuesta siempre es la misma: Depende de la naturaleza de tu problema, del caso de uso y, sobre todo, de tus datos.

Los datos ahora hablan sobre historias. Por tanto, una vez entendida la historia, seremos capaces de decidir que herramienta es la que mejor encaja para nuestro caso de uso.


## U2. Los datos
### Tratamiento del Dato (Video)
![[476.E1._Tratamiento_del_Dato.mp4]]
[Tratamiento del Dato](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora)

Dentro del mundo de los datos se considera un concepto que se llama pipeline, en el cual hay diferentes fases. Un ETL consiste en extracción, transformación y carga. Extraction, Transformation and Load. Entonces, la fase de extracción normalmente corresponde a una serie de scripts o tareas que suelen leer bases de datos y cargan toda esa información en memoria. En memoria, lo que se hace es, mediante... en general, se usa Python para esto, pero también SQL, realizar unas transformaciones de estos datos que se leen. Por ejemplo, puedes tener diferentes tablas en una base de datos y quieres unirlas; todo eso se hace en ese momento. Es el momento de solucionar esos problemas, porque luego se realiza una carga dentro de un Data Warehouse, donde no se permiten las ediciones.

Cuando todos los datos van al Data Warehouse, ya están preparados para ser consumidos por especialistas de todo tipo, sean analistas, CEOs o CCOs. Actualmente, se ha puesto bastante de moda el ELT, que se llama así básicamente porque el precio de almacenamiento ha bajado considerablemente en los últimos años y también la potencia de cómputo de los Data Warehouses. Entonces, lo que se hace es extraer, como en el ETL, pero se realiza la transformación dentro del propio Data Warehouse. Se utilizan diferentes herramientas, pero ahora mismo la que está más en auge es DBT, que utiliza lenguaje SQL para conseguir este tipo de transformaciones. Se carga, como siempre, otra vez en el mismo Data Warehouse.

Aunque puedas ver tablas tanto en un Data Warehouse como en una base de datos relacional tradicional, las operaciones que puedes hacer sobre ellas varían bastante. Por ejemplo, normalmente se utiliza un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, inserciones o borrados de forma continua. Sin embargo, en el Data Warehouse, todas estas operaciones no están permitidas; en general, no se permite la edición de filas. Realmente no se hace una edición, sino que lo que se hace es añadir filas. Por ejemplo, tampoco se permiten los borrados a la ligera, aunque en algunas tecnologías sí que se permiten. En general, el coste por almacenamiento en un Data Warehouse es mucho menor que en una base de datos tradicional.

Un Data Lake también permite almacenamiento masivo, pero el dato está totalmente desestructurado y es ideal para guardar información que nos llegue en forma de eventos, que pueden tener cualquier estructura interna y pueden estar totalmente desordenados. Muchos se pueden preguntar dónde podemos usar un pipeline tipo ETL o un pipeline tipo ELT. Realmente no hay tanta diferencia a nivel de lo que se puede conseguir, porque al final siempre estamos cargando los datos a un sitio para que sean consumidos, pero realmente sí que hay que valorar el esfuerzo económico y el esfuerzo de tiempo de desarrollo y de reacción al cambio.

Entonces, por ejemplo, tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda, y todos esos datos llegan a nuestra pipeline. Son extraídos y luego aquí viene la gran diferencia. ¿Qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente? Si cargamos estos datos, realmente tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no esperábamos; a lo mejor tienen un campo que realmente no es correcto o tienen un campo que no está dentro del esquema que nosotros estamos buscando. Entonces, una buena medida es transformarlos antes de cargarlos, simplemente para asegurarnos de este tipo de errores. Pero al mismo tiempo, también queremos almacenarlos en un sitio, ahorrándonos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar. Entonces, es un tema de coste, de potencia de cómputo y de rapidez en la gestión del dato que diferentes negocios pueden valorar de diferentes maneras.

En general, ahora mismo el modelo ELT es más barato que el ETL. En Internex empezamos usando el modelo ETL, pero rápidamente nos dimos cuenta de que tenía mucho sentido migrar a un enfoque tipo ELT, básicamente porque nos costaba mucho más económico todas las gestiones que queríamos hacer y la potencia de las nuevas tecnologías como DBT, junto a BigQuery. En este caso, usamos BigQuery, y era una combinación que prácticamente toda la industria de datos estaba utilizando, y la comunidad también se estaba moviendo a desarrollar nuevos conectores y documentación que nos podía ayudar a distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente la pipeline que teníamos.

No solo eso, también teníamos un problema de que realmente nuestra extracción de datos estaba bastante acoplada directamente a guardar en BigQuery, así que guardar primero y luego transformar tenía mucho sentido. El primer paso de un pipeline de datos siempre es la extracción; la mayor parte de las veces se hace desde bases de datos, pero no siempre es el caso. Muchas veces quieres extraerlo de acciones que toman los propios usuarios; pueden ser clics, pueden ser cualquier tipo de gizmos, cualquier cosa. Entonces, la extracción desde este tipo de fuentes también es importante. Hoy en día se utilizan CDPs para realizarlo, y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, porque muchas veces se pierden eventos y se pierde información por el camino. Entonces, este desafío de tener un sistema robusto durante la extracción de datos es importante.

Es fundamental tener un esquema prefijado porque esto nos ayudará a que luego el dato en nuestro Data Warehouse esté de una manera clara para el resto del equipo. Entonces, lo que se suele hacer es, antes de la ingesta, establecer un protocolo de: "Oye, espero estos datos con estos campos, con estos tipos", y cuando pase por nuestra pipeline, todo lo que no entre dentro de ese marco lo podemos descartar o hacer una transformación para que entre dentro de ese marco. ¿Por qué? Porque en el mundo real pasan un millón de cosas y estamos sujetos a caídas de diferentes sitios o servicios, estamos sujetos a posibles ataques. Entonces, tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. Se suele decir que el 70 o 80% del trabajo de un data scientist es limpiar datos, y por eso no es una cuestión trivial.

Cuando llegamos a la visualización del dato, muchas veces el equipo de edición tiene que reportar al equipo directivo o a otro departamento. Es importante tener claras qué herramientas de visualización se van a utilizar y, sobre todo, qué implica, cuánto tiempo de trabajo implica utilizar esas herramientas, porque la visualización no es trivial. Muchas empresas tienen un departamento exclusivo para ello. Tenemos que optar entre tener un equipo para ello o usar una herramienta. Vas a tener que hacer consultas de SQL, vas a tener que contar con algún experto en usabilidad que pueda decir: "Oye, aquí van estos colores, aquí quiero presentar la información de esta manera o de esta otra manera", y tienes que tener claro cuáles son los gastos que vas a tener que hacer en los diferentes casos. Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redash que te permiten hacer más cosas, aunque son mucho más costosos y probablemente necesites un equipo dedicado a ello.

### Tratamiento del dato: ¿Qué es una ETL?
Dentro del mundo del Data, es un concepto denominado Pipeline, en el cual encontramos diferentes fases.

Una ETL consiste en Extracción, Transformación y Carga (_Extraction, Transform and Load_).

![[477.E1_Tratamiento_del_Dato.png]]

#### 1 | Extracción
- La fase de extracción, normalmente, corresponde a una serie de scripts o tareas que normalmente suelen leer en las fases de datos y cargan toda esa información en la memoria. 
- En general, se utiliza Python, pero también SQL. 
- Esos datos son leídos para posteriormente ser transformados. 
- Te encontrarás diferentes tablas en una base de datos y dichas tablas pueden tener valores nulos. 
- Como es necesario unir todas esas tablas, debes corregir todos esos valores nulos y otros problemas antes de hacer una carga en Data Warehouse, ya que una vez dado este paso, no se permiten ediciones.

Al trasladar todos estos datos al Data Warehouse, ya están listos para ser consumidos por especialistas de todo tipo, ya sean analistas, CEOs o CTOs. 

#### ETL y ELT | Diferencias
ELT (Extract, Load & Transform) se ha puesto bastante de moda por el bajo precio del almacenaje durante estos últimos años, así como por la potencia de cómputo de los Data Warehouse.
- Lo que se hace es extraer datos, como en la ETL, pero la transformación se realiza dentro del propio Data Warehouse. 
- Se utilizan diferentes herramientas, pero ahora mismo la que está más en auge es DBT, que utiliza el lenguaje SQL para conseguir este tipo de transformaciones. 
- Por último, se carga otra vez en el mismo Data Warehouse.

#### Data Warehouse y Base de Datos | Diferencias
Podemos encontrar tablas, tanto en un Data Warehouse, como en una base de datos tradicional. Lo que realmente las diferencia son las operaciones que puedes realizar sobre ellas. 

Normalmente, se usa un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, inserciones y borrados de forma continua. 

Sin embargo, en el Data Warehouse todas estas operaciones no están permitidas:
- **No se permite la edición de filas.** Realmente no se hace una edición, si lo que se hace es añadir fila. 
- **No se permiten los borrados de forma habitual,** aunque en algunas tecnologías sí que se permiten. 
- **El coste por almacenamiento en un Data Warehouse es mucho menor** que en una base de datos tradicional.

#### Data Lake
Un data lake permite almacenamiento masivo, pero los datos están totalmente desestructurados. 

Es ideal para guardar información que nos llega en forma de eventos, que pueden tener cualquier estructura interna y pueden ser totalmente desordenados.

#### ETL y ELT | Casos de uso
No existe tanta diferencia entre lo que podemos llegar conseguir utilizando cualquiera de estas dos Pipelines. 

Al final siempre estamos cargando los datos a un sitio para que se sean consumidos. 

Lo que hay que valorar es: 
1. Esfuerzo económico 
2. Tiempo de desarrollo 
3. Tiempo de reacción al cambio 

Tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda y todos esos datos llegan a nuestra Pipeline.
- Son extraídos y, aquí viene la gran diferencia, ¿qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente?. 
- Si cargamos estos datos, tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no nos esperamos, que puedan contener un campo que no es correcto o tienen un campo que realmente no está dentro del esquema que nosotros estamos buscando.
- Entonces, una buena medida es transformarlos antes de cargarlos, simplemente para asegurarnos de no tener ese tipo de errores. 
- Pero al mismo tiempo, también queremos almacenarlos en un sitio, ahorrarnos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar. 
- Entonces, es un tema económico y también de potencia de cómputo y de rapidez de la gestión del dato, que diferentes negocios pueden valorar de diferentes maneras. 
- Ahora mismo el modelo ELT es más barato que el ETL.

#### El Caso de Internxt
Empezamos utilizando un modelo ETL, pero rápidamente nos dimos cuenta de que tenía mucho sentido migrar a un Pipeline tipo ELT, básicamente porque resultaba mucho más económico para todas las gestiones que queríamos hacer.

La potencia de las nuevas tecnologías como DBT junto a [BigQuery](https://cloud.google.com/bigquery/?utm_source=google&utm_medium=cpc&utm_campaign=emea-be-all-en-dr-bkws-all-all-trial-e-gcp-1011340&utm_content=text-ad-none-any-DEV_c-CRE_574561847454-ADGP_Hybrid%20%7C%20BKWS%20-%20EXA%20%7C%20Txt%20~%20Data%20Analytics%20~%20BigQuery%23v6-KWID_43700072681098079-kwd-12297987241-userloc_2056&utm_term=KW_big%20query-NET_g-PLAC_&gclsrc=ds&gclsrc=ds&gclid=CJrW0POoxPwCFWdCHQkdj-8PxQ) (en este caso que usamos BigQuery) era una combinación que prácticamente toda la industria Data estaba utilizando y la compañía también se estaba moviendo a desarrollar nuevos conectores, documentación que nos podía ayudar a poder distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente el Pipeline que teníamos.

No solo eso, también teníamos un problema: Nuestra extracción de datos estaba bastante acoplada a directamente guardar en BigQuery.

> Entonces, almacenar primero y luego transformar tenía mucho sentido. El primer paso de un Pipeline de data siempre es la extracción.

##### Otras fuentes de datos
- La mayor parte de veces, la extracción se genera desde bases de datos, pero no siempre es el caso. También se pueden extraer de acciones que realizan los propios usuarios, como clics, heatmaps, puede ser cualquier cosa. 
- La extracción desde este tipo de fuentes también es importante. 
- Hoy en día se utilizan CDPs para realizarlo y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, ya que, muchas veces, se pierden eventos e información por el camino. Entonces, el challenge es tener un sistema robusto. 

Durante la extracción de datos es importante tener un esquema prefijado, esto nos ayudará a que luego el dato en nuestro Data Warehouse esté reflejado de una manera clara para el resto del equipo. 

Entonces, antes de la ingesta de datos, se establece un protocolo: 
1. Esperar con estos datos, campos y tipos, y, cuando pase por nuestro Pipeline, todo lo que no entre dentro de ese marco, lo podemos descartar o hacer una transformación para que entre dentro de ese marco.
2. En el mundo real pueden suceder un millón de cosas; estamos sujetos a caídas de diferentes sitios o servicios o a posibles ataques. Entonces, tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. 
3. Se suele decir que el 70%-80 % del trabajo de un Científico de Datos es limpiar esos datos, por eso no es una cuestión trivial. 

Cuando llegamos a la visualización del dato, muchas veces, el equipo de data tiene que reportar al equipo directivo o a otro departamento. Es importante tener claras qué herramientas de visualización se van a utilizar y sobre todo, qué implica. 

¿Cuánto tiempo de trabajo implica utilizar esas herramientas? La visualización tampoco es trivial. Muchas empresas tienen un departamento exclusivo para ello y tenemos que optar un poco en detener un equipo para ello o usar una herramienta. 

Vas a tener que hacer consultas de SQL, vas a tener que tener algún experto en usabilidad y que pueda decir: Aquí van estos colores, aquí estos otros, y aquí quiero presentar la información de esta manera o de esta otra manera. Y tienes que tener claro cuáles son los gastos que vas a tener que hacer en los diferentes casos.

Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redis que te permiten hacer más cosas, pero son mucho más costosos y necesitas probablemente un equipo que esté dedicado a ello.

## U3. Análisis Avanzado
### Análisis Avanzado (Video)
![[478.E1.U3.1_Análisis_Avanzado.mp4]]
[Análisis Avanzado](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris)

Vamos a hablar de varios conceptos clave para el correcto funcionamiento de nuestro proceso de desarrollo de algoritmos. Es decir, son aquellos conceptos que siempre debemos revisar antes de comenzar a entrenar nuestro algoritmo o cuando observamos que la precisión no es la adecuada. Siempre tendremos que volver a revisar si cumplimos con esta serie de características. Para ello, vamos a intentar analizar punto por punto en qué debemos prestar atención.

El primero, como ya hemos mencionado anteriormente, sería el concepto de los valores faltantes (missing values). Como hemos dicho, el primer punto sería visualizar nuestro conjunto de datos (dataset). Al visualizar o solicitar información, dependiendo del framework o del lenguaje de programación que estemos utilizando, lo que haremos será consultar las estadísticas de este conjunto de datos para que nos indique de una forma científica cuántos valores faltantes tenemos. No se trata de comprobarlo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de valores faltantes tenemos en nuestras columnas o variables. Aquí lo importante, una vez que los detectamos, es saber qué estrategia seguir. ¿Cómo manejamos esta situación? Las estrategias a seguir se pueden dividir en dos grupos: la primera sería eliminar y la segunda, reemplazar.

Aquí hay que hablar de dos escenarios muy diferentes. Uno es cuando tenemos una gran cantidad de datos, estamos hablando de 300,000 o 400,000 filas; en este caso, de alguna forma, estamos sobrados. Entonces, si tenemos un porcentaje de valores faltantes, digamos que afecta a un 20% de las filas, el hecho de que las eliminemos no va a importar demasiado. Pero existe otro escenario en el cual tenemos pocos datos. Imaginemos que tenemos 3,000 o 4,000 filas. Si aquí estamos hablando de eliminar filas donde hay valores faltantes, tendríamos un problema, porque si ya íbamos justos, estaríamos yendo aún más justos, lo cual puede afectar la performance de nuestro algoritmo. Por tanto, esta sería la primera medida: eliminar filas, decisión que tomaríamos en base a nuestra cantidad de datos.

Otra estrategia sería si vemos que una variable tiene una gran cantidad de valores faltantes, por ejemplo, un 40% de las filas. En este caso, lo mejor es eliminarla porque no habrá forma de reemplazar estos valores. Esto nos lleva a hablar de la segunda estrategia, que sería reemplazar. Esta estrategia la aplicaríamos cuando estamos en el escenario de que tenemos pocos datos. Por tanto, lo que tenemos que hacer es exprimir al máximo estos datos. ¿Reemplazar? Se puede reemplazar por la media, se puede reemplazar por la mediana, se puede comenzar con una constante o hay otro tipo de estrategias. Por ejemplo, si en un conjunto de datos un factor muy importante es la edad, y resulta que tenemos muchos valores faltantes en la variable edad y no sabemos la edad de una cantidad considerable de usuarios, pero imaginemos que también tenemos información sobre sus estudios o su ocupación.

Imaginemos que estamos trabajando con un conjunto de datos sobre estudios universitarios y tenemos información sobre lo que han estudiado o a qué se han dedicado después. Con esta información, podríamos suponer, en base a la ocupación o al nivel de estudios, el rango de edad. No es necesario que tengamos la edad exacta; podemos determinar si esta persona, a lo mejor, está en primaria, secundaria, estudios universitarios, trabajando o jubilada. Aquí tendríamos cinco grupos, y lo que estaríamos haciendo es crear una nueva columna donde podríamos transformar el parámetro de edad. De esta forma, podríamos resolver el problema de los valores faltantes. En definitiva, yo lo resumiría con una frase: comprender el contexto y ser creativo.

El segundo concepto clave es la validación cruzada (cross-validation). La validación cruzada puede ser un concepto al principio un poco confuso. A mí me gusta representarlo y visualizarlo como el concepto de mezclar las cartas. Cuando jugamos a un juego de cartas y finaliza una partida, lo que hacemos es mezclar las cartas. Ahí lo que estamos buscando es heterogeneidad. Imaginemos que hemos terminado una partida y los cuatro ases o tres ases están sobre la mesa, porque seguimos una escala. Si simplemente juntamos las cartas, lo que sucederá es que estarán los ases juntos, los caballos juntos; es decir, habrá homogeneidad. Cuando jugamos a cartas, necesitamos mezclar, heterogeneidad.

¿Y por qué es importante el concepto de validación cruzada? Recordemos que hemos hablado de que partimos en entrenamiento y en test, 80-20 o 70-30. ¿Pero qué ocurre? Vamos a pensar sobre este concepto de entrenamiento y test. Imaginemos que intentamos hacer un algoritmo predictivo, un clasificador que quiere predecir si un paciente va a sufrir coronavirus o no, en base a ciertos datos sociológicos y de historial médico. Vamos a visualizar cómo capturamos los datos, ya que hemos pasado por procesos de vacunaciones masivas. Imaginemos que cogemos un centro sanitario de una capital y medimos diversos parámetros médicos de estos usuarios.

Imaginemos que hacemos una convocatoria masiva porque necesitamos personas de todas las edades. Primero llega un autobús de un parvulario, llegan todos los niños y se les toman todos los parámetros médicos, que se almacenan en una base de datos. Luego llega un instituto, y lo mismo ocurre con todos los alumnos. Después llega gente de una empresa y, finalmente, otro autobús con personas de una residencia de ancianos. ¿Qué ocurriría aquí? Imaginemos que hacemos un 80-20 para entrenamiento y test. ¿Qué pasa? El primer autobús que ha llegado es el de los niños de preescolar y el último es el de los ancianos. Ahora, si partimos el 80-20, probablemente dejaremos fuera el autobús de los ancianos.

¿Qué ocurriría? Que este algoritmo que estamos entrenando con unos datos tiene un sesgo porque no ha visto casos de personas ancianas y no ha considerado un rango de edad. Por lo tanto, aquí hay un sesgo. ¿Qué deberíamos hacer? Mezclar las cartas de forma que en este 80% de entrenamiento tengamos todos los rangos de edad mezclados y en el 20% de test tengamos ejemplos de todos los rangos de edad. Este es el concepto de validación cruzada y se basa en el concepto de k, que son el número de particiones que hacemos. Es una forma de evitar sesgos porque, al final, no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos.

El siguiente concepto clave es el sobreajuste (overfitting), que es un concepto que a priori es bastante abstracto, pero tiene mucho sentido. Como hemos dicho, los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. El sobreajuste simplemente significa que han aprendido demasiado de esos datos del pasado, que se han ajustado demasiado a esos datos. El objetivo central de un algoritmo es poder generalizar bien en sus predicciones. No es importante que realmente tenga una precisión súper elevada; es importante que sepa generalizar bien.

Es decir, ¿por qué? Para que no haga predicciones aberrantes o incoherentes. En el caso del sobreajuste, imaginemos un ejemplo visual muy sencillo: imaginemos que vais a haceros un vestido a medida con un sastre. Este sastre os mide vuestras dimensiones corporales y no deja ninguna holgura; simplemente construye un traje totalmente ceñido a vuestro cuerpo, porque esas son vuestras dimensiones. Esto nos haría caminar rígidos; siempre dejamos una holgura, un margen de confianza. Cuando hablemos del sobreajuste, hay que empezar a pensar en algo rígido, en algo que no generaliza bien, que no nos deja margen, que no nos deja tolerancia.

Ahora imaginemos que vais a comprar una cama. Es cierto que en una cama algunos van a dormir de lado, otros hacia arriba y otros boca abajo. Fijémonos en que una cama es rectangular y nos deja un margen, una tolerancia. Fijémonos en la gráfica de la derecha y veremos un ejemplo clásico de sobreajuste. Estamos hablando de que, al final, los datos son una nube de puntos. ¿Qué ocurre con un punto hasta aquí, otro hasta aquí, otro hasta aquí? Cuando un algoritmo aprende con sobreajuste, lo que está ocurriendo es que va prediciendo cada punto aprendido; se ha entrenado de forma demasiado exacta. No nos interesa que se ajuste a cada punto; nos interesa que pase por la media de los puntos. Esto se relaciona con el concepto de media y varianza; es decir, nos interesa que nuestro algoritmo no haga una predicción exacta de cada punto, sino que esté en la media correcta de todos los puntos.

¿Por qué? Porque esto lo que nos permitirá es que sepa generalizar bien. Si un algoritmo se entrena con sobreajuste, lo que ocurrirá es que, en el momento en que tenga que hacer la predicción de ciertos valores que se salen un poco de este patrón, no realizará predicciones muy acertadas. Dicho de una forma sencilla, puede pasar de predicciones del 99% a predicciones del 55%. Esto no nos interesa, ya que se ha entrenado en sobreajuste y se ha ajustado demasiado a los datos históricos. Por tanto, cuando ve un dato nuevo extraño, no sabe cómo comportarse. Nos interesa más que el algoritmo tenga una precisión más estable, por ejemplo, del 85% o 89%, pero que cuando vea casos atípicos sepa generalizar bien.

Es decir, cuando hablamos de sobreajuste, estamos hablando en Machine Learning y en Inteligencia Artificial de que el 100% no existe, por lo cual siempre asumiremos un error. Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y que al final permita generalizar de forma coherente, porque el error ya es asumido. La pregunta es: ¿cómo detectar el sobreajuste? Se detecta cuando, al entrenar el algoritmo, los resultados que arroja sobre los datos de entrenamiento muestran precisiones muy elevadas, estamos hablando del 99%. Por tanto, aquí debemos prestar atención a que nuestro algoritmo se ha ajustado demasiado a los datos, lo que significa que no será flexible ante datos nuevos. Aquí es donde debemos iterar, revisar y aplicar técnicas para evitar el sobreajuste.

¿Qué técnicas nos ayudarán a evitar este sobreajuste? Lo primero es la validación cruzada, como hemos mencionado, partir en un número de particiones y mezclar las cartas. También podemos aplicar aumento de datos (data augmentation), que es una técnica que aumentará de forma artificial nuestra cantidad de datos. Podríamos probar también otras técnicas como el dropout o el early stopping, que al final buscan heterogeneidad. El dropout consiste en dejar fuera ciertos datos de forma aleatoria; al final, lo que buscamos es tener esta heterogeneidad y que los algoritmos aprendan de forma correcta.

Otro concepto clave sería el concepto de valores atípicos (outliers). En primer lugar, hay que mencionar que debemos hacer una buena práctica para detectar un valor atípico. Para ello, lo que tenemos que hacer es visualizar y graficar nuestros datos. Un valor atípico es un punto de datos que tiene un valor muy atípico, es decir, que se desvía mucho de la media. Intentemos pensarlo en el mundo real. Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce que venda todo tipo de productos, similares a los que puede tener Amazon. Capturaremos la base de datos y comenzaremos a analizar los valores.

Resulta que vemos que, imaginemos, que todos los días entre semana se venden de media, y nos encontramos que un día, un viernes de un mes concreto, ese día se vendieron diez veces más. Lo primero que haríamos sería visualizar ese dato, detectarlo y comprobar si ese valor es real o no, porque podría pasar que, en lugar de mil, en la base de datos se ha insertado manualmente diez mil. Entonces estaríamos hablando de una anomalía, y habría que corregirlo. Pero resulta que hacemos las comprobaciones, hablamos con el cliente y nos dice que sí, que efectivamente ese día se vendieron diez mil unidades.

A nivel de variables, diríamos: ¿qué pasó ese día? ¿Había una promoción? No. Imaginemos que el hecho que provocó esto fue que en Amazon se terminaron las existencias de un producto muy demandado. Por tanto, como en Amazon se terminó, los clientes llegaron hacia nosotros. Este hecho ha sucedido y, obviamente, es importante. Pero si pensamos en términos matemáticos y nos imaginamos esa gráfica, y estamos hablando de valores de mil o cercanos a los 1,900, 1,100, 1,200 y, de repente, 10,000, este valor sucedió, pero recordemos que un algoritmo puede ser visto de una forma abstracta, conceptual, como una línea recta o curva que tiene una fórmula matemática.

¿Qué ocurre? Que esta línea se distorsionaría enormemente por este punto, por este valor tan elevado. Entonces, es aquí cuando hay que volver al concepto de generalizar. Como queremos que generalice bien, en este caso es más útil eliminar el valor atípico porque generará una distorsión en todo el conjunto. Es aquí donde es muy importante remarcar la necesidad de entender la naturaleza del problema que estás abordando. Pero no siempre es la mejor estrategia eliminar los valores atípicos; es decir, tenemos que entender, como hemos dicho varias veces, la naturaleza del problema.

Por ejemplo, en los casos de predicción de los mercados de valores, en el caso de acciones, en una bolsa de valores o en casos de criptomonedas, los valores atípicos son muy importantes; es decir, hay que prestarles mucha atención. De hecho, las métricas que hay que utilizar en estos casos son métricas que están preparadas para prestar especial atención a los valores atípicos. ¿Por qué? Porque en la gráfica, por ejemplo, de un proyecto de criptomonedas, de un activo digital, los valores atípicos son muy importantes: picos y valles potentes que, al final, indican que ese día ocurrió algo significativo. Por tanto, respecto a los valores atípicos, hay que analizar la naturaleza del problema para determinar la estrategia que seguiremos.

Otro concepto muy importante es la normalización y estandarización, también conocido como escalado de características (feature scaling). Como hemos comentado anteriormente, podemos trabajar con múltiples variables. Estas pueden ir desde 4 hasta 10, 12 o incluso hasta 40. Aquí es muy importante entender que estas variables se encuentran en diferentes escalas. Por ejemplo, si estamos hablando de productos del sector inmobiliario, podríamos decir que la variable euros puede ir desde 0 hasta 4 millones de euros, o 10 millones de euros, por ejemplo, pero la variable cuartos de baño puede ir desde 0 hasta 5 como máximo.

¿Qué tenemos aquí? Escalas muy diferentes, escalas muy desproporcionadas. Entonces, la normalización y estandarización lo que pretende, de una forma muy sencilla y conceptual, es reducir esta escala manteniendo la proporción. Lo que vamos a hacer al normalizar es transformar todos estos valores para que estén entre el rango de 0 y 1. ¿Y por qué vamos a hacer esto? Porque al final estamos hablando de que el algoritmo, para que lo visualicemos conceptualmente, es una curva o una recta, es una gráfica, y lo que vamos a hacer es facilitarle las cosas, es decir, que entienda de una forma mucho más ordenada estos valores.

Al final, lo que ocurre es que, cuando aplicamos una normalización, lo único que sucederá es que, en lugar de ver 8 millones de euros, veremos 0.9, o en lugar de ver 100 mil euros, veremos 0.1. Este es el cambio que apreciaremos, pero lo que lograremos es que, durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. También hay que recordar que el proceso de normalización hay que hacerlo y deshacerlo, porque después, a la hora de interpretar resultados, hay que recordar que tenemos todos estos valores en esta escala.

La estandarización es un proceso similar. Lo único es que se utiliza la estandarización cuando estamos resolviendo problemas que asemejan la distribución de los datos a la campana de Gauss. De esta forma, simplemente colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta -1. Podemos aplicar una, podemos aplicar las dos, pero, en general, es una buena práctica intentar llevar los datos a la escala correcta cuando observamos grandes diferencias de escala o magnitud.

Y, por último, otro concepto muy importante que podríamos denominar codificación one-hot (one-hot encoding) es de suma importancia porque hay que recordar que podemos tener variables. Hemos visto tipos de datos; podemos tener timestamp, podemos tener enteros, podemos tener double, que son al final números, pero también podemos tener strings. Imaginemos que, en el ejemplo inmobiliario, estamos hablando de la ubicación de una vivienda. Puede ser centro, puerto, periferia o zona montañosa. Aquí tenemos palabras; puede haber cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo, lo que necesitan los algoritmos son realmente números, son ceros y unos. Al final, todo se basa en esto, por lo cual no nos va a admitir palabras.

El concepto de One-Hot Encoding lo que hace es que, si una variable puede tener cinco valores, por ejemplo, si puede ser playa, montaña, etc., la desplegará en cinco. Es decir, playa, montaña, periferia, centro; la variable ubicación se convertirá en cinco columnas, en cinco posibles valores, y a cada fila le asignará un 1 o un 0. Imaginemos que la siguiente casa está en la montaña. En la variable ubicación playa tendrá un valor 0, en ubicación montaña tendrá un valor 1 y en el resto tendrá un valor 0. Al final, lo que hacemos es desplegar. El punto clave aquí es saber cuántos valores puede tomar nuestra variable; si son 5, se transformará una variable en 5 variables. De esta forma, le permitiremos al algoritmo trabajar de forma óptima, ya que muchos de ellos solo aceptan números.

### Desarrollo del Algoritmo
Vamos a hablar de varios conceptos claves para el correcto funcionamiento del proceso de desarrollo de nuestro algoritmo.

Son aquellos conceptos que siempre tenemos que revisar antes de comenzar a entrenar nuestro algoritmo. Cuando observamos que la precisión no es la correcta, tendremos que volver a revisar si cumplimos esta serie de características. 

Para ello, vamos a intentar analizar punto por punto dónde tenemos que prestar más atención. 

![[479.E1_Desarrollo_del_Algoritmo_1.png]]

#### 1 | Missing Values
El primer paso sería visualizar nuestro Dataset, consultar las estadísticas que nos da para que nos diga de una forma científica cuantos Missing Values tenemos. 

No se trata de ir comprobándolo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de Missing Values que tenemos en nuestras columnas o en nuestras variables. 

Aquí lo importante es que, una vez las detectamos, saber qué estrategia seguimos y como manejamos esta situación.

Una de las estrategias a seguir, dividirlas en dos grupos. 
- Grupo 1: Eliminar
- Grupo 2: Reemplazar 

 ##### 1 | Eliminar
Aquí hay que hablar de dos escenarios muy diferentes. 
- **Cuando tenemos muchísimos datos, p. ej. 300k / 400k filas.** Si tenemos un porcentaje de Missing Values de un 20%, El hecho de que las eliminemos no va a importar demasiado. 
- **Cuando vamos justos de datos: 3k o 4k filas.** Si estamos hablando de eliminar filas donde Missing Values estaríamos teniendo un problema, porque si íbamos muy justos, estamos siendo aún más justos. Con lo cual esto puede tener una afectación en la performance de nuestro algoritmo. Por tanto, esta sería la primera medida, no eliminar filas que decidiríamos con base en nuestra cantidad de datos. 
- **Si vemos que una variable tiene una gran cantidad de Missing Values.** Realmente lo que está afectando a un 40% de los datos, un 40% de las filas, Lo mejor es eliminarlas, porque no va a haber forma de reemplazar estos valores.

##### 2 | Reemplazar
Esa estrategia la vamos a aplicar en el supuesto escenario de que tengamos pocos datos. 

Tenemos que hacer es exprimir al máximo estos datos. 
- Se puede reemplazar por la media. 
- Se puede reemplazar por la mediana. 
- Se puede reemplazar con una constante.
- Etc…

Hay otro tipo de estrategias, por ejemplo: 
1. En un dataset, un factor muy importante es la edad. 
2. Tenemos Missing Values en la variable edad, que tenemos muchos y no sabemos la edad de una cantidad muy grande de usuarios. 
3. Pero sí que tenemos su nivel de estudios y su ocupación actual. 
4. Podríamos interpolar o averiguar un poco el rango de edad, no tenemos la edad exacta, pero podemos determinar si esta persona está en primaria, secundaria, en estudios universitarios, trabajando o jubilada. 

Aquí tendríamos ya cinco grupos y aquí lo que estaríamos haciendo es crear una columna nueva donde podemos también el parámetro de edad, transformarlo y de esta forma podríamos salvar el problema de los Missing Values. 

> Hay que comprender el contexto y ser creativo.

![[479.E1_Desarrollo_del_Algoritmo_2.png]]

#### 2 | Cross Validation
Podemos representarlo como el concepto de mezclar las cartas. 

Cuando jugamos a un juego de cartas y cogemos una baraja, lo que hacemos es mezclarla, estamos buscando heterogeneidad. Imaginaros que hemos terminado la partida y los cuatro están sobre la mesa porque seguimos una escala homogénea.

Si nosotros simplemente las cartas las juntamos, lo que nos pasará es que estarán los ases y los caballos juntos, es decir, habrá una homogeneidad. 

Cuando jugamos a cartas, necesitamos mezclar y buscar heterogeneidad. Recordad que hemos hablado que nosotros partimos el entrenamiento en test 80/20 o 70/30. 

Vamos a poner el foco en lo que es el concepto de la captura del dato. 

#### Ejemplo: Algoritmo predictivo 
Queremos saber, basándonos en ciertos datos sociológicos y de historial médico, intentar predecir si un paciente va a sufrir o vamos coronavirus o no. 

Vamos a visualizar cómo capturar los datos, ya que hemos pasado por procesos de vacunaciones masivas. Imaginamos que cogemos un centro sanitario de una capital y ahí lo que hacemos es a medir diversos parámetros médicos de estos usuarios. 

Hacemos una convocatoria masiva porque necesitamos personas de todas las edades, todo el rango de edades.
1. Llega primero un autobús de un parvulario, llegan todos los niños y se les cogen todos los parámetros médicos y se almacena en una base de datos del cero al 50. 
2. Llega a un instituto y aplicamos el mismo protocolo a todos los alumnos. 
3. Llega un autobús de una empresa. 
4. Llega otro autobús con gente de una residencia de ancianos. 

Hacemos un entrenamiento, test 80/20 con todos estos datos.
- El primer autobús que ha venido es el de los niños de preescolar y el último es el de los ancianos. 
- Si partimos el 80/20, probablemente nos vamos a dejar fuera el autobús de los ancianos. 
- Este algoritmo, que lo estamos entrenando con unos datos, tiene un sesgo porque no ha visto casos de gente anciana, un rango de edad. Por lo tanto, aquí hay un sesgo. 

¿Cuál sería la solución? Mezclar las cartas, mezclar los 4 autobuses. De forma que en este 80% de entrenamiento tendríamos todos los rangos de edad mezclados.

En el rango del 20% de test tendríamos ejemplos de todos de test de todos los rangos de edad. 

Este es el concepto de Cross Validation y se basa en el concepto de K, que son el número default o el número de particiones que hacemos. 

Es una forma de evitar sesgos, porque al final no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos.

### Overfitting y Outliers
A priori, puede parecer conceptos bastante abstractos, pero tienen mucho sentido… ¿Qué son y para qué sirven?

#### Overfitting
Los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. Simplemente, significa que pueden haber aprendido demasiado de esos datos del pasado.

El objetivo principal de un algoritmo es poder generalizar en sus predicciones, no es importante una precisión súper elevada, sino que sepa generalizar bien para que no haga predicciones de aberraciones o de cosas incoherentes.

Veamos un ejemplo Overfitting muy sencillo:
- Vamos a un sastre para hacernos un traje a medida. 
- Este sastre mide las dimensiones corporales exactas y no deja ninguna holgura, simplemente, construye un traje totalmente ceñido a tu cuerpo, tus dimensiones. 
- Esto nos haría caminar rígidos.

Cuando hablamos de Overfitting, empezamos a pensar en ese cuerpo rígido con el traje sin tolerancia alguna.

Imaginemos ahora que los datos son una nube de puntos. ¿Qué ocurre cuando un algoritmo aprende con Overfitting? 
- Lo que está ocurriendo es que va prediciendo cada punto aprendido, ha entrenado de forma demasiado exacta.
- De hecho, no nos interesa que se ajuste cada punto. Nos interesa que pase por la media de los puntos, está un poco ligado al concepto de media y varianza.
- Queremos que nuestro algoritmo no haga una predicción exacta de cada punto, sino que esté en la media correcta de todos los puntos porque nos va a permitir generalizar correctamente.

> Si un algoritmo se entrena con Overfitting, en el momento que hace una predicción y ciertos valores se salen un poco de este patrón, va a realizar predicciones nefastas, puede pasar del 90% al 55% y esto no nos interesa, se ha ceñido demasiado a los datos históricos.

- Por tanto, cuando ve un dato nuevo, extraño, no sabe cómo comportarse. 
- Nos interesa más que el algoritmo tenga una precisión más estable, por ejemplo, entre el 85% y el 89%, pero cuando vea casos atípicos, generalice bien.
- En Machine Learning e Inteligencia Artificial, el 100% no existe, con lo que estamos hablando de que siempre vamos a asumir un error. 
- Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y que, al final, permita generalizar de forma coherente porque dicho error ya está asumido.

![[480.E1_Overfitting_y_Outliers_2.png]]

##### ¿Cómo detectar el Overfitting?
Lo podemos detectar en los datos, cuando se entrena el algoritmo. Si en los entrenamientos arroja precisiones muy elevadas (99%), tenemos que prestar atención, pues nuestro algoritmo se está ajustando demasiado a los datos, por lo tanto, no va a ser flexible ante los nuevos datos.

En este punto, debemos iterar, revisar y aplicar técnicas para poder evitar el Overfitting.
- Cross Validation, mezclar las cartas. 
- Data Aumentation, que sería aumentar, de forma artificial, nuestra cantidad de datos.
- Drop Out. Dejar fuera ciertos datos de forma aleatoria.
- Early Stopping.

Al final lo que buscamos es tener Heterogeneidad y que los algoritmos aprendan de forma correcta.

#### Outliers
En primer lugar, hay que tener una buena praxis para detectar un Outlier y, para ello, lo que tenemos que hacer es visualizar y pintar nuestros datos. 

Es un metapoint que tiene un valor muy atípico, es decir, que se desvía muchísimo de la media de temas.
- Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce que venda todo tipo de productos similares a los que puede tener Amazon.
- Capturamos los datos, cogemos la base de datos y empezamos a analizar los valores.
- De lunes a viernes venden, de media global, 1000 productos.
- De repente, un viernes de un mes concreto, se vendieron 10000, hay un x10.

**Lo primero que haremos es visualizar ese dato, detectarlo y comprobar si ese valor es real o no,** porque puede pasar que en lugar de 1000, la base de datos haya insertado manualmente 10000, entonces estaríamos ante una anomalía que habría que corregir.
- Pero hacemos las comprobaciones, hablamos con el cliente y resulta que sí, que, efectivamente, ese día se vendieron 10000 unidades.
- A nivel de variables diríamos que lo que sucedió ese día es que había una promoción, pero el hecho que provocó esto fue que en Amazon, ese mismo día, se acabaron las existencias de un producto determinado. Los clientes llegaron hacia nosotros.
- Este hecho es, obviamente, importante, pero si pensamos en términos de gráficas, de que estamos hablando de valores de 1000 o cercanos y, de repente, 10000.
- Un algoritmo puede ser visto en forma abstracta o conceptual, como una línea recta o curva que tiene una fórmula matemática. 
- Esta línea nos la distorsionaría de forma abrupta con este valor tan elevado.

Entonces es aquí cuando hay que volver al concepto de generalizar. 

Como queremos que generalice bien, en este caso, es más útil eliminar el Outlier porque va a crear una distorsión en todo el conjunto. Es muy importante remarcar que debemos entender la naturaleza del problema que estamos abordando.

![[480.E1_Overfitting_y_Outliers_1.png]]

Pero, ==eliminar el Outlier no siempre es la mejor estrategia==.

Por ejemplo, la naturaleza del problema en los casos de predicción de los Stock Market, de acciones en la Bolsa de Valores o en los casos Cripto, los Outliers son realmente muy importantes y hay que prestarles muchísima atención.

De hecho, las métricas que hay que utilizar en estos casos, son métricas que están preparadas para prestar especial atención a esto porque en la gráfica, esos Outliers, esos picos y valles, son datos muy potentes porque al final están hablando de que ese día ocurrió algo especial, importante, por tanto, hay que analizar la naturaleza del problema para ver la estrategia a seguir.


###  Feature Scaling: Normalización y Estandarización
Como hemos comentado anteriormente, podemos trabajar con múltiples variables que pueden ir desde 4 hasta 10, 12 o hasta 40. 

**Es muy importante entender que estas variables se encuentran en diferentes escalas.**
- Si estamos hablando de productos del Real Estate o del sector inmobiliario, podríamos hablar que la variable Euros (€) puede ir desde cero hasta 4 millones de € o 10 millones de €.
- En cambio, la variable a cuartos de baño puede ir desde 0 hasta 5 como máximo.

Aquí nos encontramos escalas muy diferentes y muy desproporcionadas. Lo que pretende normalización y estandarización, de una forma muy sencilla y más conceptual, es reducir esta escala manteniendo la proporción.

#### Normalización
- Al normalizar es todos estos valores vamos a transformarlos para que estén entre el rango de 0 y 1. 
- ¿Por qué vamos a hacer esto? Porque estamos hablando de que el algoritmo, para que lo visualicemos conceptualmente, es una curva o es una recta, una gráfica. 
- Lo que vamos a conseguir es facilitarle las cosas, es decir, que entienda estos valores de una forma más ordenada.

Cuando aplicamos la normalización, lo único que va a suceder es que en lugar de ver 8 millones de €, vamos a ver 0.9, o en lugar de 100.000 €, vamos a ver 0.1. 

Este es el cambio que vamos a apreciar nosotros. Pero lo que vamos a hacer es que, durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. 

También hay que recordar que, el proceso de normalización, hay que hacerlo y deshacerlo, porque después, a la hora de interpretar resultados, hay que recordar que tenemos todos estos valores en esta escala.

#### Estandarización
La estandarización se utiliza cuando estamos resolviendo problemas que se asemejan a la distribución de los datos a [la campana de Gauss](https://es.wikipedia.org/wiki/Funci%C3%B3n_gaussiana). 
- Simplemente, colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta -1. 
- Podemos aplicar una o podemos aplicar las dos, pero bueno, por lo general es una buena praxis el intentar llevar los datos a la escala correcta cuando observemos muchísimas diferencias de escala o de magnitud.

#### One-Hot Encoder
Es de suma importancia, pues hay que recordar que podemos tener muchas variables. 

Podemos tener Timestamp, podemos tener Integer, podemos tener Double que son al final números. 

Pero también podemos tener strings. 

Pensemos en el ejemplo inmobiliario, en la posición o situación de una vivienda, que puede ser centro, puerto, periferia o zona montañosa. Aquí puede coger cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo lo que necesitan realmente los algoritmos son números, ceros y unos. 

  
|  | Playa | Montaña | Periferia | Centro |
| ------ | ------ | ------ | ------ | ------ |
| Casa Playa | 1 | 0 | 0 | 0 |
| Casa Montaña | 0 | 1 | 0 | 0 |
| Casa Periferia | 0 | 0 | 1 | 0 |
|  Casa Centro | 0 | 0 | 0 | 1 |

Lo que consigue es One-Hot Encoding es que si una variable, sí que puede tener 4 valores.
- Playa
- Montaña
- Periferia
- Centro

Por ejemplo, sí puede ser playa, montaña, etc., la va a desplegar en 4. La variable ubicación se va a convertir en sus 4 columnas, en 4 posibles valores.

Si la casa está en la playa, en la variable, en la columna en la columna Ubicación/Playa tendremos valor uno y en el resto valor 0. 

Imaginemos que la siguiente casa está en la montaña. En la variable Ubicación/Playa tendrá un valor 0 en Ubicación/Montaña tendrá un valor 1 y en el resto tendrá un valor 0. 

El punto clave aquí es saber cuántos valores puede coger nuestra variable. Si son 4, se va a transformar una variable en 4 variables. De esta forma le permitiremos al algoritmo poder trabajar de forma óptica, ya que, muchos de ellos, solo aceptan números.

## U4. Predecir el Precio de Bitcoin
### Predecir el Precio de Bitcoin (Video)
![[482.E1_Predecir_el_Precio_de_Bitcoin.mp4]]
[Predecir el Precio de BitcoinTitulo](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866107-u4-1-1-predecir-el-precio-de-bitcoin-jose-peris)

En la clase de hoy vamos a analizar un caso práctico que se llama "el caso de cómo desarrollar un algoritmo predictivo del Bitcoin". Vamos a ver todo el proceso de desarrollo y, sobre todo, vamos a estudiar paso a paso qué tácticas o decisiones tomar de cara a un desarrollo exitoso.

En primer lugar, como os he comentado en las otras clases, el primer objetivo es conocer el caso de negocio, analizar el contexto y entender la naturaleza del problema. Al final, básicamente, los datos que nos vamos a encontrar en el caso del Bitcoin van a ser un timestamp y un precio. Es una gráfica muy simple, pero claro, debemos entender qué hay detrás de esta gráfica, porque detrás de ella hay emociones de personas, sentimientos; pueden haber emociones como el miedo o el optimismo que impulsan a la gente a comprar o a vender.

Podéis ver, por ejemplo, este ejemplo que os muestro aquí en la pantalla, donde podéis observar todas las transacciones que se están ejecutando en tiempo real sobre la red de blockchain de Bitcoin, y esto os ayudará a entender de una forma gráfica el concepto de volatilidad. Al final, como os decía, detrás de todo esto, simplemente hay personas arrastradas por el optimismo, por el pesimismo o por la estrategia o táctica para obtener ciertos resultados. Por tanto, el primer factor es entender qué significa esta gráfica.

Para ello, un ejercicio muy interesante también es estudiar la gráfica del Bitcoin. Si os fijáis, en este esquema podéis apreciar que en cada cambio, en cada repunte, hay un hecho histórico detrás que ha marcado este cambio. Estas palancas son las que alteran lo que se llama el momentum, y es de vital importancia poder rastrear este tipo de palancas o eventos para poder predecir el cambio de tendencia. Esta, sin duda, va a ser la clave. La complejidad de este ejercicio radica en saber qué variables escoger, qué variables monitorizar y, finalmente, qué tipo de algoritmo escoger y qué experimentos realizar para obtener la mejor precisión posible, que es nuestro objetivo.

Lo primero que debemos entender es cuál es nuestro input y cuál es nuestro output. Básicamente, nosotros tenemos una gráfica con un timestamp y un precio, y lo que queremos es generar una gráfica predictiva que coincida, que se ajuste lo máximo posible, que calque o clone los movimientos, sobre todo a futuro, en una ventana temporal hacia adelante. ¿Para qué? Para tener la ventaja competitiva de saber cuándo va a subir o cuándo va a bajar el precio del Bitcoin.

Ya os aseguro que se han realizado muchos estudios sobre este campo. Realmente es muy complejo, es un ejercicio muy complicado, pero es muy interesante para analizar la relación causa-efecto que hay entre las diferentes variables. Y, por otro lado, hay algo también que debéis saber: no sabemos si hay algún tipo de algoritmo que una persona pueda utilizar para conseguir una precisión muy elevada, o si existe, hasta ahora no se ha publicado. Sí que es cierto que se publican muchísimos resultados, pero hay que saber leerlos y entenderlos, porque muchas veces, cuando se presentan resultados en papers, también se presentan sobre datos de entrenamiento o sobre datos de test que realmente no son demasiado fiables. Por lo cual, hay que tener presente esta forma de leer los papers y saber que siempre hay que indagar un poco más hacia lo que tenemos delante, que a primera vista puede parecer magnífico.

El primer punto, como os expliqué, es la recolección de datos, que es capturar los datos. En este caso, tenemos diferentes APIs; desde Yahoo Finance API, por ejemplo, podemos seleccionar todas las fechas que tenemos disponibles de los precios del Bitcoin, desde 2015 hasta la actualidad. Y hay que tener claro cuál es nuestro objetivo. Nuestro objetivo es predecir el precio de Bitcoin a un día, a cinco días y a 30 días. Es decir, tenemos tres escenarios: uno, cinco y 30. El reto es bastante potente y siempre hay que pensar en qué métricas obtenemos a un día, qué métricas obtenemos a cinco y qué métricas obtenemos a 30. Esa va a ser la conclusión de nuestro estudio, nuestro experimento.

El segundo paso en el proceso de creación de un algoritmo es siempre el estado del arte. Como hemos visto en anteriores lecciones, el estado del arte se basa en indagar y evaluar cuál es la última tecnología disponible para nuestro caso de uso. Como estamos hablando de una serie temporal y de una regresión, deberíamos observar todos aquellos experimentos publicados, sobre todo aquellos papers. Un paper, os explico aquí lo que es, es como una tesina; es un formato que se utiliza en la comunidad científica para mostrar resultados de una forma simplificada. Por ejemplo, si un equipo de investigación ha estado realizando estudios sobre un tipo de algoritmo para resolver un problema concreto, lo que hacen es exponer, por un lado, los impulsos que tenían, el problema que querían resolver, y por otro lado, explicar los resultados a los que han llegado y todos los experimentos que han realizado.

Esto es muy útil y permite a la comunidad de Machine Learning y de Inteligencia Artificial avanzar muy rápidamente, porque al final es un poco la sabiduría de la comunidad lo que permite poder avanzar de forma rápida. Por tanto, en el primer punto estaríamos hablando de una revisión de la literatura de problemas similares y observaríamos el estado del arte. En este caso, podríamos observar que tenemos diferentes caminos que se centran prácticamente en dos bloques: Machine Learning y Deep Learning. Sí que es cierto que se están haciendo muchos más experimentos, quizás con Transformers o con Reinforcement Learning, pero nos vamos a centrar básicamente en los dos focos principales y en los tipos de algoritmo que ya se han probado con cierto éxito para este tipo de predicciones.

El siguiente paso, una vez descargados los datos, es el más crucial, porque sería decidir qué tipo de variables afectan al precio del Bitcoin. Es decir, podemos buscar indicadores de todo tipo, como indicadores de momentum, como el RSI, o incluso podríamos hablar de medias móviles a 20, a 100 o a 200 días. Podríamos ver toda clase de indicadores técnicos financieros y, por otro lado, quizás descubrir e intentar discernir si están vinculados, por ejemplo, con el precio del oro o de otros mercados de valores y una correlación directa o indirecta. Por tanto, aquí se trata de coleccionar aquellos indicadores o variables que a priori pueden tener una correlación para, a partir de ahí, hacer una selección amplia y estudiar las correlaciones para ver con cuáles nos quedamos.

Si os fijáis, después de realizar una serie de experimentos, sobre todo basados en correlaciones, al final la correlación no es más que, explicado de una forma muy sencilla, observar si dos gráficas se parecen mucho y cuánto se parecen. Es decir, si se parecen mucho, estamos hablando de que la correlación siempre va desde 0 hasta 1. Si se parecen mucho, la correlación será de 1 o de 0.99. La correlación de 1 es prácticamente imposible, pero estaríamos hablando de 0.92 o correlaciones altísimas. Si no se parecen, tendrá un valor muy bajo. Si observáis las siguientes gráficas de correlación, de una forma muy sencilla, lo único que queremos ver es qué variables, qué gráficas coinciden con el precio del Bitcoin. Y no son tantas. Las más importantes que vemos son la hash rate, el miner revenue y Google Trends. Vamos a analizar cada una de las tres porque tienen correlación, y este es un ejercicio un poco de imaginación y lógica.

Hash rate. ¿De qué estamos hablando? El concepto hash está relacionado con la minería en una red de blockchain. Por tanto, ¿qué está diciendo esta gráfica? Está diciendo que cuando el precio del Bitcoin sube, lleva a que el valor se desvanezca. La misma correlación se observa con la tasa de hash que se está produciendo en la red de blockchain, es decir, con la creación de bloques. Por tanto, ¿qué quiere decir? Mucha creación de bloques influye directamente en que el precio esté subiendo o bajando. Por tanto, realmente no nos está aportando ningún valor, porque simplemente describe un mecanismo de funcionamiento de una red de blockchain. Es por eso que el contexto es tan importante. Necesitamos entender la naturaleza del problema.

La siguiente gráfica que tiene correlación con el precio del Bitcoin es el miner revenue. ¿Por qué tiene relación? Si comprendemos bien el funcionamiento de una red de blockchain, sabemos que un minero, a medida que va minando y resolviendo la proof of work o proof of stake, obtiene beneficios a cambio. Es decir, si se están produciendo muchas acciones de compra-venta, lo que está sucediendo es que la ganancia de los mineros tiene una relación directa; esto es también bastante obvio. Por lo tanto, son variables que realmente no aportan ningún valor. Pero, ¿qué ocurre si analizamos la gráfica de la correlación entre Google Trends y el precio del Bitcoin? ¿Qué está ocurriendo aquí? Pensad en cómo funcionamos también los seres humanos. Al final, lo que hacemos es imaginaros, ¿no? De momento, vemos que se está hablando muchísimo de Bitcoin en las noticias, en círculos sociales; te cuentan que alguien ha invertido y ha ganado dinero o ha perdido, da igual si es en positivo o negativo. Entonces, ¿qué ocurre rápidamente? En vez de... empezamos a ver qué precio tiene, qué está pasando con esta cripto, y rápidamente en Google Trends comenzamos a activar toda esta serie de búsquedas. Esto sí que tiene una correlación directa. Y sí que es una variable importante porque, al final, el mensaje que nos está diciendo es que cuando mucha gente a la vez está buscando Bitcoin, después ocurren cosas con el precio del Bitcoin. Aquí hablamos de un lapso de tiempo que puede ser para los traders a corto.

Bueno, esta ha sido una pequeña muestra previa de análisis de aquellas variables importantes, pero el experimento se ha realizado con una matriz de correlación. Si os fijáis, la matriz de correlación lo que está midiendo es desde 0 hasta 1 y desde 0 hasta -1 para medir correlación directa o correlación inversa. En la matriz de correlación, lo que intentamos buscar es la correlación directa o inversa y analizar este tipo de hechos que acabamos de comentar previamente. Es decir, hay que tener mucho cuidado porque, en este caso, fijaros que el hash rate y el miner revenue tienen una correlación muy elevada con el precio del Bitcoin, pero estarían añadiendo ruido a nuestro modelo, porque es simplemente redundancia. Es decir, es una consecuencia de que haya compra o venta de Bitcoin. Sin embargo, otras como Welltrend tienen una correlación alta, ya sea positiva o negativa, y sí que están hablando de indicadores de momentum, que es lo que estamos buscando. Por lo tanto, este proceso que se llama feature selection, que se hace con una matriz de correlación y es muy fácil de ejecutar con pocas líneas de código en Python, nos permite comenzar a descartar aquellas variables que aportan ruido y empezar a ver aquellas variables que sí que realmente son importantes.

Una vez hemos realizado la matriz de correlación, podemos ya empezar también a trabajar con aquellas variables con las que nos vamos a quedar, porque se trata de feature selection. Al final, lo que está indicando es que "feature" hace referencia a variables; es una selección de variables. Lo que queremos es reducir, porque en este caso hay tantos indicadores, hablando de en torno a 80 indicadores técnicos, y esto es demasiado, ¿no? Para el coste computacional sería demasiado elevado. Nuestro objetivo con el feature selection es reducir. Reducir y quedarnos con aquellas variables que realmente son importantes. Y otra forma de reducir es utilizar, por ejemplo, algoritmos como XGBoost, que al final trabajan con tecnología de árboles de decisión, pero con un gradiente mucho más potente, lo que nos permite obtener de alguna forma una clasificación de aquellas variables más importantes, de mayor o menor peso en la afectación al precio.

Por tanto, una buena práctica es utilizar la matriz de correlación y después utilizar feature selection mediante algoritmos como XGBoost. ¿Para qué? Para al final quedarnos con los indicadores técnicos más importantes. Es curioso si analizáis los indicadores ganadores que obtenemos; en este caso pasamos de 80 a 36, lo cual es una reducción potente, y la media móvil está en el top, es de los más potentes. Es cierto que muchas herramientas de trading se basan en medias móviles. Y funcionan; al final, las herramientas de trading lo que hacen es utilizar diferentes medias móviles de 5, de 10, de 20, de 200 días, y básicamente lo único que hacen es, en el momento que se solapan las diferentes medias móviles, estos son puntos donde te indican que puedes comprar o vender. Básicamente, este es el funcionamiento, pero veremos que no solo afectan estos indicadores, estas medias móviles, que hablan un poco más de la historia, del pasado, de lo que conocemos a través del pasado, sino que con Machine Learning es un enfoque bastante diferente a jugar solo con el pasado.

Otro factor importante, ya que con el Bitcoin estamos hablando de una serie temporal, es comprobar su estacionalidad. Esto se puede hacer de forma objetiva mediante el Dickey-Fuller Test. ¿Al final de qué se trata todo esto? Si os fijáis en la gráfica, esto es el concepto que se llama Rolling Mean. Simplemente se trata de buscarle una coherencia, una historia a la gráfica del Bitcoin. Se trata de entender, por un lado, lo que hacemos es desplegarla. Entonces, al desplegarla, lo que vemos es, por un lado, la tendencia y, por otro lado, intentamos de una forma muy sencilla, para que lo entendamos. Lo que intentamos es desplegarla en cuatro perspectivas. Por un lado, vamos a tener siempre la tendencia, pero después vamos a tener unos picos y valles. Aquí el problema que tenemos, después de hacer el Dickey-Fuller test, es que observamos que el Bitcoin no es estacional. ¿Por qué? Porque si descubrimos que una serie temporal es estacional, tenemos unos caminos a seguir bastante marcados que nos van a garantizar el éxito. Pero lo que ocurre con el Bitcoin es que es tremendamente volátil. Al ser tan volátil, no nos podemos apalancar en la estacionalidad; es decir, porque sea Navidad o porque sea verano, no vas a comprar o vender más Bitcoin. Esa es un poco la conclusión que nos aporta este experimento.

Una vez he conectado los datos, capturado los datos y agregado las variables, y habiendo hecho una selección de variables, simplemente el siguiente punto es, con este dataset que hemos comenzado a recordar, que había empezado con dos columnas, el timestamp y el precio, hemos pasado a tener 38 columnas. Aquí lo que empezamos a hacer es ya entrenar nuestros modelos. Para ello, seleccionamos diferentes modelos de Machine Learning y de Deep Learning y vamos a analizar un poco la performance. En esta gráfica, lo primero que hacemos es observar la performance que tiene en los datos de entrenamiento. Aquí podéis ver nuestro objetivo. Recordad que partíamos el dataset en entrenamiento y test, 80-20 o 70-30. Lo que hacemos primero es ver cuáles de estos modelos de Machine Learning o de Deep Learning son los que mejor aprenden, cuáles son los que mejor se ajustan a la curva. Recordad que lo que queremos es que se ajuste más o menos bien, por decirlo de alguna forma; tampoco queremos que se ajuste exactamente igual punto por punto, porque esto nos llevaría a overfitting. Queremos que tenga una coherencia, que sea como una especie de media que pase siempre con ese concepto de media y varianza, que al final nos va a decir que va a poder generalizar bien el modelo.

Aquí podéis ver los diferentes experimentos en pantalla y ya estaríamos en condiciones de hacer los experimentos. Una vez realizados los experimentos de entrenamiento, vamos a pasar a realizar los experimentos en test y recordad que estamos haciendo predicción a un día, a cinco días y a 30 días. Vamos a analizar las diferentes métricas de regresión; en este caso, una métrica útil podría ser el Root Mean Square Error, y ¿por qué podría ser útil? Porque presta muchísima atención a los outliers. Recordad que estuvimos hablando de que hay que analizar el concepto de los outliers; es un concepto clave para entender la naturaleza de mi problema. En este caso, ¿qué ocurre? Fijaros, la gráfica de Bitcoin hace picos muy marcados, picos y valles de forma constante. Esto, si la naturaleza del problema fuese otra, serían outliers probablemente. Pero en todo lo que está relacionado con las series temporales del stock market, hay que prestar atención a los outliers, porque estos outliers hablan de hitos; imaginaos, en este caso, de Bitcoin, hitos en el desarrollo, en el roadmap o en la adopción que han hecho que cambie este momentum.

En este caso, al final, el Root Mean Square Error está hablando de una distancia respecto a la recta de predicción, a nuestra fórmula que hemos sacado de alguna forma para predecir el tiempo. Bitcoin está hablando de una media; no hay una distancia que hay desde nuestra predicción hasta el valor real. Con lo cual, vamos a utilizar la métrica del R cuadrado ajustado. ¿Por qué? Porque nos arroja valores mucho más entendibles, ¿no? Vamos de 0 a 1, donde 0 es 0% de precisión y 1 es 100% de precisión. Si os fijáis, los resultados a un día son muy buenos. Estamos hablando de valores muy elevados. Hay varios modelos que hacen predicciones que superan el 90% a un día. Con lo cual, estaríamos hablando de que nuestros modelos podrían estar haciendo una predicción de que mañana el Bitcoin no solo va a subir o a bajar, sino que hace una predicción de a cuánto va a estar. Y estamos hablando de valores cercanos al 95%. Por tanto, ¿os imagináis saber mañana que el precio del Bitcoin va a estar en lugar de 28.000 a 32.000, con una precisión del 95%? ¿Os aportaría valor? ¿Haríais movimientos en vuestras wallets, por ejemplo? Estamos hablando de que esto tiene un potencial enorme, ¿no? Pero claro, es cierto también que estamos hablando de predicciones a corto, a un día.

¿Qué ocurre cuando observamos los resultados a tres días? Si os fijáis, pasamos prácticamente del 95% de precisión al 65%. Es decir, aquí hay un salto importante. La performance, tanto de Machine Learning como de Deep Learning, comienza a caer. También observamos algo que hemos contado en clases anteriores: los resultados de Deep Learning, en términos generales, son bastante malos. ¿Y por qué son malos? Porque realmente hay muy pocos datos para trabajar con Deep Learning. Esto es lo que provoca que, sí que es cierto que hay un tipo de algoritmos de Deep Learning, como son las redes neuronales recurrentes o RNN, en este caso las LSTM (Long Short Term Memory), que funcionan muy bien para este tipo de señales. Es decir, que están mirando el pasado muy lejano y el pasado corto, de alguna forma. Y realmente tienen una performance muy buena para señales de este tipo. Luego ocurre que no tenemos suficiente historial de datos, no tenemos suficiente cantidad de datos.

Claro, si observamos ya los resultados a 30 días, realmente lo que estamos viendo es que los resultados son desastrosos. Estaríamos hablando de que no aportamos ningún valor; es decir, aportamos el mismo valor que lanzar una moneda al aire. Por tanto, si os fijáis un poco y analizamos, diríamos que como conclusión tenemos una ventana temporal de un día a tres días, sobre todo a un día vista, donde tenemos una buena precisión, y a medida que se acercan los tres días empieza a caer, lo cual a lo mejor nos da un margen de un día y medio. Claro, a 30 días el momentum es imposible de predecir con esta técnica. Y, sobre todo, quiero que os fijéis en algo. La pregunta aquí sería: ¿por qué? ¿Por qué no nos estamos aproximando? ¿Por qué a 30 días no somos capaces de predecir el cambio de momentum? Bueno, la respuesta estaría en entender que hemos utilizado variables que son indicadores que están en el mercado, por decirlo de alguna forma, que ya están generados, que simplemente los hemos ingestado, y que realmente lo que nos está diciendo es que, a corto, estos indicadores, a un día, un día y medio, explican algo, pero a más de dos días ya no explican nada. Aquí hay otro tipo de hechos o eventos que cambian el momentum y afectan la volatilidad.

Básicamente, tenemos que entender qué agentes o eventos externos están influyendo en el cambio de precio del Bitcoin, que no sean los indicadores financieros clásicos. ¿Y qué influye? Pues, por ejemplo, es muy fácil. Lo primero que sabemos que influye son los movimientos de las ballenas. Son operaciones que mueven entre 1.000 y 5.000 bitcoins. ¿Qué está ocurriendo? Que si de momento se juntan un grupo de inversores y deciden hacer un movimiento muy potente, esto afecta directamente al precio del Bitcoin de forma instantánea. Claro, esto a priori es difícil de predecir, pero se podría monitorizar, que es lo que nos interesa. Lo que nos interesa es tener un seguimiento de estos movimientos de las ballenas, para tener un lapso de tiempo que nos permita tener ventaja competitiva.

Claro, ¿qué características tiene Blockchain? En Blockchain sabemos que es mutable, que es segura, pero que es trazable. Entonces, desde aquí podríamos estudiar movimientos de wallets calientes a wallets frías. ¿Por qué? Porque en este momento se retira el dinero. Con lo cual, movimientos de ballenas, si los detectamos y hacemos un ejercicio de seguimiento muy potente y exhaustivo, podrían arrojar luz sobre el rastreo de movimientos de ballenas, que a priori es lo que más sombras aporta en todo este caso.

Claro, sabemos que los movimientos de ballenas son importantes, pero recordad que pasó otro factor muy importante, aparte de las ballenas, que eran los cisnes negros. ¿Qué pasó cuando vivimos la pandemia? El Bitcoin cayó hasta los 6.000 euros. Esto es una afectación muy directa de un cisne negro. Los cisnes negros son eventos que suceden a lo largo de la historia que son impredecibles. En este caso, tenemos un ejemplo muy claro como fue el COVID-19. ¿Se pueden monitorizar o rastrear los cisnes negros? Sí, los podríamos monitorizar o rastrear. Es decir, en este caso, lo que haríamos sería añadir una variable que sería "cisne negro". Le daríamos un valor desde 0 hasta 1. Fijaros que en este impacto social que hemos vivido lo podríamos haber cuantificado de esta forma, desde el momento en que podría salir a cenar y la aforo era del 50% o del 25%. Digamos que a nivel de limitación de vida podríamos medirlo en una escala y lo tenemos registrado por fechas; sabemos las restricciones, en qué momentos han sido. Por tanto, no solo esto es...

Y fijaros, los cisnes negros, como podría ser una guerra o una pandemia, no se pueden predecir. Es cierto que es muy difícil predecirlos, pero fijaros que sí que se puede. No se puede predecir de forma, digamos, a dos años vista o tres; eso sería imposible. Pero sí que se puede predecir a un mes, dos, tres vista. Porque, por ejemplo, fijaros en el caso de las guerras. No se invade un país de la noche a la mañana. Por ejemplo, en el caso de la guerra del Sur, no se puede predecir. Es decir, no se puede predecir. Primero, hay muchas noticias al respecto, hay muchas conjeturas, muchas declaraciones, y sí que podemos rastrear las noticias, y sí que podemos detectar en el PIB qué está pasando y detectar qué grado de intensidad hay, qué probabilidad hay de que haya un conflicto bélico. Esto es un ejercicio que es bastante sencillo, con inteligencia artificial.

Por tanto, lo mismo ha pasado con el COVID-19; sí que podríamos hacer una prueba, una predicción de que hay 100 casos en China, las noticias que empiezan a subir el pulso de esta noticia y sabemos que después se pasan a 500, a 1.000, a 2.000. Es decir, sí que podríamos hacer una predicción de pandemia. Con lo cual, poder monitorizar los cisnes negros puede darnos una ventaja competitiva también de cara a la predicción de este tipo de algoritmos. Por tanto, ya tenemos, por un lado, las ballenas y, por otro lado, los cisnes negros, más fácil de rastrear hasta ahora un cisne negro que el movimiento de ballenas, aunque se puede rastrear, como hemos dicho, haciendo análisis on-chain.

Y el tercer factor sería el análisis de sentimiento. ¿Y por qué el análisis de sentimiento? Al final, quien no conoce los casos en los que, por ejemplo, el más famoso es el de Elon Musk sobre Twitter, haciendo diferentes afirmaciones, ha generado variaciones importantes sobre el precio del Bitcoin. El caso de Elon Musk sería un caso para darle de comer aparte, porque sería un mega influencer, pero no el concepto de influencer que conocemos, sino un influencer a nivel mundial que está cambiando la historia. Van a haber pocos genios como Elon Musk en la historia que han revolucionado tantos sectores. Es por ello que la voz de influencers como Elon Musk o Bill Gates es muy escuchada, porque al final tienen una relevancia histórica muy potente.

Cuando hablamos de análisis de sentimiento, no es simplemente escuchar a Elon Musk; estaríamos hablando de, por ejemplo, poder hacer un seguimiento de cuánto se está hablando en las redes sobre Bitcoin, por ejemplo, en Twitter o en otra red social. Al final, el análisis de sentimiento lo que hace son algoritmos que están entrenados para, cuando leen un texto, cuando capturan un texto, lo que deciden es si ese contenido es positivo, negativo o neutro respecto al concepto que estamos monitorizando. De esta forma, sí que podemos medir el sentimiento que hay en las redes sociales hacia un concepto o un hashtag concreto. En este caso, sería Bitcoin. Y sí que hay estudios que demuestran que hay una correlación bastante directa, como podéis ver en la gráfica, del análisis de sentimiento con el precio de productos cripto y, en este caso, el de Bitcoin.

Por tanto, para recapitular, aquí concluye nuestro experimento donde lo que hemos realizado es capturar, en un inicio, datos sobre Bitcoin: su timestamp y su precio. Hemos añadido variables, hemos hecho una reducción de variables, hemos hecho experimentos, hemos probado diferentes algoritmos, hemos analizado los resultados y también hemos propuesto posibles mejoras de variables para poder desarrollar un algoritmo de predicción del Bitcoin con éxito en una ventana temporal que supere los 3 días hasta los 30 días. Al final, en los proyectos de Machine Learning e Inteligencia Artificial, la parte táctica, estratégica y lógica es muy importante para saber qué variables tengo que rastrear para poder alcanzar la máxima precisión.

### Algoritmo Predictivo de Bitcoin. Gráficas de correlación
Las gráficas de correlación más importantes que vemos son: 
1.  Hash Rate
2.  Miner Revenue
3.  Google Trends

Vamos a analizar cada una de las tres porque tienen correlación y este es un ejercicio un poco de imaginación y de lógica.

![[484.E1_Algoritmo_Predictivo_de_Bitcoin_1.png]]

#### 1 | Hash Rate
El concepto Hash está relacionado con la minería en una red de Blockchain. 

**Por tanto, ¿Qué nos dice esta gráfica?**
Está diciendo que cuando el precio del Bitcoin sube, lleva la misma correlación con el Rate, con la tasa de Hatch que se está produciendo en la red de Blockchain, es decir, con la creación de bloques. 

![[484.E1_Algoritmo_Predictivo_de_Bitcoin_2.png]]

**¿Qué nos dice esto?** 
Mucha creación de bloques influye directamente con que el precio esté subiendo o esté bajando. Por tanto, no nos está aportando ningún valor, porque simplemente describe un mecanismo de funcionamiento de una red de Blockchain. 

> Es por eso que para ello el contexto es tan importante. Necesitamos entender la naturaleza del problema.

#### 2 | Miner Revenue
**¿Por qué tiene relación?** 
Si comprendemos bien el funcionamiento de una red de Blockchain, sabemos que un minero, a medida que va minando y resolviendo la Proof of Work o Proof of Stake, obtiene beneficios a cambio. Por tanto, si se están produciendo muchas acciones de compraventa, lo que está sucediendo es que la ganancia de los mineros tiene una relación directa. Esto es bastante obvio. 

En consecuencia, son variables que realmente no aportan ningún valor. 

Pero ¿qué ocurre si analizamos la gráfica de la correlación entre Google Trends y el precio del Bitcoin? ¿Qué está ocurriendo aquí?. 

Pensad en cómo funcionamos los seres humanos. De momento vemos que se está hablando muchísimo de Bitcoin en las noticias, en círculos sociales, te cuentan que alguien ha invertido y ha ganado dinero o ha perdido. 

**¿Qué pasa entonces?**
Rápidamente, empezamos a ver qué precio tiene, qué está pasando con esta cripto y rápidamente, en Google Trends empezamos a activar toda esta serie de búsquedas. Esto sí que tiene una correlación directa. Y es una variable importante, porque al final nos está hablando, el mensaje que nos está diciendo es: Cuando mucha gente a la vez está buscando Bitcoin, pues están sucediendo cosas con el precio del Bitcoin. Estamos hablando de un lapso de tiempo que puede ser importante, sobre todo para los Traders a short. 

Esta ha sido una pequeña muestra previa de análisis de aquellas variables importantes, pero el experimento se ha realizado con una matriz de correlación. Si os fijáis, la matriz de correlación, lo que está midiendo es:
- De 0 hasta 1: Correlación directa
- De 0 hasta -1: Correlación inversa

En la matriz de correlación, lo que intentamos buscar es la correlación directa o inversa y analizar este tipo de hechos que acabamos de comentar previamente. Es decir, hay que tener mucho cuidado, porque en este caso el Hash Rate y el Miner Revenue tienen una correlación muy elevada con el precio del Bitcoin, pero estarían añadiendo ruido a nuestro modelo, porque es simplemente redundancia. Esto es una consecuencia de que haya compra o venta de Bitcoin.

#### 3 | Google Trends
Sin embargo, otras como Google Trends tienen una correlación alta, ya sea positiva o negativa, y sí que están hablando de indicadores de Momentum, que es lo que estamos buscando. Por lo tanto, este proceso, que se llama Feature Selection, que se hace con una matriz de correlación y es muy fácil de ejecutar con pocas líneas de código en Python, nos permite comenzar a descartar aquellas variables que aportan ruido y empezar a ver aquellas que, realmente, son importantes.

Una vez hemos realizado la matriz de correlación, podemos ya empezar también a trabajar con aquellas variables que nos vamos a quedar, porque se trata de Feature Selection. Al final, lo que está indicando es feature hace referencia a variables, es una selección de variables. Lo que queremos es reducir, porque en este caso hay tantos indicadores. Estamos hablando de en torno a 80 indicadores técnicos y esto es demasiado. El coste computacional sería demasiado elevado. 

> Nuestro objetivo con el Feature Selection es quedarnos con aquellas variables que realmente son importantes.

Otra forma de reducir las variables es utilizar, por ejemplo, algoritmos como [XGBoost](https://xgboost.readthedocs.io/en/stable/), que trabajan con tecnología de árboles de decisión, pero con un gradiente mucho más potente y lo que nos permiten, es obtener, de alguna forma, una clasificación de aquellas variables más fundamentales de mayor o menor, de cuánto peso tienen en la afectación al precio. 

Por tanto, una buena praxis es el emplear la matriz de correlación y después usar Feature Selection mediante algoritmos como XGBoost. 

**Para qué?**
Para quedarnos con los indicadores técnicos más importantes. 

Si analizamos los indicadores ganadores que obtenemos (en este caso, pasamos de 80 a 36, lo cual es una reducción potente) que la media móvil está en el top, es de los más potentes y es cierto que muchas herramientas de trading se basan en medias móviles. Y funciona, pues las herramientas de trading lo que hacen es utilizar diferentes medias móviles de 5, 10, 20 o 200 días y, básicamente, lo único que hacen es, en el momento que se solapan las diferentes medias móviles, indicarte los puntos en los que puedes comprar o vender. 

Básicamente, este es el funcionamiento, pero veremos que no solo afectan estos indicadores. Estas medias móviles que hablan un poco más de la historia del pasado, con Machine Learning es un enfoque bastante diferente a jugar solo con el pasado. 

Otro factor importante, ya que con el Bitcoin estamos hablando de una serie temporal, es comprobar su estacionalidad. Esto se puede hacer de forma objetiva mediante el [Dickey–Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test).

**¿De qué trata todo esto?**
Si nos fijamos en la gráfica, esto es el concepto que se llama Rolling Min. Simplemente, se trata de buscarle una coherencia, una historia a la gráfica del Bitcoin. Se trata de entender, por un lado, lo que hacemos es desplegarla. Al desplegarla, lo que vemos es, por un lado, la tendencia y, por otro lado, intentamos desplegarla en cuatro perspectivas. 

Por un lado, vamos a tener siempre la tendencia, pero después vamos a tener unos picos y valles. Aquí el problema que tenemos, después de hacer el Dickey–Fuller test, es que observamos que el Bitcoin no es estacional. ¿Por qué? Porque si descubrimos que una serie temporal es estacional, tenemos unos caminos a seguir bastante marcados donde nos va a garantizar el éxito. Pero lo que ocurre con el Bitcoin es que es tremendamente volátil.

Al ser tan volátil, no nos podemos apalancar en la estacionalidad. Porque sea Navidad o porque sea verano, no vas a comprar o vender más Bitcoin. Esta es una de las conclusiones que nos aporta este experimento.

### Algoritmo Predictivo de Bitcoin. 
#### 1 | Movimientos de Ballenas
A priori, es difícil de predecir, pero se podría monitorizar.

Es como tener un tracking de estos movimientos de las ballenas para tener un lapso de tiempo que nos permita tener ventaja competitiva. 
- Para ello, es importante conocer las características de Blockchain. 
- Sabemos que es inmutable, que es segura, pero que es trazable. 
- Desde aquí, podemos estudiar los movimientos de wallets calientes y frías en el momento que se retira el dinero, con lo cual podemos detectar los movimientos de las ballenas. 
- Se trata de un ejercicio de tracking muy potente y muy exhaustivo, pero que nos podría arrojar luz para trackear movimientos de ballenas, que es lo que más sombras nos aporta en este caso.

Sabemos que los movimientos de ballenas son importantes, pero recordad que pasó también con otro factor muy importante aparte de las ballenas: Los Cisnes Negros.

![[485.E1_Algoritmo_Predictivo_de_Bitcoin_1.png]]

#### 2 | Los Cisnes Negros
¿Qué sucedió cuando vivimos la pandemia?: El Bitcoin cayó hasta los 6000 €. Esto es una afectación muy directa de un Cisne Negro. 

Los cisnes negros son eventos que suceden a lo largo de la historia que son impredecibles. En este caso tenemos un ejemplo muy claro como fue el Covid-19. 

¿Se pueden monitorizar o trackear los Cisnes Negros? 
- Sí, los podríamos monitorizar o trackear, es decir, lo que haríamos sería añadir una variable “Cisne Negro” y darle un valor desde 0 hasta 1. 
- Con respecto a la limitación de vida podríamos medirlo en una escala y lo tenemos registrado por fechas. 
- Sabemos en qué momento se han producido las restricciones. 
- Los Cisnes Negros, una guerra o una pandemia, no se pueden predecir a 2 o 3 años vista, eso sería imposible. Pero sí que es posible predecirlos a varios meses, porque no se invade un país de la noche a la mañana. 
- Antes, hay muchas noticias al respecto, muchas conjeturas, declaraciones y sí que podíamos trackear las noticias y detectar, mediante los sucesos y su grado de intensidad hay, la probabilidad de que haya un conflicto bélico. 

Esto es un ejercicio bastante sencillo, con IA. Lo mismo ha pasado con el Covid-19, podríamos hacer una predicción de casos, por ejemplo, 100 casos en China. Monitorizamos las noticias y, si empiezan a subir el pulso, sabemos qué se pasa a 500, 1000, 2000 casos. Podríamos hacer una predicción de pandemia. 

> Ser capaces monitorizar los cisnes negros puede darnos una ventaja competitiva de cara a la predicción de este tipo de algoritmos.

Hasta ahora, es más fácil de rastrear un Cisne Negro que el movimiento de ballenas, aunque se puede rastrear, como hemos dicho, haciendo análisis On Chain.

#### 3 | Sentiment Analysis
Y el tercer factor sería el Sentiment analysis, cuyo más famoso es Elon Musk y sus hilos sobre y declaraciones sobre Twitter, que han generado variaciones importantes sobre el precio del Bitcoin. 

El caso de Elon Musk es un caso extraordinario, estamos hablando un mega influencer, pero no el concepto de influencer que conocemos, sino un influencer a nivel mundial que está cambiando la historia. Van a haber pocos genios como él en la historia, capaces de revolucionar tantos sectores. 

Es por ello que la voz de influencers como Elon Musk o Bill Gates, son muy escuchadas. Tienen una relevancia histórica muy potente. Cuando hablamos de Sentiment analysis no es simplemente escuchar a Elon Musk, podíamos hacer un tracking de cuánto se está hablando en las redes sobre Bitcoin, ya sea en Twitter o en otra red social.

Al final, los algoritmos de Sentiment analysis están entrenados para que, cuando lean un texto, decidan si ese contenido es positivo, negativo o neutro respecto al concepto que estamos monitorizando. De esta forma, sí que podemos medir el sentimiento que hay en las redes sociales, hacia un concepto o un hashtag concreto. En este caso sería Bitcoin. 

Y sí que hay estudios que demuestran que hay una correlación bastante directa, como puedes ver en la gráfica del Sentiment analysis con el precio del proyecto escrito y, en este caso, el Bitcoin. 

![[485.E1_Algoritmo_Predictivo_de_Bitcoin_2.png]]

**Aquí concluye nuestro experimento, resumamos lo que hemos hecho:**
1. Capturar sobre Bitcoin: Timestamp y precio 
2. Añadir variables
3. Reducir esas variables
4. Hacer experimentos
5. Probar diferentes algoritmos 
6. Analizar los resultados 
7. **Para terminar:** Proponer posibles mejoras de variables para poder desarrollar un algoritmo de predicción del Bitcoin con éxito a una ventana temporal que supere los 30 días

En los proyectos de Machine Learning e Inteligencia Artificial, la parte táctica, estratégica y lógica es muy importante para saber qué variables hay que trackear para poder alcanzar la máxima precisión.

### Algoritmo Predictivo de Bitcoin
Una vez hemos capturado los datos, agregado las variables y hecho una selección, nuestro Dataset, que había comenzado con dos columnas: _Timestamp_ y _Precio_, ha pasado a tener 38 columnas.

Aquí ya podemos empezar a entrenar nuestros modelos. Para ello, seleccionamos diferentes modelos de Machine Learning y de Deep Learning y vamos a un poco analizar la performance. En esta gráfica, lo primero que hacemos es observar la performance que tiene en los datos de entrenamiento. 

Recordemos que partíamos el Dataset en entrenamiento y test, 80/20 o 70/30. Lo que vamos a hacer primero es ver estos modelos de Machine Learning o de Deep Learning para saber cuál es el que mejor aprende y el que mejor se ajusta a la curva. 

> Es importante remarcar que, lo que queremos, es que se ajuste más o menos bien y no exactamente, punto por punto, para no llegar al Overfitting.

Queremos que tenga una coherencia, que pase siempre con ese concepto de media y varianza, que al final lo que nos va a decir es que va a poder generalizar bien el modelo.

Una vez realizados los experimentos en entrenamiento, vamos a pasar a realizar los experimentos en test. Estamos haciendo predicción a 1, 5 y 30 días. Vamos a analizar las diferentes métricas de regresión.  En este caso, una métrica útil podría ser el [Root-mean-square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation). 

![[486.E1_Algoritmo_Predictivo_de_Bitcoin.png]]

#### Root-mean-square error
**Por qué puede ser útil?**
Porque presta muchísima atención a los Outliers, un concepto clave para entender la naturaleza de nuestro problema.

**¿Qué ocurre en este caso?**
Fijémonos en que la gráfica de Bitcoin hace picos muy marcados, picos y valles de forma constante. Esto, si la naturaleza del problema fuese otra, serían, probablemente, Outliers. 

Pero en todo lo que está relacionado con series temporales de Stock Market, hay que prestar atención a los Outliers, porque estos Outliers hablan de hitos en el desarrollo, en el Roadmap o en la adopción que han hecho que cambie este Momentum.

Root-mean-square error está hablando de una distancia respecto a la recta de predicción, a nuestra fórmula que hemos sacado de alguna forma para predecir el Bitcoin, está hablando de una media, de una distancia que hay desde nuestra predicción hasta el valor real. Con lo cual, vamos a utilizar la métrica del R2 ajustado.

**¿Por qué?**
Porque nos arroja valores mucho más entendibles. Vamos de 0 a 1, donde 0 es 0% de precisión y 1 es 100% de precisión. 

#### Resultados a 1 día.
Los resultados a 1 día son muy buenos, valores muy elevados. Hay varios modelos que hacen predicciones que superan el 90%. 

Nuestros modelos podrían estar haciendo una predicción de que mañana el Bitcoin, ya no es que va a subir o va a bajar, hace una predicción de a cuánto va a estar. Y estamos hablando valores cercanos al 95%. 

Por tanto, ¿os imagináis saber mañana que el precio del Bitcoin va a estar en lugar de 28000 a 32000 o una precisión del 95%? ¿Os aportaría valor? ¿Haríais movimientos en vuestras wallets, por ejemplo? 

Estamos hablando de que esto tiene un potencial enorme ¿no? Pero claro, es cierto también que estamos hablando de espacios a short, a 1 día.

#### Resultados a 3 días.
Pasamos, prácticamente, del 95% de precisión al 65%, aquí hay un salto importante.

La performance, tanto de Machine Learning como de Deep Learning, comienza a caer. 

También observar algo que hemos contado en clases anteriores, que los resultados de Deep Learning, en términos generales, son bastante malos.

**¿Por qué son malos?**
Porque realmente hay muy pocos datos para trabajar con Deep Learning. Esto es lo que provoca. Sí que es cierto que hay un tipo de algoritmos de Deep Learning, como son las redes neuronales recurrentes o RNN, en este caso, las LSTM, _long short term memory_, que funcionan muy bien para este tipo de señales. 

Es decir, que están mirando el pasado muy lejano y el pasado a corto, de alguna forma. Y realmente tiene una performance muy buena para señales de este tipo. Lo que ocurre es que no tenemos suficiente historial, suficiente cantidad de datos.

#### Resultados a 30 días.
Si observamos ya los resultados a 30 días, estamos viendo que son desastrosos. 

Estaríamos hablando que no aportamos ningún valor, es decir, aportamos el mismo valor que tira lanzar una moneda al aire. Por tanto, si os fijáis un poco y analizamos, diríamos que como conclusión, tenemos una ventana temporal de 1 día a 3 días donde tenemos una buena precisión y a medida que se acerca a los 3 días, empieza a caer, lo cual a lo mejor nos da un margen de 1 día y medio. 

Claro, a 30 días el momento es imposible de predecir con esta técnica y sobre todo quiero que os fijéis en algo.

**¿Por qué a 30 días no somos capaces de predecir el cambio de Momentum?**
La respuesta estaría en entender que hemos utilizado variables que son indicadores que están en el mercado, por decir alguna forma, que ya están generados, que simplemente los hemos ingestado. Y que, realmente lo que nos está diciendo es que a corto plazo, estos indicadores a un día, un día y medio, explican algo.

Pero a más de dos días ya no explican algo. Aquí hay otro tipo de hechos o de eventos que cambian el Momentum y afectan en la volatilidad.

**¿Cuáles son estos elementos?**
Básicamente, tenemos que entender que agentes o eventos externos están influyendo al cambio de precio del Bitcoin, que no sean los indicadores financieros clásicos.

**¿Qué influye?**
Lo primero que sabemos que influye son los movimientos de las ballenas. Son operaciones que mueven en torno a entre 1000 y 5000 Bitcoin.

**¿Qué está ocurriendo?**
Que si de momento se juntan un grupo de inversores y deciden hacer un movimiento muy potente, esto afecta directamente al precio del Bitcoin.

## U5. Machine Learning
### Machine Learning (Video)
![[487.E1_Machine_Learning.mp4]]
[Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866144-u5-1-1-machine-learning-jose-peris)

Big Data es la gasolina de nuestros algoritmos. Cuando hablamos de Big Data, tenemos que hablar de las 4 Vs. ¿Y por qué 4 Vs? Volumen, variedad, velocidad y veracidad. Muy a menudo en los medios, sobre todo escritos, vemos esta palabra también utilizada de forma errónea en lo que al Machine Learning se refiere. Por ejemplo, si estamos viendo una publicación sobre el Big Data del clásico, de un partido de fútbol de Madrid contra el Barça. ¿Qué ocurre? Que aquí tenemos un histórico; hay una estadística de muchos años de enfrentamientos, de muchos jugadores y de muchos eventos y acciones que han pasado. Estábamos comentando que los datos cuentan historias. Cuando se plantea este contexto, el concepto de Big Data, estamos hablando de visualización de datos, de KPIs, de Key Performance Indicators. Realmente es una representación visual de estos indicadores. Pero cuando hablamos de Big Data en Machine Learning, estamos hablando de nuestra gasolina. Estamos hablando de poner los datos a trabajar.

¿Qué significa volumen? Volumen significa que necesitamos cantidades inmensas de datos. Para que os hagáis una regla muy básica: a más datos, más precisión o más accuracy, que es nuestro objetivo. Pero también necesitamos variedad. ¿Por qué? Porque necesitamos datos heterogéneos. Es mucho mejor tener datos heterogéneos que homogéneos. ¿Por qué? Porque necesitamos que nuestro algoritmo vea muchísimas casuísticas. Es como el ejemplo del padre que le enseña al niño lo que es un vaso. Es mejor que vea mil modelos de vasos que diez modelos de vasos. ¿Por qué? Porque cuando realmente salga a la calle y tenga que detectar lo que es un vaso, a más ejemplos vistos, más capacidad de reconocimiento.

¿Qué es la velocidad? Bueno, cuando hablamos de velocidad, básicamente nos referimos a la ingesta de los datos. En este caso, haría referencia a la capacidad que tenemos hoy de trabajar con datos en streaming. También es importante saber que los algoritmos se entrenan, pero se pueden reentrenar constantemente. A más datos que entran, se van reentrenando y van mejorando su precisión. ¿A qué nos referimos con veracidad? Estamos hablando de la calidad del dato. Estamos hablando de que estos datos hay que procesarlos, hay que transformarlos; lo primero que hacemos siempre es visualizarlos para ver que no haya incongruencias. ¿Por qué? Porque pueden haber pasado eventos específicos en una fecha referente a cualquier caso de uso que realmente estemos hablando de un caso aislado. Y esto puede generar una distorsión en el planteamiento de nuestro algoritmo. Por tanto, simplemente hace referencia al proceso de ETL.

Para comprender mejor este ejemplo, quiero que hagamos el caso de Teachable Machine de Google, donde vais a poder comprobar vosotros en casa qué son las cuatro V's. Para ello, lo que me gustaría es que presionaseis en el enlace que tenéis aquí en la plataforma y que entrenéis un detector de objetos. Aquí lo que veréis es que vamos a entrenar un algoritmo de visión artificial. Lo que va a hacer es clasificarnos diversas clases, las que nosotros determinemos. Podéis coger cualquier objeto que tengáis en casa, asignarlo a una clase; podéis coger una botella, podéis coger un lápiz y simplemente lo que tenemos que hacer es configurarlo. Al presionar en la cámara, lo que veréis es que os genera en la videocámara, la webcam, una cantidad infinita de imágenes. Lo que tenéis que hacer es mover el objeto en muchas posiciones para comprobar el efecto de variedad. Seguidamente, simplemente tenéis que seguir los pasos, entrenar el modelo y comprobar su rendimiento.

Siguiendo con el concepto de Big Data, la traducción que tiene de una forma física o más tangible sería el dataset. ¿Qué es un dataset? Un dataset es un conjunto de datos. Para que os hagáis una idea, a día de hoy el Big Data puede llegarnos en forma de dataset de diversas maneras. Una muy simple y común sería un archivo .txt que simplemente deberemos transformarlo para llevarlo a nuestro terreno. Pero el tipo de archivo dominante es el CSV. También podemos tener ficheros Excel o incluso archivos PDF. Cabe destacar también que el sonido también son datos, aunque debo remarcar aquí que el sonido como tal no existe para el Machine Learning; se convierte en un espectrograma, igual que pasa a ser una imagen. Y podemos tener también vídeos e imágenes.

Cuando hablamos de un dataset, vamos a diferenciar entre dos grandes grupos: los datos tabulares, que son el primer grupo y el más común, y los datos de visión artificial, que serían imágenes o vídeos. Vamos a centrarnos en el primer grupo de los datos tabulares y al final lo que vamos a ver de una forma muy simple es una tabla; imaginaos una tabla de Excel con filas y columnas. Normalmente, hablamos de un dataset de calidad; por ejemplo, una buena cifra sería tener, por ejemplo, 5,000 o 10,000 filas hacia adelante y de 8 a 10 columnas, que estas serían nuestras variables. Ahí estaría nuestro target, nuestro objetivo a predecir. Es cierto que se pueden llegar a hacer pruebas de concepto con 1,000 filas utilizando Machine Learning, pero no es lo recomendable. Si queremos hablar de un buen tamaño, estaremos hablando de 100,000 filas para garantizar una buena calidad. Esto nos permite utilizar todo tipo de algoritmos, tanto de machine learning como de deep learning.

Cuando abrimos un dataset de datos tabulares, nos podemos encontrar diferentes tipos de datos. Por un lado, tenemos los datos double, que serían simplemente números con fracción, por ejemplo, 1.7854. O nos podemos encontrar tipos de datos como enteros, que en este caso no tendrían partes fraccionales, por ejemplo, un 7. También tenemos las strings que hacen referencia a categorías; por ejemplo, imaginemos que estamos hablando de una variable donde simplemente añadimos si un paciente padece algún tipo de enfermedad y tenemos cinco opciones, por ejemplo, una de ellas es diabetes. Esto sería una string; estamos hablando de una categoría, no estamos hablando de números, estamos hablando de palabras, por decirlo de alguna forma simple. Por otro lado, también podemos encontrar el timestamp; como su nombre indica, es simplemente una fecha, 24 del 7 de 1997. Simplemente tenemos que tener en cuenta que este tipo de dato hay que enseñarle a nuestro sistema que, al ser marcado como timestamp, le estamos diciendo que no es un número corriente, sino que está haciendo referencia a un evento temporal sucesivo. O, por ejemplo, podemos tener un diccionario. Un caso muy común del diccionario sería cuando vamos al supermercado y hacemos nuestra bolsa de la compra; ahí tenemos un diccionario de artículos, de palabras, con los objetos, con los ítems que hemos comprado.

Otro tipo de datos serían imágenes, JPEG o JPG, o simplemente documentos donde encontraríamos, por ejemplo, reseñas, documentos de texto donde encontraríamos, por ejemplo, "Great Food", "Excellent Service". Esto sería un poco el abanico de datos, de tipología de datos que podemos encontrar. Nuestra primera misión es, en el momento que abrimos un dataset, identificar los diferentes tipos y, dependiendo del objetivo que queramos, veremos qué tratamiento hay que aplicar a cada tipo de datos.

¿Qué es ETL? ETL en inglés significa Extract, Transform and Load. La ETL quizás es el proceso más pesado y más largo para un científico de datos porque, como os decía, estamos hablando de que los algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial. Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente, en unas condiciones diferentes. Por tanto, nuestra primera misión como científico de datos es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. Normalmente, el principal problema que solemos encontrar y el que primero vamos a buscar es si hay valores nulos. Esto es bastante común; puede haber diferentes variables, recordad que las variables son las columnas donde encontramos valores nulos, y muchas veces vienen marcados por NAN o un simple interrogante. Lo primero que tenemos que hacer siempre es visualizar los datos, porque si no visualizamos los datos nunca vamos a entender la historia. Cuando pintamos las gráficas de las diferentes variables, vamos a observar qué distribución tienen los datos, también vamos a observar cuántos valores nulos tenemos y vamos a observar diferentes incongruencias. Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo y simplemente implica ir comprobando columna a columna el tipo de dato que tenemos y ver qué transformación le tenemos que aplicar para que podamos trabajar de forma correcta con nuestros algoritmos.

También quiero comentaros que, a día de hoy, cuando empezamos en este mundo, ya sabéis que el tema de los datos y la privacidad es un tema bastante recurrente. Lo primero que quiero comentar es que en el campo de Machine Learning los datos se tratan siempre de forma anonimizada; es decir, para nosotros no tiene valor el nombre de una persona, se trabaja con IDs. Pero, ¿es cierto que si queremos practicar y entrar en este mundo, tenemos repositorios como Kaggle o Google Dataset, donde podemos descargar datasets de todo tipo de casos de uso? Esto nos será muy útil para poder entrenar nuestros algoritmos y ampliar nuestro conocimiento y nuestra destreza.

El desarrollo de un algoritmo sigue unos pasos pautados que vamos a narrar a continuación. El primer paso se llama Data Collection. Aquí se trata simplemente de descargar los datos, de capturarlos. Esta parte es muy importante. Normalmente, el que mejor captura el dato es el que mejor algoritmos desarrolla. Por tanto, la primera parte va a ser siempre descargarlo. Nos podemos encontrar en muchos escenarios. Muchas veces, a lo mejor toca descargarlos desde una API. O sucede que a lo mejor tenemos que buscarnos unos datos por un lado y otros por otro y después fusionarlos. Aquí nos podemos encontrar en múltiples escenarios. Vamos a hablar del proceso de desarrollo de un algoritmo. El primer punto indispensable es el business analysis; es decir, para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema. Imaginad que viene una empresa a pedirnos que desarrollemos un algoritmo predictivo de ventas, por ejemplo. El primer punto sería hacer una entrevista con la empresa para obtener un briefing y comprender el caso de uso al que se trata. Debemos entender todos los stakeholders, todas las variables que están afectando, cómo funciona ese negocio y hacer de forma intuitiva una primera hipótesis de qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. Esta parte es fundamental. De hecho, es una parte en la cual muchos científicos de datos quizás se pierden demasiado en la programación. Aquí hay una parte que es de táctica y estrategia, de lógica, de entendimiento de negocio, y otra parte que es de programación y desarrollo.

Una vez hemos hecho esta primera parte de Business Analysis, pasaríamos al Data Collection o la captura del dato. Aquí simplemente analizaremos, siguiendo con el caso de esta empresa, si los datos son internos, son externos, si nos vamos a descargar de una API, desde una base de datos o si nos lo van a proporcionar de forma aislada en un CSV. Una vez hemos coleccionado estos datos, los hemos capturado, pasaríamos a prepararlos, a Data Preparation o también ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, pasamos a limpiarlos y evaluamos la calidad del dato, porque muchas veces nos va a pasar que a lo mejor tenemos 10,000 filas, pero cuando hacemos el proceso de ETL nos podemos quedar en 4,000 filas. Esto es muy común. ¿Por qué? Porque la calidad del dato es anómala, con lo cual hay que pensar que el algoritmo trabaja de una forma muy matemática y muy fina; por tanto, si nuestra gasolina contiene algún error, la va a reproducir en el resultado. Por tanto, esta parte es crucial y nos puede llevar muchísimo tiempo.

El siguiente paso sería escoger un modelo; como os decía, estos son cajas de herramientas y, a medida que vamos avanzando en conocimiento, de un golpe de vista ya prácticamente sabemos qué tipo de modelo puede encajar en nuestro problema. De ahí pasaríamos a entrenar el modelo; luego explicaremos qué significa un entrenamiento de un algoritmo y después pasaríamos a la evaluación. La evaluación, al final, se trata básicamente de que nuestro modelo coge unos datos, aprende de unos datos históricos, hace unas predicciones y contrastamos los resultados. ¿Cómo los contrastamos? Con datos de test. ¿Por qué? Porque no utilizamos todos los datos. Nosotros vamos a lanzar predicciones que las vamos a contrastar con la realidad. Esto es la evaluación. Aquí normalmente también lo que se hace es iterar con el parameter tuning; imaginad que hemos llegado a una accuracy del 80% y no es suficiente para nuestro caso de uso, porque un 80% puede ser muy positivo para ciertos casos de uso. Imaginad que para una predicción de ventas ya es un valor interesante, pero si estamos prediciendo si una persona tiene cáncer o no, no podemos fallar un 20%. Es aquí donde estableceremos nuestros umbrales; si el 80% es suficiente, iteraremos y pasaremos al parameter tuning, donde lo que vamos a hacer es optimizar los parámetros del algoritmo para aumentar la precisión. Finalmente, la última parte serían las predicciones o el deploy o puesta en producción.

Imaginemos que somos un banco y esto es un caso real, porque a día de hoy sucede. Un banco quiere saber a cuántos clientes les quiere dar un crédito o les puede dar un crédito. ¿En qué se basa un banco para darte un crédito? Pues va a estudiar todo tu historial de ingresos, gastos, impagos, cobros, etc. Imaginad una base de datos de millones de personas. Si tenemos que hacer un estudio manual, fila a fila, ID a ID, esto sería una labor muy tediosa. ¿Cómo se entrenaría un algoritmo de machine learning para hacer una predicción de si a esta persona debemos darle el crédito o no? En este caso, lo que haríamos sería ir a la base de datos, descargaríamos un dataset donde tendríamos un histórico de 10 años atrás de aquellos usuarios con unas características y al lado tendríamos una etiqueta supervisada; es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no. Es decir, si se le debe dar un préstamo o no. ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo. Entonces, a partir de aquí, podríamos hacer un algoritmo predictivo de forma que, en el momento que entre un usuario nuevo y el algoritmo esté funcionando, haría una predicción de si a este usuario hay que darle crédito o no. ¿Por qué? Porque en base a su historial, de alguna forma. Entonces, aquí estamos hablando de que el dataset contenía una columna donde teníamos el histórico de un sí o un no, o de impago o de pago. Esto es supervisado. Supervisado significa que tenemos etiqueta, que conocemos el resultado de antemano, en el pasado.

Cuando hablamos de lenguaje de aprendizaje no supervisado, estamos hablando de que los datos nos vienen desordenados. O sea, imaginaos fichas de diferentes formas, ¿vale? Triángulos, cuadrados, círculos, y nos llegan todas las fichas desordenadas. Nosotros, cuando es aprendizaje no supervisado, lo que vamos a hacer es ordenarlas, clasificarlas. Ordenarlas, clusterización; esto significa poner los triángulos con los triángulos, los círculos con los círculos y los cuadrados con los cuadrados. Por tanto, quiero que diferenciemos aquí: no supervisado son datos desordenados. Supervisado significa que tenemos una etiqueta, sabemos qué ha pasado, sabemos el resultado; por tanto, esto nos define el tipo de algoritmos que vamos a utilizar. Normalmente, en aprendizaje no supervisado, los algoritmos más comunes serían los de clustering, donde lo que se hace es que, en el momento que un cliente entra a tu página web, y esto está funcionando en muchos e-commerce, a día de hoy hay un algoritmo que, en base a tus características y a tu patrón de uso, como experiencia de usuario, ya te clasifica con un tipo de cliente o un tipo de buyer persona. Imaginad que se podría incluso clasificar como un cliente arriesgado, un cliente conservador o un cliente dudoso. Otro ejemplo muy común del aprendizaje no supervisado son los sistemas recomendadores, algo que vemos todos los días cuando estamos haciendo una compra en Amazon o cuando estamos en Netflix. Al final, siempre tenemos recomendaciones. Estamos hablando en este caso de aprendizaje no supervisado.

En el caso de aprendizaje supervisado, vamos a diferenciar entre dos grandes grupos: clasificación y regresión. Con esto retomamos la idea de que los algoritmos de IA, de una forma muy sencilla y muy simplificada, lo que hacen es predecir; esto sería regresión, predecir un precio. Clasificación sería predecir si va a suceder algo o no, y clustering, que es agrupar. Y un tercer bloque, que sería un poco más técnico y de nicho, sería el Reinforcement Learning. El Reinforcement Learning es una técnica que se utiliza sobre todo en el ámbito de los videojuegos. Es la típica afirmación de "estoy jugando contra la máquina" o "la máquina me ha vencido". Al final, este tipo de aprendizaje se basa en un sistema de castigo o recompensa. Así que es cierto que está cogiendo bastante forma y se está utilizando ahora un poco más incluso en el trading. Ya existen modelos de reinforcement learning que se están utilizando para el tema del trading, porque al final, por este castigo y recompensa, aprende a hacer trading en corto. Esto sería el concepto de los market makers, pero aún son modelos experimentales y falta por demostrar si pueden sustituir o reemplazar de forma eficiente en este campo a la programación tradicional.

### Introducción al Machine Learning
#### ¿Qué es Big Data?
![[488.E1_Introducción_al_Machine_Learning.png]]

El Big Data es la gasolina de nuestros algoritmos. 

Cuando hablamos de Big Data, tenemos que hablar de las cuatro uves:
1. Volumen
2. Variedad
3. Velocidad
4. Veracidad

##### 1 Volumen
**Necesitamos cantidades inmensas de datos.**
A más datos, más precisión, así que es el principal objetivo.

##### 2 | Variedad
**También necesitamos variedad, datos heterogéneos. Es mucho mejor tener datos heterogéneos que homogéneos para que nuestro algoritmo vea muchísimas casuísticas.**

Es como el ejemplo del padre que le enseña al niño lo que es un vaso. Es mejor que vea mil modelos de vasos que diez modelos de vasos, porque cuando salga a la calle y tenga que detectar lo que es un vaso, a más ejemplos vistos, más capacidad de reconocimiento.

##### 3 | Velocidad
**Básicamente, nos referimos a la capacidad de ingesta de los datos.**
En este caso haría referencia a la capacidad también que tenemos hoy de trabajar con datos en streaming. Los algoritmos se entrenan constantemente a medida que más datos entran y van mejorando su precisión.

##### 4 | Veracidad
**La calidad del dato.**
Hay que procesar y transformar los datos para, posteriormente, visualizarlos para comprobar que no existan incongruencias. Pueden haber sucedido eventos específicos en una fecha, un caso aislado, y que esto cree una distorsión en el planteamiento del algoritmo.

##### Google Teachable Machine
**Este es un ejemplo perfecto para ver el funcionamiento de las 4 V.**
Para ello, debéis ir [este enlace](https://teachablemachine.withgoogle.com/train/image) y que entréis un detector de objetos.
Aquí lo que veréis es que vamos a entrenar un algoritmo de visión artificial. 
- Lo que va a hacer es clasificarlos en diversas clases, las que nosotros determinemos. 
- Elegid cualquier objeto que tengáis en casa, podéis coger una botella, un lápiz y, simplemente, lo que vamos a hacer, es configurarlo, le asignaremos una clase.
- Al presionar en la cámara veréis que lo que os genera, es una cantidad infinita de imágenes. 
- Ahora tenéis que mover el objeto en muchas posiciones para comprobar el efecto de variedad. 
- Seguidamente, solo hay que seguir los pasos, entrenar el modelo y verificar su performance.

##### Dataset
**La forma tangible del Big Data, sería el Dataset: Un conjunto de datos.**
El Big Data puede llegarnos en forma de Dataset de diversas formas. Una muy simple y común sería un archivo txt. Simplemente, deberemos transformarlo para llevarlo a nuestro terreno. 

No obstante, el tipo de archivo dominante, es el CSV. También podemos tener ficheros excel o incluso archivos PDF. Cabe destacar también que el sonido también son datos, aunque hay que remarcar aquí que el sonido como tal no existe.  
  
Para el Machine Learning se convierte en un espectrograma, con lo cual pasa a ser una imagen. También podemos tener videos e imágenes en nuestro Dataset.

Vamos a diferenciar entre dos grandes grupos.
1. Datos Tabulares (el más común)
2. Visión Artificial (imágenes o vídeo)

##### Vamos a centrarnos en el primer grupo, Datos Tabulares.
- Vamos a imaginar una tabla de Excel con filas y columnas, un Dataset de calidad con de 5000/10000 filas hacia adelante de 8 a 10 columnas. 
- Esto serían nuestras variables y ahí estaría nuestro Target, nuestro objetivo a predecir. 
- Es cierto que se pueden llegar a hacer pruebas de concepto con filas utilizando Machine Learning, pero no es recomendable. 

Si queremos hablar de un buen tamaño, estaríamos hablando de 100.000 filas para garantizar una buena calidad y esto nos permite poder emplear todo tipo de algoritmos, tanto Machine Learning como Deep Learning.

Cuando abrimos un Dataset de datos tabular nos podemos encontrar diferentes tipos de datos: 
- **Por un lado, tenemos los datos Double,** que serían simplemente números con fracción, por ejemplo, 1.7854. 
- **También podemos encontrar tipos de data como integral.** En este caso no habría partes fraccionales, por ejemplo, un 7.
- **También tenemos las Strings, que hacen referencia a categorías:** Imaginemos que estamos hablando de una variable donde simplemente añadimos si un paciente padece algún tipo de enfermedad y tenemos cinco opciones, una de ellas la diabetes, esto sería una string usando una categoría. No estamos hablando de números, estamos hablando de palabras. 
- **Timestamp: Como su nombre indica, es simplemente una fecha (Día / Mes / Año).** Simplemente, tenemos que tener en cuenta que hay que enseñarle a nuestro sistema que al marcarlo como Timestamp, estamos diciendo que no es un número corriente, sino que está haciendo una relación o un evento temporal sucesivo. 
- **También podemos tener un diccionario:** Un caso muy común del diccionario sería cuando vamos al supermercado y hacemos nuestra lista de la compra. Ahí tenemos un diccionario de artículos de palabras con los objetos, con los ítems que hemos comprado. 
- **Imágenes:** En PNG o JPG.
- **Documentos de Texto:** Por ejemplo, reviews.

Este sería un resumen del abanico de tipología de datos que podemos encontrar. Nuestra primera misión es en el momento que abrimos un Dataset, será identificar los diferentes tipos y dependiendo del objetivo que queramos lograr, ver qué tratamiento hay que aplicar a cada tipo de datos.

### Machine Learning (Video)
![[489.E1_Machine_Learning.mp4]]
[Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866154-u5-1-2-machine-learning-jose-peris)

**Cómo evaluamos nuestro algoritmo**

Aquí vamos a diferenciar dos grandes grupos y recordad que siempre hablamos desde el punto de vista de aprendizaje supervisado, cuando tiene etiqueta, cuando hay una mano detrás, cuando sabemos el resultado de antemano. Los grandes grupos son, uno, la regresión, y otro, la clasificación.

La regresión, ¿cómo vamos a evaluarla? Como podéis ver en la gráfica, básicamente lo que tenemos son los puntos de datos y nuestro algoritmo se vería representado como la línea verde que tenemos. Esta línea verde representa una regresión lineal. Si os fijáis, lo que ocurre es que pasa más o menos por el medio de la nube de puntos. ¿Qué significa esto? Significa que esa recta tiene una fórmula matemática y esa fórmula matemática es la que nos permite predecir dónde va a caer el próximo punto.

¿Cómo medimos la precisión? Lo que hacemos es comparar nuestras predicciones frente a los resultados reales y medimos la distancia de cada punto real respecto a nuestra recta. A partir de aquí, midiendo esta distancia para cada punto, vamos a sacar una media y esa será la media de nuestro error. Esto es cuando estamos hablando de regresión, donde el valor podría ser, por ejemplo, 1824 euros.

¿Cómo evaluamos la clasificación? La clasificación se evalúa mediante un instrumento llamado matriz de confusión, y el nombre está muy bien puesto porque realmente estamos hablando de elaborar una matriz donde vamos a clasificar los diferentes resultados. En este caso, por ejemplo, vamos a tener dos columnas: una será para los resultados reales y dos filas donde estarán los resultados del algoritmo. Aquí, básicamente, lo que hacemos es contar cuántas veces hemos acertado y cuántas veces hemos fallado en la clasificación.

Este concepto es un poco lioso, pero el hecho de que hemos pasado por una pandemia ha ayudado a entenderlo un poco mejor, porque esto se basa en falsos positivos y falsos negativos. ¿Qué ocurre con el coronavirus, por ejemplo? Tenemos el caso de falso positivo y falso negativo. A día de hoy, todos sabemos ya lo que significa. Te pueden diagnosticar el coronavirus y no lo tenías; eso sería un falso positivo. Pero también hay un falso negativo: te pueden diagnosticar que no tienes coronavirus y sí que lo tienes, y ya has contagiado a todo tu entorno. Este es el concepto clave para la clasificación, porque cuando clasificamos si es sí o no, tenemos que ver si es un falso positivo o un falso negativo.

A continuación, os voy a dejar aquí un ejemplo interactivo donde podéis experimentar un poco con el concepto de la matriz de confusión. Lo que vais a ver son dos grupos de imágenes: un clasificador que representa imágenes de comida y imágenes que no representan comida. Lo que tenéis que hacer es clasificar las que van a un lado. Tenéis que pensar que para evaluar una clasificación es muy importante tener en cuenta cuántos hemos acertado. En este caso, miraríamos los true positives y los true negatives, y cuánto hemos fallado, los false positives o false negatives. Lo que hacemos es comparar los que hemos acertado contra los que hemos fallado.

Lo que pasa es que no es lo mismo fallar prediciendo una enfermedad que fallar prediciendo si una campaña de marketing va a funcionar o no; es aquí donde debemos establecer nuestro umbral. Existen otros métodos más avanzados, pero a nivel general vamos a crear un método más avanzado con estos conceptos básicos de medición y evaluación de algoritmos.

Vamos a seguir hablando de los datasets y, sobre todo, de un concepto muy importante que es el entrenamiento y el test. Fijaros que estamos hablando de que tenemos que medir y evaluar la predicción de nuestros algoritmos, ya sea en regresión como en clasificación. Para ello, lo que vamos a hacer es que nunca vamos a trabajar con el 100% de los datos. ¿A qué me refiero con esto? Imaginaros que me envían un dataset de 100.000 filas y 15 columnas, 15 variables.

El primer paso que tenemos que hacer es, aparte de realizar un análisis del negocio, llevar a cabo un proceso de ETL. Lo siguiente que haríamos sería partir el dataset en entrenamiento y test. Hay diversas formas de partirlo; normalmente es un 80-20, aunque puede ser un 70-30. También hay formas de incluso partirlo más, en entrenamiento, test y validación, pero en este punto en el que nos encontramos vamos a hablar de entrenamiento y test.

Lo que vamos a hacer siempre es partir nuestro dataset: el 80% será de entrenamiento y el 20% será de test. ¿Esto qué significa? Que desde la fila 0 hasta la fila 80.000 serán nuestros datos de entrenamiento y desde la fila 80.000 hasta la 100.000 serán los datos de test. ¿Por qué? Porque lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos y, una vez esté entrenado, lo que vamos a hacer es, imaginemos que seguimos con el ejemplo del algoritmo de predicción de si un cliente va a pagar un crédito o no, para saber si se lo podemos ofrecer o no.

En este caso, lo que sucedería es que, una vez nuestro algoritmo está entrenado con el 80% de los datos, nosotros simplemente cogeríamos los datos del test, ese 20% donde lo que veríamos realmente sería un ID de un usuario y todas sus características, todo su histórico, todo su histórico de cobros, de pagos, de ingresos y de gastos con el banco, y el algoritmo aquí haría una predicción.

¿Qué ocurre? Que como tenemos el dato real porque hemos partido en 80-20, podemos contrastar la predicción del algoritmo con el dato real: predicción contra realidad. Siempre es este el patrón que seguimos, por tanto, analizaríamos punto a punto, ¿cuánto hemos acertado o cuánto hemos fallado? Si estamos hablando de regresión, será un número; estaremos hablando de un error que, haciendo la inversa, es la precisión. Si estamos hablando de clasificación, en una primera instancia lo haríamos con una matriz de confusión y, de forma más avanzada, ya utilizaríamos análisis como el ROC o AUC. Pero nos vamos a quedar en esta base, en que para medir y poder trabajar, tenemos que partir siempre en entrenamiento y en test.

Vamos a hablar sobre la parte más importante en Machine Learning y en Inteligencia Artificial, que son las variables. Recordad la estructura que es aumentada de un dataset: tendríamos muchas filas y tendríamos columnas. Estas columnas son las variables. Las variables, realmente, de una forma entendible, serían aquellos parámetros que explican todo el contexto.

Vamos a llevarlo a cabo con un caso muy fácil y entendible, como puede ser el caso de la hostelería: un bar, un restaurante, un hotel. Pongamos que un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar a dos semanas vista. Si pensáis bien, si sabemos cuánto vamos a facturar y cuántos trabajadores vamos a necesitar, tenemos mucha ventaja competitiva porque realmente sabemos que no va a haber rotura de stock, sabremos que va a haber un buen servicio, podemos preparar la sala y el personal para un determinado escenario, con lo cual estamos hablando de un retorno considerable. Esto es el rol de trabajar con Machine Learning y con IA.

Sigamos en este caso. Imaginad que el caso común es que un cliente, por ejemplo, te dé su base de datos y te diga: "Bueno, aquí tienes mi base de datos". Lo que vas a tener es un timestamp, como hemos dicho anteriormente, porque tendremos fechas. Tendremos un histórico de fechas; pongamos que tenemos 10 años, y al lado tendremos una columna con la facturación de cada día. A día de hoy, lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo realmente a ojo.

Entonces, ¿qué significa hacerlo a ojo? Pues bueno, que si calculamos un incremento o un descenso de x, porque este año es mejor o es peor. Esto nos lleva a que son predicciones muy inexactas, que rozan el 50%. Un 50% en Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada. Vamos a ver cómo configuraríamos este algoritmo, estas variables, para poder realizar un buen algoritmo.

Lo primero es pensar con lógica. Nosotros tenemos una fecha, un timestamp, y una facturación, unos datos de facturación en euros. De una forma muy sencilla, ¿creéis que se factura lo mismo en hostelería un sábado o un viernes que un lunes o un martes? Evidentemente no. Aquí hay un componente psicológico en el cual un lunes o un martes va a ser difícil que la gente acuda más a los bares; sin embargo, un viernes, un sábado o un domingo sí. Sabemos que, obviamente, esto va a afectar a la facturación. ¿En qué día de la semana estoy? Va a afectar directamente a mi facturación. Por tanto, el día de la semana es una variable muy importante que, haciendo ETL, la podemos añadir a nuestro dataset. Esto se traduciría en que si el día de la semana es el 1 o es el 7, o es el 2 o es el 3.

¿Qué otra variable podría afectar? Por ejemplo, la variable de festivos. Si es Halloween, es obvio que ese día es probable que afecte la facturación, o incluso la víspera. Por tanto, el factor festivo en el calendario nacional es una variable también muy importante. Pero pensad que este bar, este restaurante o este hotel está en España. Cuando está lloviendo, la gente va a los bares o a los restaurantes; les cuesta muchísimo más salir de casa. Sin embargo, cuando hace sol, nos lanzamos a la calle. Por tanto, la temperatura va a influir claramente también en nuestro objetivo, que se denomina Machine Learning, que es la facturación.

O, por ejemplo, las precipitaciones, la lluvia, que también podríamos englobar dentro de esta misma variable. Pero tenemos que tener en cuenta que hay otro tipo de variables que pueden afectar, porque imaginaros que alrededor de este restaurante se realiza un evento, hay un fin de semana de conciertos donde vienen artistas de todo el mundo. ¿Qué va a pasar? Que va a haber mucha más gente alrededor. Por lo tanto, esto en Machine Learning lo veríamos como un evento: si hay evento, sería un 1, y si no hay evento, sería un 0, por ejemplo. Así es como traduciríamos esta información de forma binaria.

Pero podrían afectar también otra serie de variables, como, por ejemplo, qué día del mes es. No es lo mismo; no gastamos lo mismo el día 5 que el día 30. Los ánimos no son los mismos. Pero, por otro lado, también hay otras variables que podrían influir, que son Open Data, que son variables externas. Imaginaros que la tasa de paro juvenil se sitúa en un 40%. Esto va a afectar también; esta variable externa, este Open Data, va a afectar. O digamos que el PIB ha bajado muchísimo. Todo este tipo de variables son lo que nosotros denominamos variables, y es aquí donde está la génesis de la creación de un buen algoritmo.

Es realmente la clave comprobar qué variables son las que mejor explican este suceso o evento. A partir de aquí, ya podríamos, una vez hemos conformado nuestro dataset, hemos añadido esta serie de variables y hemos comprobado su importancia, pasar a partir en entrenamiento y test y desarrollar nuestro algoritmo de nuestra caja de herramientas, ajustarlo y conseguir la precisión adecuada.

Para que os hagáis una idea, estaríamos hablando en términos cuantitativos. Por ejemplo, en términos de precisión, estaríamos hablando de que si simplemente lo que vamos a tener es un timestamp de una columna con un valor en euros, podríamos situarnos en precisiones alrededor del 60%. En el momento en que añadimos variables, variables que apuntan a qué pasó en esos días y utilizamos Open Data, podríamos subir a precisiones del 90%, incluso del 95%. Lo cual, como podéis comprender, puede ser una ventaja competitiva muy potente.

El campo del Machine Learning y la Inteligencia Artificial es un campo técnico, es cierto, pero no todo es desarrollo y tecnicismos. Realmente puede acceder gente desde el ámbito del negocio que va a tener que aprender una serie de técnicas que son perfectamente alcanzables. Al final, realmente lo que importa es una buena combinación de la parte táctica y estratégica con la parte técnica.

### Extract, Transform and Load (ETL)
Es, quizás, el proceso más pesado y largo para un científico de datos; los Algoritmos son herramientas de una caja de herramientas que se llama Machine Learning o Inteligencia Artificial.

Hay que pensar que cada herramienta trabaja con un tipo de tornillo diferente en unas condiciones diferentes. Por tanto, nuestra primera misión, como científicos de datos, es analizar los tipos de datos que tenemos y cómo tenemos que prepararlos para los algoritmos que vamos a utilizar. 
- El principal problema que solemos encontrar son los **valores nulos,** situación bastante común, ya que hay diferentes variables (columnas). Los valores nulos, muchas veces, vienen marcados por un NAN o un simple interrogante.
- **Lo primero que tenemos que hacer siempre es visualizar los datos,** porque si no visualizamos los datos, nunca vamos a entender la historia.
- Cuando dibujamos las gráficas de las diferentes variables vamos a observar que distribución tienen los datos. Vamos a observar también, cuántos valores nulos o diferentes incongruencias tenemos. 
- **Este proceso de ETL puede ocupar hasta el 80% de todo el proceso de desarrollo de un algoritmo** y, simplemente, implica ir comprobando, columna a columna, el tipo de datos que tenemos para ver qué transformación le tenemos que aplicar para trabajar de forma correcta con nuestro algoritmo.

#### Privacidad
Para que podamos trabajar de forma correcta con nuestros algoritmos, la Privacidad es un tema bastante recurrente.

En el campo del machine learning, los datos se tratan siempre de forma anonimizada, es decir, para nosotros no tiene valor el nombre de una persona; se trata de trabajar con ID's.

Pero es cierto que si necesitamos practicar y entrar en este mundo tenemos repositorios como Google Dataset, donde podemos descargar Dataset de todo tipo de casos de uso. 

Esto nos será muy útil para poder entrenar a nuestro Algoritmo y ampliar nuestro conocimiento y destreza.

#### El proceso de desarrollo de un algoritmo. 
##### 1 | Data Collection 
Esta parte es muy importante, pues, normalmente, quien mejor captura los datos es quien mejores algoritmos desarrolla.

Podemos encontrar muchos escenarios para realizar esta descarga, como hacerlo desde una API o, incluso, buscarnos unos datos, por un lado, y otros por otro para después fusionarlos. 

Aquí nos podemos encontrar en múltiples escenarios.

##### 2 | Business Analysis
Para poder desarrollar un algoritmo necesitamos comprender la naturaleza del problema.

Ejemplo:
Nos piden que desarrollemos un algoritmo predictivo de ventas.
1. Briefing con la empresa, expectativas y casos de uso para comprender todos los Script Holders y variables que afectan al negocio para elaborar una primera hipótesis.
2. A través de esa hipótesis averiguaremos qué variables van a afectar y cuáles van a ser las claves para poder desarrollar este algoritmo. 

Esta parte es fundamental. De hecho, es una parte en la cual muchos científicos de datos se pueden perder por pensar demasiado en la programación. 

Hay dos partes muy bien diferenciadas
- Parte 1: Táctica, estrategia y lógica de entendimiento de negocio. 
- Parte 2: Programación y Desarrollo.

##### 3 | Programación y desarrollo
Una vez hemos hecho esta primera parte de Business Analysis de nuestro cliente, pasaríamos a analizar los datos que hemos capturado con la Data Collection.

Seguimos con el caso de la empresa.

Ahora analizaremos:
- Si los datos son internos o externos.
- Si los hemos descargado de una API o desde una base de Datos. 
- Si no nos lo van a proporcionar de forma aislada en un CSV.

Una vez hemos coleccionado estos datos, pasamos a preparar los ETL, que es donde pasamos a visualizarlos, pasamos a transformarlos, a limpiarlos y evaluamos la calidad del dato. Porque muchas veces nos va a pasar que a lo mejor partimos de 10.000 filas y cuando hacemos el proceso de ETL nos podemos quedar en 4000.

Esto es muy común cuando la calidad del dato es anómala, por lo que tenemos que pensar que el algoritmo trabaja de una forma muy matemática y muy fina, por tanto, si nuestra "gasolina" encuentra algún error, lo va a reproducir en el resultado. Esta parte es crucial y nos puede llevar muchísimo tiempo.

##### 4 | Escoger un modelo
Como decíamos, esto funciona como cajas de herramientas y, a medida que vamos avanzando en conocimiento, de un golpe de vista, ya podemos saber qué tipo de modelo puede encajar en nuestro problema. 

Una vez hecho esto, pasamos a entrenar el modelo. Luego explicaremos que significa un entrenamiento de un algoritmo.

##### 5 | Evaluación final
Nuestro modelo coge unos datos, aprende unos históricos, realiza unas predicciones y contrastamos los resultados con datos de test, y lo haremos así porque no vamos a utilizar todos los datos.

Nosotros vamos a lanzar predicciones que vamos a contrastar con la realidad. 

También es común iterar con el Parámetro Tuning.

Imaginemos que hemos llegado a una predicción de una precisión del 80%. pero no es suficiente para nuestro caso de uso. Este valor puede ser bueno para una predicción de ventas.

Pero en el caso de predecir una posible enfermedad, no podemos errar un 20%, por lo que estableceremos nuestros umbrales e iteraremos, pasando al Parámetro Tuning, que no es otra cosa que optimizar los parámetros del algoritmo para mejorar la precisión.

##### 6 | Deploy o Puesta en Producción
Imaginemos que somos un banco y queremos saber a qué potenciales clientes les podemos ofrecer un crédito.

¿En qué se basan?: En el historial de ingresos, gastos, impagos, cobros, etc. Imaginad si tenemos que hacer una revisión manual de una BBDD de millones de personas, fila por fila, ID por ID… Sería una labor muy tediosa.

¿Cómo se entrenaría un algoritmo de machine learning para hacer esta predicción?

Descargaremos un Dataset con un histórico de 10 años con unas características y al lado tendríamos una etiqueta supervisada, es decir, una columna donde tendríamos el histórico de si este usuario ha pagado o no, si se le debe dar un préstamo o no, ¿Por qué? Porque esto es algo que se ha ido registrando durante el tiempo.

A partir de aquí, podemos crear un Algoritmo Predictivo de forma que, cuando entre un usuario nuevo y este algoritmo esté en funcionamiento, hará una predicción si a este usuario se le puede o no ofrecer un préstamo.

¿Por qué? Basándonos en su historial.  

Aquí estamos hablando de que en él dataset contenía una columna, donde si teníamos el histórico de un "sí" o un "no", de impago o de pago.

**Esto es supervisado, significa que tenemos una etiqueta donde colocamos el resultado de antemano en el pasado. Vamos a profundizar sobre esto en la siguiente sección.**

### Aprendizaje Supervisado y Aprendizaje No Supervisado
Cuando hablamos del lenguaje de aprendizaje no supervisado nos referimos a que los datos nos llegan desordenados.

Imaginemos fichas de diferentes formas: Triángulos, cuadrados, círculos y nos llegan todas las fichas desordenadas.

Cuando ese aprendizaje no está supervisado lo que vamos a hacer es ordenarlas y clasificarlas. Dicho de otra manera: Clusterizarlas.

#### Clusterización
La Clusterización sería ordenar esas formas; poner los triángulos con los triángulos, círculos con círculos y cuadrados con cuadrados.

![[491.E1_Aprendizaje_Supervisado_y_No_Supervisado_1.png]]

Diferenciemos.
1. **No supervisado:** Datos desordenados.
2. **Supervisado:** Etiquetado y clasificado, sabemos el histórico y el resultado.
3. **Reinforcement Learning:** Este es un campo más técnico y de nicho.

Todo esto nos define el tipo de algoritmos que vamos a utilizar.

#### 1 | No Supervisado
Los más comunes, en este caso, son conocidos como algoritmos de Clustering.

Por ejemplo: 
Un cliente entra a una web de e-commerce, donde, a día de hoy, hay un algoritmo que, basándonos en sus características y a su patrón de uso como experiencia de usuario, ya lo clasifica con un tipo de cliente o de buyer-persona.

Los podría clusterizar como:
- Cliente arriesgado
- Cliente conservador
- Cliente dudoso

**Otro ejemplo son los Sistemas Recomendadores,** algo que podemos ver todos los días mientras realizamos una compra en Amazon o cuando estamos en Netflix, pues al final siempre nos aparecen recomendaciones de otros productos, de series o de películas basándose en lo que consumimos habitualmente.

#### 2 | Aprendizaje Supervisado
En el caso de aprendizaje supervisado vamos a diferenciar entre dos grandes grupos:
1. Clasificación
2. Regresión

Con esto retomamos la idea de que los algoritmos de guía, de una forma muy sencilla y simplificada, lo que hacen es:
- **Regresión:** Predecir un precio.
- **Clasificación:** Predecir si va a suceder algo o no.
- **Clustering:** Agrupar.

#### 3 | Reinforcement Learning
Esta es una técnica que se utiliza, sobre todo, en el ámbito de los videojuegos. 
- Es la típica afirmación: “No estoy jugando contra la máquina” o “La máquina me ha vencido”.
- Este tipo de aprendizaje se basa en un sistema de castigo o recompensa, así que está cogiendo bastante forma y se está empezando a implementar incluso en el trading, donde ya existen algunos modelos.
- Por este castigo y recompensa se puede aprender a hacer trading en short.
- Esto sería el concepto de los Market makers, aunque todavía se trata de modelos experimentales y falta por demostrar si pueden sustituir o reemplazar, de forma eficiente en este campo, a la programación tradicional.

![[491.E1_Aprendizaje_Supervisado_y_No_Supervisado_2.png]]

#### Evaluando nuestro algoritmo
¿Cómo evaluamos nuestro algoritmo? Siempre teniendo en cuenta de que hablamos del Aprendizaje Supervisado, cuando tiene etiqueta, cuando hay una mano detrás, cuando sabemos el resultado de antemano.

Recordemos los dos grandes grupos: la Regresión y la Clasificación.

#### Evaluando la Regresión
Lo que tenemos son Data Points y nuestro algoritmo se vería representado con la línea verde, que no es más que una Regresión Lineal que pasa más o menos por en medio de la nube de puntos.

**¿Qué quiere decir esto?**
Significa que esa recta tiene una fórmula matemática, y que esa fórmula nos permite predecir donde va a caer el próximo punto.

**¿Cómo medimos la precisión?**
Lo que hacemos es, ver nuestras predicciones frente a los resultados reales y, lo que vamos a hacer es medir la distancia de cada punto real respecto a nuestra recta.

A partir de aquí vamos a ir midiendo esta distancia a cada punto y vamos a sacar una media, que va a ser la media de nuestro error.

Cuando estamos hablando de Regresión, donde el valor sería, por ejemplo, 1824 €.

#### Evaluando la Clasificación
Evaluamos la Clasificación mediante un instrumento llamado Matriz de Confusión.
- El nombre está muy bien puesto porque, realmente, vamos a elaborar una matriz donde vamos a clasificar los diferentes resultados.
- En este caso vamos a tener dos columnas, en una van a ir los resultados reales y dos filas a ir los resultados del algoritmo. 
- Aquí básicamente lo que hacemos es contar cuantas veces hemos acertado y cuantas veces hemos fallado en la clasificación.

> Este concepto, que puede resultar confuso, pero hemos pasado una pandemia que ha contribuido a que lo entendamos un poco mejor. ¿Por qué?

Porque se basa en falsos positivos y en falsos negativos.

¿Qué ocurre con el Covid? Tenemos casos de falsos positivos y falsos negativos y, a día de hoy, todos sabemos lo que significa.
1. **Falso positivo:** Te pueden diagnosticar con un virus y no lo tenías.
2. **Falso negativo:** Te dicen que estás bien, pero tenías el virus.

¡Ya has contagiado a todo tu entorno!.

Este es el concepto clave para la clasificación porque cuando clasificamos, SÍ es SÍ o es NO, tenemos que comprobar si se trata de un falso positivo o un falso negativo.

**En conclusión, tenemos que medir y evaluar la predicción de nuestros algoritmos, ya sean tanto de Regresión como de Clasificación.**

Para ello, lo que nunca vamos a hacer, es trabajar con el 100% de los datos. 

Imaginemos que nos envían un dataset de 100000 Filas y 15 Columnas, 15 Variables. 

El primer paso será, aparte de un primer análisis del negocio o un proceso de ETL, partir el Dataset en entrenamiento y Testing.

### Variables
#### Nunca debemos trabajar con el 100% de los datos.
**Imaginemos, por ejemplo, que nos envían un Dataset de 100000 Filas y 15 Columnas: 15 Variables.**

El primer paso será, aparte de un primer análisis del negocio o un proceso de ETL, partir el Dataset en Entrenamiento y Testing.  

Hay diversas formas de partirlo. Normalmente, es un 80/20, aunque puede ser un 70/30. Y hay formas de incluso partirlo más: 
- Entrenamiento
- Test
- Validación

#### Entrenamiento / Test
**Cuando decimos 80/20, nos referimos a que el 80% será de entrenamiento y el 20% será de test.**
- Desde la fila 0 hasta la fila 80.000 van a ser nuestros datos de entrenamiento.
- Desde la fila 80.000 hasta la 100.000 van a ser los datos de test.

Seguimos con el ejemplo práctico de predecir si un cliente va o no a pagar un crédito.
1. Lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos.
2. Una vez entrenado ese 80%, usaremos el 20% restante para tener un ID de un usuario, todas sus características y todo su histórico de movimientos con el banco (pagos, ingresos, gastos) y el algoritmo aquí haría una predicción.
3.  Como tenemos el dato real porque hemos partido en 80/20, podemos contrastar la predicción del algoritmo con el dato real, pues nuestro patrón siempre es Predicción vs Realidad.
4.  Analizaremos, Data Point a Data Point, cuanto hemos acertado o fallado.
5.  Si estamos hablando de Regresión, el resultado será un número, estaremos hablando de un error.
6.  Este error, haciendo la inversa, nos da la precisión.
7.  Si, en cambio, estamos hablando de Clasificación, lo haríamos con una Matriz de Confusión y de forma más avanzada, como el ROC o el AUC.

  

### Las Variables
**Estamos hablando de la parte más importante en Machine Learning y en Inteligencia Artificial.**

Para ello, vamos a recordar cómo estaba compuesta la estructura de un Dataset: Filas y columnas. Las columnas son las variables.

Las variables son aquellos parámetros que explican todo el contexto. 

Vamos a imaginar un caso práctico como la hostelería (un bar, restaurante u hotel). 
- Un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar. 
- Si sabemos estos datos, tendremos mucha ventaja competitiva porque, realmente, sabemos que no va a haber rotura de stocks y habrá un buen servicio. 
- Podemos preparar la sala y el personal para un determinado escenario, con lo cual estamos hablando de bastante retorno. Este es el ROI de trabajar con Machine Learning e IA.

El caso más habitual es que un cliente te proporcione su base de datos, con lo que vas a tener un Timestamp, un histórico de fechas, por ejemplo,10 años. 
- Al lado de cada fecha, tendremos una columna con la facturación de cada día. 
- Lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo a ojo. ¿Y qué implica hacerlo a ojo? 
- Si calculamos un incremento un descenso de X porque este año es mejor o es peor, lo cual nos lleva a que a predicciones muy inexactas que rozan el 50%.  
- En Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada.

Vamos a ver cómo configurar estas variables, para poder realizar un buen algoritmo.

Lo primero es pensar con la lógica. Nosotros tenemos una fecha Timestamp y una facturación, unos datos de facturación en euros. Vamos a seguir con el caso práctico en la siguiente unidad.
## Nunca debemos trabajar con el 100% de los datos.
**Imaginemos, por ejemplo, que nos envían un Dataset de 100000 Filas y 15 Columnas: 15 Variables.**

El primer paso será, aparte de un primer análisis del negocio o un proceso de ETL, partir el Dataset en Entrenamiento y Testing.  

Hay diversas formas de partirlo. Normalmente, es un 80/20, aunque puede ser un 70/30. Y hay formas de incluso partirlo más: 
- Entrenamiento
- Test
- Validación

## Entrenamiento / Test
**Cuando decimos 80/20, nos referimos a que el 80% será de entrenamiento y el 20% será de test.**
- Desde la fila 0 hasta la fila 80.000 van a ser nuestros datos de entrenamiento.
- Desde la fila 80.000 hasta la 100.000 van a ser los datos de test.

Seguimos con el ejemplo práctico de predecir si un cliente va o no a pagar un crédito.
1. Lo que vamos a hacer es entrenar nuestro algoritmo con este 80% de los datos.
2. Una vez entrenado ese 80%, usaremos el 20% restante para tener un ID de un usuario, todas sus características y todo su histórico de movimientos con el banco (pagos, ingresos, gastos) y el algoritmo aquí haría una predicción.
3.  Como tenemos el dato real porque hemos partido en 80/20, podemos contrastar la predicción del algoritmo con el dato real, pues nuestro patrón siempre es Predicción vs Realidad.
4.  Analizaremos, Data Point a Data Point, cuanto hemos acertado o fallado.
5.  Si estamos hablando de Regresión, el resultado será un número, estaremos hablando de un error.
6.  Este error, haciendo la inversa, nos da la precisión.
7.  Si, en cambio, estamos hablando de Clasificación, lo haríamos con una Matriz de Confusión y de forma más avanzada, como el ROC o el AUC.

### Las Variables
**Estamos hablando de la parte más importante en Machine Learning y en Inteligencia Artificial.**

Para ello, vamos a recordar cómo estaba compuesta la estructura de un Dataset: Filas y columnas. Las columnas son las variables.

Las variables son aquellos parámetros que explican todo el contexto. 

Vamos a imaginar un caso práctico como la hostelería (un bar, restaurante u hotel). 
- Un cliente nos pide que desarrollemos un algoritmo predictivo que nos permita predecir cuánto vamos a facturar a dos semanas vista o cuántos trabajadores vamos a necesitar. 
- Si sabemos estos datos, tendremos mucha ventaja competitiva porque, realmente, sabemos que no va a haber rotura de stocks y habrá un buen servicio. 
- Podemos preparar la sala y el personal para un determinado escenario, con lo cual estamos hablando de bastante retorno. Este es el ROI de trabajar con Machine Learning e IA.

El caso más habitual es que un cliente te proporcione su base de datos, con lo que vas a tener un Timestamp, un histórico de fechas, por ejemplo,10 años. 
- Al lado de cada fecha, tendremos una columna con la facturación de cada día. 
- Lo que se suele hacer cuando no hay una transformación digital por medio es hacerlo a ojo. ¿Y qué implica hacerlo a ojo? 
- Si calculamos un incremento un descenso de X porque este año es mejor o es peor, lo cual nos lleva a que a predicciones muy inexactas que rozan el 50%.  
- En Machine Learning es lanzar la moneda, con lo cual no aportas absolutamente nada.

Vamos a ver cómo configurar estas variables, para poder realizar un buen algoritmo.

Lo primero es pensar con la lógica. Nosotros tenemos una fecha Timestamp y una facturación, unos datos de facturación en euros. Vamos a seguir con el caso práctico en la siguiente unidad.

### Caso Práctico
#### Variables, Dataset, Open Data
**¿Se factura lo mismo en hostelería un sábado? ¿O un viernes que un lunes o un martes?**

Evidentemente no, aquí hay un componente psicológico: Un lunes o un martes va a ser difícil que la gente acuda más a los bares. Sin embargo, el viernes, un sábado o domingo sí y, obviamente, esto va a afectar a la facturación. 

Vamos a estudiar el porqué de posibles variables
1. Día de la semana
2. Festivos
3. Climatología
4. Eventos
5. Día del mes

##### 1 | Día de la semana 
El día de la semana es una variable muy importante y, haciendo el ETL, la podemos añadir a nuestro Dataset. Esto se traduciría si el día de la semana es el uno, o es el siete, o es el dos o tres. 

##### 2 | Festivo
Otra variable podría afectar, por ejemplo, los festivos, tanto el mismo día como la víspera. Por tanto, el factor festivo en el calendario nacional es una variable muy importante. 

##### 3 | Climatología
Tengamos en cuenta que este establecimiento hostelero está en España. Imaginemos un día de lluvia, a la gente le cuesta más salir de casa.

Sin embargo, cuando hace sol nos lanzamos a la calle, en consecuencia, la temperatura va a influir claramente también en nuestro objetivo, nuestro target, nuestra “Y”, que se denomina Machine Learning, que es la facturación. La lluvia que estaría también lo podemos englobar dentro de esta misma variable. 

##### 4 | Eventos
Pero tenemos que tener en cuenta que hay otro tipo de variables que pueden afectar, porque imaginaros que alrededor de este restaurante se realiza un evento, hay un fin de semana de conciertos. 

Esto atraerá a mucha más gente, por lo tanto, esto en Machine Learning lo veríamos como evento. Si hay evento sería un 1, y si no hay evento es un 0. 

##### 5 | Día del mes
Otra serie de variables puede ser el día del mes en el que nos encontremos. 

No gastamos lo mismo el día 5, cuando acabamos de ingresar la nómina, que el día 30, a finl de mes, pues los ánimos y el dinero disponible no son los mismos.

##### Open Data
**Pero también existen otras variables que pueden influir, los Open Data o Variables Externas.**
6. Tasa de desempleo juvenil
7. Aumento / Destenso del PIB

##### 6 | Tasa de desempleo juvenil
Imaginad que la tasa del paro juvenil se sitúa en un 40%. Esto va a afectar también esta variable externa, este Open Data va a afectar a la clientela del establecimiento, pues perderemos público de ese margen de edad.

##### 7 | Aumento / Descenso del PIB
Pensemos en un caso en que el PIB ha bajado muchísimo, la gente va a gastarse menos dinero en hostelería. 

En las variables y está la génesis para la creación de un buen algoritmo, es la clave. 

**Debemos comprobar qué variables son las que mejor explican este suceso o evento. **

Ya hemos conformado nuestro Dataset, y hemos: 
1. Añadido las variables.
2. Comprobado la importancia de estas variables.
3. Partido nuestro Entrenamiento / Test.
4. Desarrollado nuestro algoritmo.
5. Entrenar nuestro algoritmo.
6. Conseguido la precisón adecuada.

Si simplemente nos limitamos a un Timestamp y una columna con un valor de en euros, podríamos situarnos en precisiones alrededor del 60%.

> En el momento que añadimos variables, que apuntan a que pasó en esos días y utilizamos Open Data, podríamos subir a precisiones del 90%, incluso al 95%, lo que puede ser una ventaja competitiva muy potente. 

El campo del Machine Learning y la Inteligencia Artificial es un campo técnico, es cierto. Pero no todo es desarrollo y tecnicismos. 

Realmente, puede acceder gente desde el mundo del ámbito del negocio, que va a tener que aprender una serie de técnicas que son perfectamente alcanzables. Lo realmente importante es la combinación de la parte táctica estratégica con la parte técnica.

## U6. Deep Learning 
### Deep Learning (Video)
![[494.E1_Deep_Learning.mp4]]
[Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez)

Para definir lo que es la inteligencia artificial, primero hay que describir un poco lo que entendemos por inteligencia, que asociamos a nosotros como seres humanos. A mí me gusta definir la inteligencia normal, la natural o la que poseemos, como una combinación de diferentes aptitudes, como pueden ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. Y para, digamos, de alguna manera... entender la inteligencia artificial, tenemos que saber que, a día de hoy, las técnicas que nosotros llamamos inteligencia artificial son un subconjunto de tareas que son muy efectivas en el aprendizaje, es decir, a partir del conocimiento que le aportan los datos, aprender a resolver tareas.

Por tanto, la inteligencia artificial, desde un punto de vista algorítmico-tecnológico, no es una inteligencia general como la que tenemos nosotros, que es capaz de resolver todas estas tareas que he mencionado o que tiene todas estas características que he mencionado previamente. En cambio, es una serie de algoritmos que logran aprender a resolver tareas a partir de datos de entrada, y generalmente, una gran cantidad de estos datos. De hecho, estas técnicas que aprenden a partir de datos se engloban en un campo que se llama Machine Learning, el cual consiste en diferentes tipos de algoritmos y sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos de entrada. Estos datos pueden ser supervisados, es decir, con la respuesta asignada al problema que buscamos, o no supervisados, es decir, datos que no tienen a priori la respuesta al problema que buscamos, pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos.

Concretamente, lo que ha permitido que la inteligencia artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas del Machine Learning que se llaman Deep Learning. Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos. Por tanto, cuando hablamos de inteligencia artificial en las innovaciones que hemos tenido en los últimos años, en realidad estamos hablando de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas a partir de grandes bases de datos, que generalmente están supervisadas o etiquetadas.

En realidad, tanto los algoritmos de Machine Learning como los de Deep Learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos. Sin embargo, los algoritmos de Deep Learning no son diferentes, sino que son una subclase del Machine Learning. Estas subclases están basadas en redes neuronales, de las cuales hay diferentes tipos que posteriormente veremos. Este tipo de algoritmos, basados en redes neuronales artificiales, son una inspiración o una imitación algorítmica, o incluso una simplificación algorítmica, de las redes neuronales que funcionan en nuestro cerebro. Aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática.

Aunque ahora mismo estamos muy habituados a escuchar hablar de inteligencia artificial y parece un término relativamente nuevo que empezó a popularizarse a partir del año 2012, en realidad, las primeras redes neuronales surgieron en la década de los 50 y 60. En ese momento, la tecnología no estaba suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. Por dos motivos principales: estas técnicas requieren de alto poder de computación y además requieren de una gran cantidad de datos. Por tanto, estos dos factores, tanto la capacidad de computación como la cantidad de datos, han crecido de forma exponencial desde los años 60, porque la digitalización se ha ido expandiendo en todos los dominios. Así, en 2012 se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponibles era suficiente para que estas redes neuronales funcionasen. A partir de entonces, hemos podido sacar su máximo potencial.

Desde su surgimiento hasta la actualidad, ha habido diferentes intentos, o diferentes hitos, mejor dicho, en el campo de la inteligencia artificial, en los que han ido, progresivamente, superando a los seres humanos en diversas tareas. Por ejemplo, en el año 1996, Deep Blue de IBM derrotó a Garry Kasparov en ajedrez, quien era el campeón mundial en ese momento. Por lo tanto, en esa tarea concreta, la inteligencia artificial ya nos lleva muchos años superando. También podemos ver hitos muy recientes, como por ejemplo, en el año 2016, una inteligencia artificial, concretamente desarrollada por DeepMind, que es una compañía adquirida por Google, derrotó al campeón mundial del Go, un juego mucho más complejo que el ajedrez en términos de árboles de posibilidades. Además, en el año 2019, el algoritmo de OpenAI derrotó al mejor equipo del mundo en el videojuego Dota 2, que requiere coordinación de diferentes jugadores y mucha intuición, lo que demostró otro hito. Incluso en 2020, otro algoritmo de DeepMind resolvió un problema que hasta entonces no había podido resolverse: predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo, será extremadamente prometedor para hacer nuevos avances.

En general, a partir de estos hitos y esta evolución, hemos visto cómo, progresivamente, las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, y han transicionado a ser compañías basadas en los datos y, por lo tanto, en la inteligencia artificial. Ahora, las compañías más grandes del mundo son Google, Facebook, Apple, etcétera, compañías puramente basadas en datos, software e inteligencia artificial. Se prevé que esta evolución exponencial del valor que la inteligencia artificial va a aportar al mercado y a los consumidores seguirá creciendo. Por tanto, este campo es de extrema relevancia para su estudio, para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías.

El Deep Learning es un campo que ha crecido mucho en los últimos años; se han desarrollado muchas nuevas tecnologías, y cada vez es un campo que, de hecho, está adquiriendo un ritmo vertiginoso, porque cada vez se dedican más recursos y, por tanto, más novedades aparecen de forma más rápida. Por tanto, es un poco difícil mantenerse al día con todo lo que va surgiendo. Los tipos principales de redes neuronales que podemos encontrar, o los que están más establecidos y que ya han demostrado resultados muy significativos en los últimos años, son las redes neuronales totalmente conectadas, que serían las redes neuronales originales, basadas en neuronas individuales que se conectan a través de distintas capas. Luego tendríamos las redes neuronales recurrentes, que se desarrollaron específicamente para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros.

Después, tendríamos las redes neuronales convolucionales, diseñadas específicamente para trabajar con datos que están ordenados en dos o tres dimensiones. También tendríamos una subtipología de entrenamiento de redes neuronales, que serían las redes neuronales generativas adversarias. Estas redes, principalmente convolucionales, tienen la característica de que sirven para generar datos de forma artificial que son muy realistas. Finalmente, tenemos un tipo de red neuronal que ha surgido en los últimos años, los transformers, que son un tipo de red neuronal que, a través de la ingesta de grandes cantidades de datos y el desarrollo de modelos con muchos parámetros, son capaces de realizar tareas como la generación de texto y de imágenes con un realismo sin precedentes. Este tipo de redes son capaces de trabajar tanto con entradas de texto como con entradas de imagen, y, por lo tanto, pueden combinar información de ambos mundos: el del lenguaje natural, es decir, el texto, y el de la imagen. Son muy efectivas trabajando con datos no estructurados.

Una neurona es un núcleo de cálculo muy simple. Simplemente es una función que tiene como entrada un número o varios números y aplica una función de activación específica, que puede ser determinada por el programador, para obtener un número diferente al de entrada. Esta neurona, como se mencionó, tiene una o varias entradas. Estas entradas tienen asociada un peso. Por lo tanto, si una neurona tiene diversas entradas, como por ejemplo en un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa, como el número de habitaciones, los metros cuadrados, el número de baños, etcétera. Esta información entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada. Cuando combina todos estos números con estos pesos, les aplica una función de activación que convierte este número en otro, o puede ser el mismo, dependiendo de la función de activación que se utilice y el número de entrada. Esto, al final, introduce en el sistema no linealidades que permiten adaptarse a entornos de datos muy complejos.

Sabiendo ya lo que es una neurona, podemos ver cómo se crea una red neuronal. Es muy simple: una red neuronal es un conjunto de neuronas ordenadas por capas. En la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etcétera, hasta la capa de salida, que es la que nos dará la respuesta a nuestro problema, que en este ejemplo sería el precio de la casa que queremos predecir. Por lo tanto, lo que vemos es una estructura por capas, y al final lo que vamos a conseguir con esto es que cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto nos permite transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para obtener relaciones de muy alto nivel, muy complejas, que quizás no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa que estamos evaluando.

Las redes neuronales recurrentes, por su parte, están diseñadas para entender o procesar información de datos que tengan una relación temporal. Por ejemplo, en una frase, el orden de las palabras puede cambiar el significado de la misma. Por tanto, las redes neuronales recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. Estas redes neuronales procesan secuencialmente las palabras de entrada, teniendo en cuenta, a la hora de analizar cada una de las palabras, cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma.

Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate mejoró drásticamente en muy poco tiempo. ¿Por qué fue? Porque se pasó de utilizar técnicas más convencionales para la traducción a utilizar técnicas basadas en redes neuronales. Esto permitió que los resultados de traducir un texto, del español al inglés, por ejemplo, sean extremadamente robustos y prácticamente no se cometan incoherencias textuales. Cada vez que se incrementan los datos y los modelos son más grandes, este error y esta, digamos, falta de similitud con lo que diría una persona, se van reduciendo paulatinamente.

También tenemos, por ejemplo, el reconocimiento de voz, que no es más que la conversión de audio a texto y la posterior interpretación de este texto. Así como el texto escrito, el audio también es una secuencia temporal, y por lo tanto, las redes neuronales recurrentes pueden utilizarse para transcribir o convertir este audio en una cadena de texto que sea posteriormente interpretada por otra red recurrente para emitir acciones específicas en los asistentes de voz.

Por otra parte, tenemos las redes neuronales convolucionales, que están específicamente diseñadas para analizar datos organizados en dos o tres dimensiones, como es el caso de las imágenes. Al final, una imagen es una matriz bidimensional de píxeles ordenados en filas y columnas. Las redes neuronales convolucionales nos permiten utilizar una operación que se llama convolución, la cual nos permite aplicar diferentes filtros a las imágenes para poder analizarlas, procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etcétera, como características de alto nivel. Si, por ejemplo, estamos entrenando un sistema de detección de caras, las características de más alto nivel serían los ojos, la nariz, la boca, etcétera. Es decir, la red neuronal convolucional aprende a ver cómo se combinan todas estas características de bajo nivel, como bordes y contornos, para convertirlas en características de alto nivel, como boca y ojos, y seguir procesándolas hasta obtener representaciones de lo que sería una cara.

Por lo tanto, al final, las redes neuronales convolucionales trabajan en comprender la información que tienen estas matrices bidimensionales que son las imágenes. La convolución es una operación matemática que nos permite, dado un filtro, que no es más que una matriz generalmente de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante, es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente, procesada con una información específica resaltada. Las aplicaciones que podemos ver en redes neuronales convolucionales son extremadamente variadas. Por ejemplo, una de las más conocidas es el reconocimiento facial. Gracias a una red neuronal convolucional, podemos detectar las caras que hay en una imagen, es decir, dónde están esas caras dentro de la imagen, y posteriormente hacer una identificación de estas caras con respecto a una base de datos. Por ejemplo, el reconocimiento facial en China está en la orden del día, y por tanto, todos sus ciudadanos están altamente identificados de forma automática gracias a estos sistemas. Las aplicaciones de la inteligencia artificial, como cualquier otra herramienta, pueden ser positivas o negativas, dependiendo del análisis y de las consecuencias que tengan. Sin embargo, son herramientas extremadamente potentes. De hecho, volviendo al caso de China, se descubrió a un fugitivo entre 60,000 personas en un evento, gracias a este reconocimiento facial. Esto nos hace darnos cuenta del potencial que tiene esto, de la precisión a la que estos algoritmos están llegando, ya que en un contexto muy caótico, como es un evento de 60,000 personas, la inteligencia artificial, concretamente la basada en redes neuronales convolucionales para detección de rostros, es capaz de identificar a un fugitivo.

Adicionalmente, el uso de las redes neuronales convolucionales para el análisis y proceso de imágenes médicas es amplísimo, ya que nos permiten, por ejemplo, hacer segmentaciones volumétricas de imágenes en 3D, como los TAC o las resonancias magnéticas, para obtener volúmenes de regiones de interés, como pueden ser pulmones, hígado, cerebro, etcétera. Además, estas redes neuronales también se utilizan para extraer información y detectar diferentes patologías, como el COVID, nódulos y una gran variedad de enfermedades pulmonares, o incluso de cualquier otra parte del cuerpo. Estas aplicaciones están creciendo de forma constante; cada día, cada año, son más numerosas las diferentes empresas y startups que se dedican a este sector.

Evidentemente, otra aplicación muy común de las redes neuronales convolucionales son los coches autónomos. Al final, un coche autónomo es un vehículo que tiene que replicar la conducción de una persona, y para ello es primordial la evaluación de la información del entorno. Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras. Por tanto, las cámaras producen imágenes que son el objetivo ideal para ser analizadas con redes neuronales convolucionales, que detectan desde objetos en la vía, como coches, peatones, ciclistas, etcétera, hasta las líneas del carril, semáforos, entre otras muchas cosas. Por lo tanto, también son clave para este sector.

He mencionado algunas aplicaciones de las GAN, pero también podemos ver otras, como por ejemplo, la alteración de imágenes médicas. Esta sería una aplicación negativa, ya que las redes neuronales también se pueden utilizar con fines nocivos, y por lo tanto, hay que estar prevenidos ante esta clase de situaciones. En un estudio, lo que hicieron fue evaluar cómo, al alterar imágenes médicas mediante estas GAN, los científicos que realizaron el estudio eran capaces de engañar a los radiólogos expertos en observar las imágenes médicas. Lo que hicieron fue incluir en imágenes de sujetos sanos nódulos pulmonares y en imágenes de sujetos con cáncer, eliminar de forma artificial estos nódulos. Los resultados mostraron que tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico, de hecho, en más del 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales.

Otra de las aplicaciones no positivas para la sociedad que podríamos evaluar de las redes neuronales generativas serían, por ejemplo, los deepfakes, en los que somos capaces de generar, de forma artificial, vídeo y audio de una persona concreta, para hacer creer al resto que esa persona ha dicho unas declaraciones concretas o ha realizado una acción específica, induciendo error a la población sobre lo que esa persona ha hecho y, por lo tanto, dañando su imagen. De hecho, hay decenas de vídeos en Internet en los que podemos ver esta clase de situaciones.

Las redes neuronales generativas adversarias son un tipo de red neuronal que nos permite generar datos artificiales. Para resolver esta tarea de forma óptima, entrenamos dos redes neuronales: una red neuronal que es la encargada de generar estos datos artificiales y otra red neuronal que es la encargada de inferir o discriminar si estos datos son reales o falsos. Por lo tanto, cuando hacemos diversas iteraciones de entrenamiento, la red neuronal generadora se vuelve muy buena creando imágenes que se parecen a la realidad, porque intenta engañar a la red discriminadora, que es la red que intenta detectar si estas imágenes son reales o ficticias. Así, entramos en un proceso, un círculo virtuoso, en el que cada vez la imagen generadora produce mejores imágenes o más parecidas a la realidad, y la red discriminadora se vuelve cada vez más efectiva detectando estas imágenes que son falsas. Por tanto, se establece un juego de superación entre ambas redes, logrando que la red generadora, finalmente, que es nuestro objetivo, genere imágenes muy similares a las imágenes reales.

Las GAN al final se pueden utilizar para una diversidad de tareas que tengan como objetivo generar información de una distribución de datos específica que antes no existía, como por ejemplo la generación de caras. De hecho, hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe, y es extremadamente realista. Por lo tanto, las GAN nos permiten generar datos de cualquier distribución con la que las hayamos entrenado y, por tanto, replicar toda esta información. También pueden servir para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. Por ejemplo, dada una imagen en blanco y negro, se puede conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permiten realizar cualquier tipo de técnica que tenga como salida una imagen que debería ser realista, es decir, la generación de datos que son realmente plausibles, como por ejemplo, la coloración de una imagen, la generación de una cara artificial, la generación de imágenes que sean realistas para tener más datos con los que entrenar otras redes neuronales, etcétera.

Ahora tenemos los transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que han iniciado un proceso de transformación en el procesamiento del lenguaje natural. Esto permite superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. Los transformers pueden obtener, sin un coste computacional disparado, la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, algo en lo que las redes neuronales recurrentes no eran tan óptimas. Además, nos permiten hacer este tipo de relaciones y entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en términos de parámetros y, finalmente, de desempeño.

Uno de los ejemplos más disruptivos de los últimos años de transformers ha sido el GPT-3, que no es más que un generador de texto tan preciso y realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido, la comprensión de contenido a través de un resumen del mismo, la traducción de ese contenido, el diseño incluso de aplicaciones que se basen en esta generación de texto para diversas aplicaciones finales, la programación automática desde un punto de vista informático, es decir, se han creado herramientas que permiten a los desarrolladores tener una generación automática del código que desean realizar, o al menos una sugerencia que después pueden adaptar, haciendo mucho más eficiente el trabajo, y toda clase de tareas de generación de contenido online que se puedan imaginar, porque tenemos una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, podemos hacer que esta herramienta específica escriba sobre multitud de temas.

Otra aplicación que ha tenido un gran impacto en los últimos años de transformers es la generación de imágenes a partir de un texto descriptivo. Es decir, podemos generar cualquier tipo de imagen dada una descripción específica. Esto nos permite, de alguna manera, acelerar o hacer más eficiente el proceso creativo, ya que, dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto, lo que puede inspirar a distintos grupos de diseño. Incluso se han creado NFTs a partir de este arte hecho por redes neuronales, que al final aportan un valor de mercado muy relevante.

Finalmente, lo que pondría el foco es que la inteligencia artificial, y por lo tanto el Deep Learning, se centra en automatizar una serie de tareas específicas a partir de datos concretos. Estamos muy lejos de esta inteligencia artificial general, que puede estar en el imaginario colectivo por las películas, que es capaz de tomar decisiones autónomas y realizar una multitud de tareas. No, lo que tenemos son sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. Aunque la inteligencia artificial, como hemos comentado, tiene ya muchos años, es decir, se empezó a desarrollar en la década de los 50 y 60, su uso decayó debido a que no teníamos ni los datos ni el poder de computación necesarios. En esta era de desarrollo de inteligencia artificial, no se había dado cuenta de que había un problema. Esto nos ha dado la oportunidad de ver el desarrollo de inteligencia artificial, que está proporcionando un gran valor de mercado. Por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, sino de una tecnología que está aportando valor de mercado, que cada vez aporta más valor y, por tanto, es una tecnología que está impactando e impactará nuestras vidas de forma muy pronunciada.

Sin embargo, teniendo en cuenta todo lo mencionado y el valor de mercado que ya está produciendo, hay que tener en cuenta que solo estamos al principio. Este último boom, esta última serie de desarrollos en la inteligencia artificial, se han empezado a producir mayoritariamente a partir de 2012. Por tanto, solo llevamos una década viendo el potencial que estas técnicas pueden tener, pero el cambio, la adopción y la evolución de estas técnicas están creciendo de forma exponencial. Cada vez veremos aplicaciones que nos sorprenderán más. Incluso ya somos capaces de ver todas estas aplicaciones que son capaces de generar información tan real que pueden engañar al ojo humano. Esto es solo el principio; por tanto, este es un campo apasionante que, sin duda, seguiremos con detenimiento.

### Deep Learning
#### Inteligencia Artificial
Para definir lo que es la IA, primero habría que describir lo que es la inteligencia que asociamos a nosotros mismos como seres humanos.

La inteligencia normal es una combinación de diferentes aptitudes, como pueden ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. 

> A día de hoy, las técnicas que nosotros llamamos Inteligencia Artificial son un subconjunto de tareas que son muy buenas en la tarea del aprendizaje, es decir, a partir de el conocimiento que le aportan los datos: Aprender a resolver tareas.

La Inteligencia Artificial, desde un punto de vista algorítmico o tecnológico, no es como la nuestra, capaz de resolver todas estas tareas que hemos mencionado previamente, sino es una serie de algoritmos que consiguen aprender a resolver tareas a partir de una gran cantidad de datos..

Estas técnicas que aprenden a partir de datos, se engloban en un campo que se llama Machine Learning, el cual es justo eso, diferentes tipos de algoritmos de sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos.

Estos datos pueden ser:
1. **Supervisados:** Con la respuesta asignada ya al problema que buscamos. 
2. **No supervisados:** Datos que no tienen, a priori, la respuesta al problema que buscamos.

Pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos y concretamente **lo que ha permitido que la Inteligencia Artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas del Machine Learning que se llaman Deep Learning.** 

Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos y, por tanto, cuando o en la mayoría de ocasiones que estamos hablando de Inteligencia Artificial en las innovaciones que hemos tenido en los últimos años, estamos hablando en realidad de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas y a partir de grandes bases de datos que generalmente están supervisadas o etiquetadas.
  
![[495.E1_Deep_Learning.png]]

##### Diferencias etre Machine Learning y Deep Learning
**La diferencia primordial es el tipo de algoritmos que utilizan.**

Tanto los algoritmos de Machine Learning como los de learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos. 

> Los algoritmos de Deep Learning son una subclase del Machine Learning.

Estas subclases están basadas en redes neuronales, de las cuales hay diferentes tipos que posteriormente veremos y este tipo de algoritmos basados en redes neuronales artificiales que simplemente son una inspiración o una imitación algorítmica, o una simplificación incluso algorítmica de las redes neuronales que funcionan en nuestro cerebro. Aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática.

##### Primeros algoritmos basados en Internet
Ahora mismo estamos muy habituados a escuchar hablar de Inteligencia Artificial y parece un término relativamente nuevo que empezó a popularizarse a partir del año 2012, más o menos de forma muy pronunciada. 

En realidad, las primeras redes neuronales surgieron en la década de los 50, 60 más o menos. En ese momento no estaba la tecnología suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. 

Por dos motivos principales: 
1. Poder de computación
2. Gran cantidad de datos

Estos dos factores han sido desarrollados de forma exponencial desde los años 60, ya que, la digitalización se ha ido expandiendo en todos los dominios. En 2012, se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponible era suficiente como para que estas redes neuronales empezasen a funcionar. 

A partir de entonces, es cuando hemos podido sacar su máximo potencial. Pero, desde su surgimiento hasta la actualidad, han sucedido diferentes intentos o diferentes hitos en el campo de la Inteligencia Artificial, en el que han ido progresivamente superando a los seres humanos en diversas tareas:

- **En el año 1996, el algoritmo Deep Blue de IBM derrotó a Gary Kasparov,** que era el campeón mundial de ajedrez en ese momento y en esa tarea concreta la Inteligencia Artificial. Ya nos lleva muchos años superando. 
- **En el año 2016, una Inteligencia Artificial concretamente desarrolla Deep Mind,** una compañía que fue adquirida por Google, derrotó al campeón mundial del GO, que es un juego mucho más complejo que el ajedrez en términos de árboles de posibilidades. 
- **En el año 2019 el algoritmo de Open EU derrota al mejor equipo del mundo en el videojuego DOTA 2,** que es un videojuego que requiere de coordinación de diferentes jugadores y de mucha intuición, consiguiendo otro hito. 
- **En el 2020, otro algoritmo también de DEEP Mind, la compañía que fue desarrollada por Google, resolvió un problema que hasta entonces no había podido resolverse,** que era el predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo va a ser extremadamente prometedor para para hacer nuevos avances en el campo y en general.

A partir de estos hitos y esta evolución, hemos visto como progresivamente las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, sino que han transicionado a ser compañías basadas en los datos y, por lo tanto, en la Inteligencia Artificial. Las compañías más grandes del mundo son Google, Facebook, Apple, etcétera compañías puramente basadas en datos, software e Inteligencia Artificial. 

Se prevé que esta evolución exponencial del valor que la Inteligencia Artificial va a aportar al mercado y a los consumidores va a seguir creciendo y, por tanto, este campo es de extremada relevancia. Su estudio para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías.

##### Tipos de Redes Neuronales
El Deep Learning es un campo que también ha crecido mucho en los últimos años. Se han desarrollado muchas nuevas tecnologías y avanza a un ritmo vertiginoso porque, cada vez, se dedican más recursos y aparecen más novedades. 

Resulta hasta difícil mantenerse en la ola de las últimas novedades que van surgiendo. 

Pero los tipos principales de redes neuronales que podemos encontrar, o los que más establecidos y que ya han demostrado resultados muy significativos en los últimos años son: 

- **Redes neuronales totalmente conectadas:** Serían las redes neuronales originales las que se basan en neuronas individuales que se conectan a través de distintas capas. 
- **Redes neuronales recurrentes:** Se desarrollaron específicamente para la evaluación o para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros. 
- **Redes neuronales convencionales:** Específicamente diseñadas para trabajar con datos que estén ordenados en dos o tres dimensiones.
- **Redes neuronales generativas o adversarias:** Estas redes, principalmente convolución, tienen la característica de que al final sirven para generar datos de forma artificial que son muy realistas. 
- **Transformers:** Red neuronal que surgió en 2020. Este es un tipo de red neuronal que a través de la ingesta de grandes cantidades de datos y a través de desarrollos de modelos con muchos parámetros, son capaces de hacer tareas como generación de texto y de imágenes con un realismo que no tiene precedentes. Por tanto, este tipo de redes son capaces de trabajar tanto con entradas de texto, con entradas de imagen o con cualquier tipo de entrada y es capaz de combinar. Es capaz de combinar información de ambos mundos, del mundo del lenguaje natural, es decir, el texto y el mundo de la imagen. Pero es muy bueno trabajando con datos no estructurados.

### Redes Neuronales, cómo se interconectan
**Una neurona es un núcleo de cálculo muy simple.**

Simplemente es una función que tiene como entrada un número o varios números y le aplica una función de activación específica que puede ser determinada por el programador para obtener un número diferente al de entrada.

![[496.E1_Redes_Neuronales_1.png]]

Esta neurona tiene una o diversas entradas y dichas entradas tienen asociado un peso. 

Por lo tanto, si una neurona tiene diversas entradas, por ejemplo, un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa:
1. Número de habitaciones
2. Metros cuadrados
3. Número de baños
4. Etc…

- Esta información entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada.
- Cuando combina todos estos números al final con estos pesos, les aplica una función de activación que convierte en este número en otro, o igual puede ser el mismo, depende de la función de activación que se utilice y el número de entrada. 
- Esto al final, lo que introduce en el sistema son no linealidades, que lo que permiten es adaptarse a entornos de datos muy complejos.

**Sabiendo ya lo que es una neurona, lo que podemos ver es cómo se crea una red neuronal.**

Una red neuronal es un conjunto de neuronas ordenadas por capas. Es decir, en la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etc. Así hasta la capa de salida, que es la que nos va a dar la respuesta a nuestro problema, que en este ejemplo que he puesto sería el precio de la casa que queremos predecir. 

Por lo tanto, lo que vemos es una estructura por capas y al final lo que vamos a conseguir con esto es que cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto es lo que nos permite es transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para conseguir u obtener, desgranar unas relaciones de muy alto nivel, muy complejas y que quizá no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa o la teoría que estemos evaluando en cuestión.

#### Redes Neuronales Recurrentes vs Redes Neuronales Convolucionales
##### Redes Neuronales Recurrentes
**Están generadas o diseñadas para entender o procesar datos que tengan una relación temporal. Por ejemplo, en una frase, el orden de las palabras, puede cambiar el significado de una frase.**

![[496.E1_Redes_Neuronales_2.png]]

Las Redes Neuronales Recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. 

Por lo tanto, lo que hacen es procesar secuencialmente las palabras de entrada, pero teniendo como información también a la hora de analizar cada una de las palabras cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma.

> Un buen ejemplo de caso de uso de la Red Neuronal Recurrente puede ser el reconocimiento de voz, que no es más que la conversión de audio a texto y posterior interpretación de este texto, así como el texto escrito.

El audio también es una secuencia temporal y, por lo tanto, las Redes Neuronales Recurrentes pueden utilizarse para transcribir o para convertir este audio en una cadena de texto que sea posteriormente interpretada por otra recurrente para emitir acciones específicas en los asistentes de voz. 

#### Redes Neuronales Convolucionales
**Están específicamente diseñadas para analizar datos que están organizados en dos o tres dimensiones, como es el caso de las imágenes.**

![[496.E1_Redes_Neuronales_3.png]]

Una imagen es una matriz bidimensional de píxeles ordenados en filas y columnas.

- Las Redes Neuronales Convolucionales nos permiten utilizar una operación que se llama Convolución, la cual nos permite aplicar diferentes filtros a las imágenes para procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etcétera, a características de alto nivel.
- Por ejemplo, estamos detectando un sistema de, entrenando un sistema de detección de caras, las características de más alto nivel, por ejemplo, serían los ojos, la nariz, la boca, etcétera. 
- La retina aprende, convolucional concretamente, aprende a ver cómo se combinan todas estas características de bajo nivel, es decir, bordes, contornos, etcétera, para convertirlas en características de alto nivel, boca a ojos, etcétera, para seguir procesándolas y subiendo el nivel hasta obtener representaciones de lo que sería una cara. 
- Por lo tanto, al final las resonancias convolucionales están trabajando en comprender la información que tienen estas matrices bidimensionales, que son las imágenes.

##### Definición de Convolución
**La Convolución es una aplicación matemática que lo que nos permite es, con un filtro que no es más que una matriz de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante.**

Es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente procesada con, digamos, una información específica resaltada.

![[496.E1_Redes_Neuronales_4.png]]

####  Redes Neuronales Generativas Adversarias (GAN)
**Es un tipo de red neuronal que nos permite generar datos artificiales.** 

Para resolver esta tarea de forma óptima, lo que entrenamos son dos redes neuronales.
- Una red neuronal es la encargada de generar estos datos que son artificiales. 
- Otra red neuronal que es la encargada de inferir o de discriminar si estos datos son reales o son falsos. 

Por lo tanto, cuando hacemos diversas iteraciones de entrenamiento, la red neuronal de generación de imágenes es muy buena creando imágenes que se parecen a la realidad porque intenta engañar a la red discriminadora, que es la red que intenta detectar si estas imágenes son reales o son ficticias. 

Así, entramos en un círculo virtuoso en la que cada vez la imagen generadora genera mejores imágenes o más parecidas a la realidad y la red discriminadora cada vez es más buena detectando estas imágenes que son falsas. 

Por lo tanto, entrar en un juego de intentar superarse la una o la otra, consiguiendo que la red generadora finalmente, que es nuestro objetivo, genere imágenes muy, muy, muy similares a las imágenes reales.

##### Aplicaciones de las GAN
- Las GANs, se pueden utilizar para toda una diversidad de tareas que tengan como objetivo generar información de una distribución de datos específica que antes no existía, como por ejemplo generación de caras. 
-   De hecho, hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe y es extremadamente realista. 
-   Por lo tanto, las GAN, lo que nos permiten es generar datos de cualquier distribución con la que la hayamos entrenado y, por tanto, replicar toda esta información. Incluso también nos puede servir, hay muchísimas aplicaciones de las GAN, pero nos puede servir también para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. 
-   Por ejemplo, dada una imagen en blanco y negro, conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permite hacer cualquier tipo de técnica que tenga como salida un dato que debería ser realista. 
-   La generación de datos que son realmente plausibles, como por ejemplo, ya os digo, la coloración de una imagen, la generación de una cara artificial, la generación, por ejemplo, de imágenes que sean realistas para tener más datos con el que entrenar otras redes neuronales, etcétera, etcétera.

#### Transformers
Ahora tenemos los Transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que empezó como una revolución en la inteligencia artificial, concretamente en el procesador del lenguaje natural, porque permitía superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. 

![[496.E1_Redes_Neuronales_5.png]]

- Los Transformers lo que pueden hacer es obtener sin un coste computacional disparado la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, cosas en las que las redes neuronales recurrentes no eran tan óptimas. 
- Además, nos permite hacer esta clase de relaciones y de entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en término de parámetros y, finalmente, de desempeño final. 

Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate mejoró de forma drástica en muy poco tiempo.

Se dio este hecho porque se pasó de utilizar técnicas más convencionales para la traducción a utilizar técnicas basadas en redes neuronales. 

Esto lo que permitió es que los resultados de traducir un texto del español al inglés, por ejemplo, ahora sean extremadamente robustos y prácticamente apenas se cometen incoherencias textuales y cada vez, según se vayan incrementando los datos y los modelos vayan siendo más grandes, este error y esta, digamos, poco similaridad con lo que diría una persona, se van a ir reduciendo paulatinamente.

### Aplicaciones de las Redes Neuronales
**Las aplicaciones que podemos ver son muy variadas y extremadamente extensas. **

Una de las más conocidas es el reconocimiento facial. 

Es decir, para empezar, gracias a una Red Neuronal Convolucional podemos detectar las caras que hay una imagen. Es decir, dónde están esas caras dentro de la imagen, para, posteriormente, hacer una identificación de estas caras con respecto a una base de datos. 

El reconocimiento facial en China, por ejemplo, está a la orden del día y por tanto todos sus ciudadanos están altamente identificados de forma automática en estos sistemas.

> Las aplicaciones de inteligencia artificial, como cualquier otra herramienta, pueden ser positivas o negativas, dependiendo del análisis y de sus consecuencias. Pero es innegable que son herramientas extremadamente potentes.

Volviendo al caso de China, se descubrió a un fugitivo entre 60.000 personas en un evento gracias a este reconocimiento facial. Por tanto, esto nos hace darnos cuenta del potencial que tiene, de la precisión a los que estos algoritmos están llegando.

#### Uso en medicina
![[497.E1_Redes_Neuronales_-_Aplicaciones_1.png]]

También es amplísimo el uso de las Redes Neuronales Convolucionales para el análisis y proceso de imágenes médicas, ya que nos permiten hacer segmentaciones volumétricas de imágenes en 3D, como los TAC o las resonancias magnéticas para obtener volúmenes de regiones de interés: Pulmones, hígado, cerebro, etc. 

Además, también se pueden utilizar estas redes neuronales para extraer información y detectar diferentes patologías como nódulos y una gran variedad de enfermedades pulmonares o incluso de cualquier otra parte del cuerpo.

Y estas aplicaciones no están haciendo más que crecer y crecer. Cada día o cada año son más numerosas las diferentes empresas de startups que se dedican a este sector.

#### Coches autónomos
![[497.E1_Redes_Neuronales_-_Aplicaciones_2.png]]

Otra aplicación muy común de las Redes Neuronales Convolucionales son los coches autónomos. 

Un coche autónomo es un coche que tiene que replicar la conducción de una persona y para ello es primordial la evaluación de la información del entorno. 

Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras y por tanto las cámaras producen imágenes que son el target ideal para ser analizadas con redes neuronales convencionales que detectan desde objetos en la vía, como coches, peatones, ciclistas, etc. 

También las líneas del carril entre semáforos, entre otras muchas cosas, por lo tanto, son clave para este sector.

#### Usos nocivos o fraudulenotos
Las GAN también tienen otro tipo de aplicaciones, por ejemplo, la alteración de imágenes médicas. Esto sería una aplicación, digamos que no sería positiva, sino todo lo contrario, sería negativa.

- Las redes neuronales también se puede utilizar con fines, nocivos y hay que estar prevenidos de esta clase de situaciones. 
- En un estudio, lo que hicieron fue evaluar cómo, alterando imágenes médicas mediante estas GAN, los científicos que realizaron estudios eran capaces de engañar a los radiólogos expertos en observar las imágenes médicas.
- Por lo tanto, lo que hicieron fue incluir en imágenes de sujetos sanos, nódulos pulmonares y en imágenes de sujetos con cáncer.
- Quitar de forma artificial estos nódulos y los resultados que se vieron es que tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico. 
- De hecho, en más de un 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales.

#### Deep fakes
Los Deep Fakes en los que somos capaces de generar de forma artificial video y audio de una persona concreta para hacer creer al resto que esa persona ha dicho unas declaraciones concretas, un hecho, una acción específica, induciendo a la población sobre lo que esa persona ha hecho y por lo tanto, dañando su imagen. 

Hay decenas de vídeos en internet en las que en las que podemos ver esta clase de situaciones.

#### Generación de Texto con ChatGPT
Ejemplos más disruptivos de los últimos años de Transformers ha sido el GPT3, que no es más que un generador de texto tan preciso y tan realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido.

Permite entre otras cosas:
1. Generación de texto o contenido.
2. Comprensión de un contenido a través de un resumen del mismo.
3. Traducción de dicho contenido.
4. Diseño de aplicaciones que se basen en esta generación de texto.
5. Programación automática.
6. Generación automática de código para desarrolladores.

Tenemos es una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, puedes hacer que esta herramienta específica escriba sobre multitud de temas.

#### Generación de Imágenes
Otra aplicación de gran impacto de los Transformers es la generación de imágenes a partir de un texto descriptivo.

![[497.E1_Redes_Neuronales_-_Aplicaciones_3.png]]

- Podemos hacer es generar cualquier tipo de imagen, dada una descripción específica. 
- Permite acelerar o hacer más eficiente el proceso creativo, ya que dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto que nos puede inspirar. 
- Esto puede servir para, por ejemplo, inspirar a distintos grupos de diseño. 
- Incluso se ha creado NFT’s de arte utilizando las redes neuronales.

#### Conclusiones
Estamos todavía muy lejos de esta inteligencia artificial general que puede estar en el imaginario colectivo por las películas, que es capaz no de tomar decisiones autónomas, de hacer una multitud de tareas.
- Tenemos sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. 
- Aunque la inteligencia artificial se empezó a desarrollar en la década de los 50 60, su uso solo decayó debido a que no teníamos todavía ni los datos ni el poder de computación, ya no es así. 
- En esta era de desarrollo artificial ya se están desarrollando herramientas tan potentes que están proporcionando mucho de valor de mercado y, por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, al contrario, ha llegado para quedarse e impactará en nuestras vidas de forma muy pronunciada.
- Hay que tener en cuenta que solo estamos al principio. Este último boom de desarrollo de la Inteligencia Artificial. se ha empezado a producir, mayoritariamente a partir del 2012. Por tanto, solo llevamos una década viendo su potencial.
- El cambio o la adopción y la evolución de estas técnicas está creciendo de forma más exponencial y por lo tanto, cada vez veremos aplicaciones que nos sorprenderán más. 
- Incluso ya somos capaces de ver todas estas aplicaciones que son incluso capaces de generar información tan real que son capaces de engañar al ojo humano.

Es un campo apasionante que hay que seguir con detenimiento.

## U6. Práctica con Diego Bonilla
### Blockchain y Machine Learning ()
![[498.E1_Blockchain_y_Machine_Learning.mp4]]
[Blockchain y Machine Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948168-p6-1-diego-bonilla-blockchain-y-machine-learning)
[Blockchain y Machine Learning (PDF)](499.E1.Blockchain_y_machine_learning-230120-152730.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98)

Hola y bienvenidos a todos a una nueva sesión de Web3. En esta sesión veremos Machine Learning aplicado a Blockchain. Como ya sabéis, tanto Machine Learning como Blockchain son dos tecnologías que en este momento son bastante punteras. Tienen muy poco tiempo en algunos casos, y ambas son tecnologías muy potentes. Hoy explicaremos los avances que ha habido en la combinación de estas dos tecnologías tan prometedoras. También veremos casos prácticos y aplicaciones en las que se están utilizando, así como ramas que las están empezando a implementar, y también discutiremos el futuro que nos espera con el uso de ambas tecnologías.

Para ello, comenzaremos explicando un poco sobre Machine Learning y Blockchain. En ambas diapositivas lo veremos de manera general, sin entrar en profundidad, ya que muchos de vosotros ya tenéis un buen contexto sobre ambas tecnologías. Sin embargo, es importante poner en contexto el resto de la explicación para que estemos todos en la misma página y para facilitar la relación entre las dos en una nueva tecnología. Luego podremos ver un poco la teoría que hay detrás de unir ambas y los casos prácticos de aplicaciones.

Como hemos comentado, empezaremos con Machine Learning. El Machine Learning, como ya sabréis, es una tecnología que tiene la capacidad de aprender de datos nuevos de forma automática. Es decir, es un conjunto de algoritmos que pueden adaptarse a casi cualquier tipo de datos de forma intrínseca, sin necesidad de muchas modificaciones en el algoritmo para adaptarse a varias tareas. Esta potencia o capacidad es lo que hace que se aplique en una gran cantidad de campos y dominios con diferentes tipos de datos y tareas. No es necesario mencionar todas esas tareas, ya que son innumerables, tanto en audio, texto, datos estructurados, datos no estructurados, datos en el dominio continuo, dominio discreto, imágenes, vídeos, entre otros. Todo esto hace que el Machine Learning sea una de las tecnologías más en tendencia en la actualidad, gracias a estas increíbles capacidades.

Blockchain, por otro lado, es el corazón de muchas tecnologías, incluyendo todas las criptomonedas. Esta tecnología se está volviendo cada vez más popular, lo que ayuda a que el almacenamiento y el intercambio de información sean mucho más descentralizados, seguros y sin intermediarios. Por lo tanto, Machine Learning puede aplicarse a la tecnología Blockchain para hacerlo más eficiente y efectivo. En este gráfico, podemos observar cómo se está utilizando o en qué áreas se puede aplicar Blockchain basado en frameworks de Deep Learning o Machine Learning en general. Podemos ver los diferentes tipos de Blockchain, los distintos modelos y protocolos basados en Machine Learning o Deep Learning, así como diferentes áreas de aplicación, servicios, tipos de datos con los que trabajar y objetivos de despliegue.

Ahora también veremos rápidamente la tecnología Blockchain, que significa cómo se utiliza. Sé que muchos de vosotros lo entendéis mejor que yo, pero es importante para el resto. Básicamente, Blockchain se puede entender como un tipo de base de datos distribuida que almacena cualquier tipo de datos y, debido a la forma en que lo hace, es muy difícil de hackear, cambiar o engañar al sistema. Las bases de datos más convencionales almacenan la información en tablas, mientras que Blockchain lo hace en bloques encadenados entre sí, de ahí su nombre. Esta sería una arquitectura bastante común de Blockchain.

Podemos ver las partes que componen Blockchain, que son, para empezar, los bloques, que son las entidades de alto nivel que vemos aquí. Cada bloque contiene una cantidad arbitraria de datos. Cada bloque contiene tres tipos principales de elementos: los datos, el nonce y el hash. Estos son los datos que se encuentran dentro de un bloque de Blockchain. También encontramos mineros y nodos. Los mineros son utilizados para crear nuevos bloques mediante el sistema de minado. Los nodos pueden ser entendidos como dispositivos que contienen una copia del Blockchain. Por ejemplo, para que una transacción sea completa, debe haber diferentes nodos y cada nodo debe tener una copia del Blockchain. Aquí vemos lo que hemos explicado anteriormente sobre los nodos. Esto sería una transacción completa.

Ahora vamos a ver la combinación de ambas tecnologías, ya que hemos hecho un breve resumen de cada una. Sabiendo que Machine Learning, como hemos explicado antes, se puede entender como un sistema que aprende de datos pasados e intenta mejorar a partir de esos datos para predecir el futuro, se considera una tecnología autoadaptativa. Además, todas las reglas que necesita para autoadaptarse no tienen que ser escritas a mano, lo que significa que las aprende por sí sola. La combinación de estas dos tecnologías, Blockchain y Machine Learning, podría cambiar significativamente el juego en áreas como las finanzas o las empresas de seguros. Un ejemplo común sería la identificación de transacciones fraudulentas.

En esta tabla podemos ver cuatro ejemplos o beneficios que se pueden obtener al unir estas dos tecnologías. El primero sería aumentar la seguridad de los datos utilizando Blockchain, que, como ya hemos explicado, incrementa la seguridad en comparación con las bases de datos convencionales. Esto se hace de manera automática gracias a la potencia de Deep Learning y Machine Learning. Por lo tanto, se convierte en un proceso automático que aprende de sí mismo. También se utiliza esta capacidad de predecir el futuro, lo que mejora la capacidad de decisión y, por ende, la robustez del sistema.

Otras capacidades o mejoras que se pueden entender de esta combinación de tecnologías incluyen, por ejemplo, mejorar la seguridad. Como hemos mencionado, los datos en Blockchain son mucho más seguros debido a su sistema encriptado, lo que lo convierte en un sistema ideal para almacenar datos personales sensibles, como recomendaciones personalizadas. Sin embargo, a pesar de que Blockchain es muy seguro, muchas aplicaciones que utilizan esta tecnología o capas que se añaden pueden hacerlo vulnerable. Por lo tanto, Machine Learning puede aplicarse para predecir posibles brechas de seguridad o amenazas en las aplicaciones que utilicen esta tecnología.

Además, se puede optimizar el mercado de datos. Muchas grandes compañías, como Google, Meta o LinkedIn, tienen enormes data pools que contienen información de millones de usuarios. Esto también implica utilizar los datos almacenados y permitir que una inteligencia artificial los utilice mediante procesos de inteligencia artificial, lo que sería extremadamente útil. También sería beneficioso que estos datos fueran accesibles para otros, no solo para estas grandes empresas. Utilizando la tecnología Blockchain, muchas startups y compañías más pequeñas podrían acceder a esos data pools de grandes empresas, utilizando los mismos procesos de inteligencia artificial que estas empresas, sin que haya brechas en la seguridad.

Por otro lado, otro beneficio podría ser la optimización del consumo de energía. Como hemos comentado anteriormente, el minado, que es una parte principal de Blockchain, consume mucha energía, lo cual es un gran problema. De hecho, es uno de los mayores desafíos para diferentes industrias. Sin embargo, gracias a algoritmos de inteligencia artificial, Machine Learning y Deep Learning, Google logró reducir en un 40% el consumo de energía del minado utilizando una red neuronal. En la compañía Google, conocida como DeepMind AI, optimizaron el consumo de energía utilizado por los robots en los data centers, especialmente en la parte de refrigeración, y lograron optimizar el consumo de energía en un 40%.

Por último, implementar un pago de seguro en tiempo real también es un beneficio significativo. Utilizando Blockchain y Machine Learning, podemos crear un proceso de pago mucho más seguro e inteligente, que puede ser completamente implementado en el entorno de Blockchain. Por ejemplo, utilizando el encriptado inteligente de Machine Learning, que es mucho más adaptativo que cualquier otro encriptado utilizado hasta ahora. En la imagen de abajo, se describe un modelo de sistema que toma una entrada, que en este caso serían datos personales de un hospital, considerados muy confidenciales. Estos datos se guardarían en Hyperledger Fabric para utilizar el protocolo IPFS, que es el InterPlanetary File System, mediante un algoritmo. Una vez que los datos están almacenados de forma segura, se pueden desencriptar o recuperar utilizando un agente de Deep Learning, en este caso, una LSTM, que es una red neuronal optimizada para datos temporales o con dependencia temporal. La sigla LSTM significa Long Short Term Memory, y está basada en redes neuronales. Al final, una vez que se recuperan esos datos utilizando esta red neuronal, se pueden enviar a un dispositivo como una alerta de diferentes actividades, como una consulta del doctor, la necesidad de tomar cierta medicación, un diagnóstico hacia un paciente, o cualquier otra cosa.

Vamos a ver ahora aplicaciones más concretas. Hemos visto los beneficios que puede tener la combinación de ambas tecnologías, y ahora exploraremos aplicaciones reales que se llevan a cabo gracias a la combinación de estas tecnologías y los beneficios mencionados. La primera aplicación que podemos considerar como muy efectiva sería mejorar el servicio al cliente. Como ya sabéis, la satisfacción del cliente es un gran desafío para todas las organizaciones, pero actualmente se están utilizando muchas tecnologías de Machine Learning para mejorar el servicio al cliente. Esto incluye desde sistemas de recomendación hasta una experiencia mejorada del cliente en la tienda. Además, se puede combinar con una aplicación basada en Blockchain para llevarlo a otro nivel.

Desde un punto de vista de seguridad, como ya hemos visto antes, ambas tecnologías pueden incrementar la seguridad y llevarla a otro nivel. La seguridad es un aspecto muy importante para las personas, y cada vez más se tiene en cuenta, especialmente en un contexto donde el cibercrimen está en aumento. Machine Learning y Blockchain pueden ser utilizados para la vigilancia de estas aplicaciones, donde Blockchain puede controlar estos datos continuos y Machine Learning puede analizar los datos que se están controlando de forma encriptada y anónima. Como ya hemos mencionado, Machine Learning necesita supervisión humana, lo que incrementa la seguridad al no haber intervención humana.

Las Smart Cities están en desarrollo constante. Cada día se busca ayudar a las personas a mejorar sus estándares de vida. La idea de crear una ciudad inteligente implica involucrar Machine Learning y tecnologías basadas en Blockchain para desempeñar un papel crucial. Por ejemplo, una casa inteligente utilizaría estas tecnologías para facilitar la monitorización de cada usuario y ofrecer una experiencia personalizada en cada dispositivo.

El trading, por supuesto, es otro ámbito relevante, ya que Blockchain es la tecnología detrás de la mayoría de las criptomonedas, especialmente las más grandes, como Bitcoin o Ethereum. Estas criptomonedas son cada vez más populares entre los inversores minoristas y las grandes instituciones financieras. Hoy en día, ya se utilizan bots de trading basados en algoritmos de Machine Learning muy potentes, lo que permite implementar estrategias de Machine Learning más avanzadas. Una de ellas se llama Reinforcement Learning o aprendizaje por refuerzo, que es un método popular en el mundo de la tecnología. Este método ha sido utilizado en sistemas que aprenden a jugar videojuegos o juegos de mesa, y recientemente también se ha aplicado en el plegado de proteínas para identificar cuáles son las que forman parte del cuerpo humano. Este tipo de aprendizaje no necesita una función definida para optimizar; simplemente necesita saber qué acciones generan refuerzos o castigos. Así, probando y jugando, aprende a ser un experto en cualquier juego o sistema. Esta idea de castigar o recompensar al algoritmo para que aprenda se puede aplicar fácilmente en el trading.

La optimización de estrategias de minado es otro aspecto importante. Como hemos mencionado antes, el minado de criptomonedas consume muchos recursos. Algunas redes neuronales o sistemas de optimización han logrado reducir el uso de esos recursos. El minado, como ya sabéis, implica adivinar valores para resolver funciones matemáticas en un Blockchain, utilizando muchos recursos computacionales. Al final, el minero es quien resuelve la función y puede actualizar el Blockchain con una transacción válida. Investigadores han demostrado que utilizando Reinforcement Learning se puede optimizar el minado de criptomonedas, como Bitcoin, mediante técnicas como Q-learning, aunque hay muchas otras investigaciones científicas que también abordan este tema. Esta es una buena área de estudio que está ganando popularidad.

Por último, el cryptojacking, que son ataques que utilizan recursos para minar criptomonedas, se está volviendo cada vez más común y requiere de medidas de seguridad más avanzadas. Algunos investigadores han encontrado un nuevo método para detectar la presencia de estos programas maliciosos, conocido como SCK-GCN, que también está basado en redes neuronales. Este sistema identifica similitudes entre dos pares de código, lo que le permite detectar, a partir de las diferencias, la presencia de cryptojackers.

Aquí tenemos un ejemplo visual de un algoritmo de predicción del precio de una criptomoneda, en este caso, Bitcoin. Se muestra cómo sería el flujo de predicción utilizando los datos disponibles para predecir la tendencia y el precio de Bitcoin mediante diversas técnicas de Machine Learning. Primero, se combinan las características del modelo con los datos del precio de Bitcoin, lo que permite entrenar un sistema de Machine Learning para identificar las diferentes tendencias que pueden observarse en Bitcoin o intentar predecir hacia dónde se dirigirá el precio en las siguientes etapas temporales. Todos estos son métodos de Machine Learning más clásicos, y el que se muestra aquí está basado en redes neuronales, específicamente en Long Short Term Memory (LSTM). Esto permite predecir al menos las tendencias que seguirá el precio.

Como ya sabéis, el trading es un sistema pseudocaótico y, en la actualidad, no existe ninguna técnica que pueda predecirlo de forma exacta, ya que depende de muchos factores. Sin embargo, las redes neuronales son cada vez más potentes y se adaptan mejor al comportamiento humano. Es probable que en el futuro se logre entender la información proveniente de diversas fuentes, lo que permitirá predecir de manera más precisa el precio de las criptomonedas o el trading en general.

Esto sería todo. Como habéis visto, hemos explorado de manera general las tecnologías de Machine Learning y Blockchain, cómo se pueden utilizar, los beneficios que ofrecen y, al final, las aplicaciones actuales y futuras de la combinación de ambas tecnologías.


### Reconocimiento Óptico de Caracteres (Video)
![[500.E1_Reconocimiento_Óptico_de_Caracteres.mp4]]
[Reconocimiento Óptico de Caracteres](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres)
[Reconocimiento optico de caracteres (PDF)](501.E1.Reconocimiento_optico_de_caracteres-230120-152820.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/14kHTyVrfLQa2g2znTjS2Yiqkqy54O0Ci?usp=sharing)

Hola y bienvenidos a la práctica de reconocimiento óptico de caracteres. La idea que tenía para esta práctica era ver un poco lo que significa el reconocimiento óptico de caracteres o OCR, así como poner en contexto qué significa esta tecnología, profundizando en en qué tipo de industria se utiliza, cómo funciona internamente y, finalmente, saltar directamente a un caso práctico de una librería muy utilizada. Actualmente, si no me equivoco, es la que más se usa, que es Tesseract. También veremos qué funcionalidades tiene y qué tipo de herramientas dispone para el usuario. Luego, abordaremos casos prácticos.

En los casos prácticos, actualmente me encuentro en un proyecto con un banco español en mi empresa, Cognizant, donde estamos llevando a cabo un caso en el que se utiliza el OCR como la principal columna sobre la que se asienta el proyecto. Tanto mis compañeros como yo lo estamos usando en el día a día, y puedo ofrecerles de primera mano un caso práctico que se está utilizando hoy en día. Por último, veremos una práctica en la que nos iremos a Google Colab. He creado unas cuantas celdas en Python y veremos cómo se ejecutan y las diferencias que tiene el OCR con diferentes imágenes, así como su funcionamiento.

En este caso, como ya hemos comentado, el OCR de la lección ha sido Tesseract, debido a su amplia utilización hoy en día y a lo fácil que es de usar. Bueno, para empezar y poner también en contexto, ¿qué es el reconocimiento óptico de caracteres? Es una tecnología que convierte texto que procede de documentos digitales que no tienen texto extraíble per se, como podría ser un Word o un PDF, y lo extrae. En este caso, el Deep Learning es la metodología más utilizada. No siempre ha sido así, pero actualmente se utilizan sistemas inteligentes para detectar el texto y extraerlo de una imagen. Esto puede incluir documentos escaneados con un escáner o PDFs que no tengan texto extraíble.

Los usos principales del reconocimiento óptico de caracteres son crear flujos de trabajo automatizados digitalizando documentos de diferentes unidades comerciales. Por ejemplo, podemos ver aquí un caso donde se utiliza OCR para identificar la matrícula de un coche. Sin embargo, no solo se limita a la matrícula, sino que también es capaz de detectar entidades, como el estado en el que se encuentra el vehículo, la matrícula y otra información adicional, como la marca. Así que, en el OCR, no solo se trata de saber dónde está el texto y ser capaz de reconocerlo, sino que también hay un fuerte reconocimiento de entidades y reconstrucción de documentos, es decir, saber qué papel juega ese texto en el documento.

Por lo tanto, todo esto se puede integrar en un flujo de inteligencia artificial o en un flujo de información que proporciona una gran cantidad de datos. También es importante tener en cuenta que todo este reconocimiento de entidades se realiza de forma anónima; no es necesario que haya alguien detrás etiquetando estos documentos. Por lo tanto, no solo se elimina una gran cantidad de trabajo y se automatiza, sino que también se vuelve más seguro, ya que se elimina el factor humano en este caso. Así, todo lo que implica el escaneo automático de DNIs, pasaportes o documentos de extractos bancarios se convierte en algo completamente anónimo, ya que no hay nadie detrás en este proceso.

Además, el reconocimiento óptico de caracteres también puede complementarse con otros sistemas, como el reconocimiento de entidades, como estamos viendo en las imágenes que reconocen tanto el texto como qué parte forma, si es el título, el precio o una tabla. La reconstrucción se refiere a la capacidad de restablecer el orden lógico de los documentos o las partes de un documento, y por último, la clasificación, que podría ser, por ejemplo, identificar si un documento es una matrícula, un DNI o un pasaporte, abarcando todo lo que implica esa parte.

Más específicamente, un motor que se encarga de realizar este tipo de trabajo es Tesseract, que actualmente es el más utilizado, ya que es muy fácil de usar, además de ser de código abierto y gratuito. Tesseract es de HP; fue creado en HP, pero luego fue patrocinado por Google, como hemos mencionado, y es open source. Actualmente, soporta 116 idiomas y 37 alfabetos diferentes. No solo detecta el texto, sino que también es capaz de devolver la jerarquía del texto. Esto significa que puede indicar, por ejemplo, si un texto depende de otro anterior, si está dentro de una tabla, si uno es el título y el resto es contenido que lo sigue. Esto es algo que puede detectar.

También es capaz, dado que detecta varios idiomas, de leer de izquierda a derecha, así como de derecha a izquierda y en diferentes formatos de texto. Por ejemplo, puede manejar texto en columnas, texto que forma un círculo o tablas, y muchas otras configuraciones que se le pueden dar a Tesseract para que detecte tipos especiales. Sin embargo, Tesseract tiene un problema: está entrenado, digamos que es muy eficaz en documentos, como podemos ver en la imagen, pero no es capaz de detectar texto "salvaje", que es el término que se utiliza. Esto se refiere a texto que, por ejemplo, se obtiene al fotografiar un coche y que Tesseract identifique el texto de la matrícula; le costará mucho trabajo. Necesita un documento que esté recortado, rotado y que se asemeje mucho a un PDF para detectar el texto de manera fiable.

Por lo tanto, incluso los propios desarrolladores de Tesseract recomiendan aplicar un preprocesamiento a la imagen antes de pasarla por el motor Tesseract. Este preprocesamiento puede incluir la mejora del contraste para que la diferencia entre el fondo del documento y las letras sea muy marcada y se diferencie claramente. También puede incluir una corrección de perspectiva. Si, por ejemplo, hemos tomado una foto del documento, se puede ajustar para que las cuatro esquinas del documento se agranden y se elimine esa perspectiva, de modo que parezca que lo hemos escaneado. Algunas aplicaciones móviles realizan esto automáticamente, pero si no se lleva a cabo este preprocesamiento, Tesseract no será capaz de detectar casi nada de texto o le costará mucho.

En el mismo sentido, también se presenta el problema de la rotación. Si le proporcionamos un documento que está rotado, tampoco será capaz de detectarlo, ya que asume que el texto está completamente recto una vez que le damos la imagen. Esto es un poco lo que hemos visto, y también se explica otro tipo de preprocesamiento que se realiza, como la reducción de ruido, que es una técnica bastante sencilla de aplicar. Básicamente, consiste en reducir el ruido de la imagen. Se pueden aplicar filtros que disminuyen el ruido de la imagen sin alterar demasiado o intentando no eliminar muchas partes del texto.

En la imagen que vemos a continuación, se presenta un flujo de procesamiento de una imagen con un OCR. La imagen sería la entrada, y primero se realizan algunos preprocesamientos a la imagen que hemos comentado, como aplicar un filtro, eliminar la rotación, convertir a blanco y negro o mejorar el contraste, y eliminar el ruido. Luego, internamente, en este caso de Tesseract, se lleva a cabo toda esta parte automáticamente. Simplemente tenemos que invocar a Tesseract a partir de aquí. Internamente, Tesseract lee las líneas, detecta cuántas líneas hay, y a partir de las líneas, las palabras, utilizando los espacios, y luego los caracteres que componen cada palabra. En este punto, se detecta cada carácter, se realiza un posprocesamiento utilizando conocimiento interno de gramática y se devuelve el texto.

Luego, el texto también puede requerir una etapa de posprocesamiento, ya que evidentemente no se puede adaptar a todos los casos que existen, y puede que necesites ajustarlo para cada caso y cada tarea que quieras realizar con la salida, adaptándola a lo que estés tratando o a tu caso específico. Estos son algunos parámetros que le podemos dar a Tesseract, como lo que hemos comentado, que puede detectar el texto en muchas formas, tanto de izquierda a derecha como en columnas, en partes de un círculo o en tablas. Esto se refiere al modo de segmentación de página, o PSM, que es un parámetro que le podemos proporcionar. Podemos indicarle que sea una sola columna, que sea un bloque uniforme de texto alineado verticalmente, que simplemente sea una palabra, que sea una línea de texto que forma un círculo o que sea simplemente un carácter. Los modos 11 y 12 son un poco más generales, ya que consideran todo el texto posible sin que forme parte de ninguna jerarquía o segmentación de página. Todas estas son más opciones que nos ofrece Tesseract para poder adaptarnos a nuestro tipo de problema.

Como he mencionado anteriormente, en mi empresa actual, Cognizant, estamos llevando a cabo un proyecto muy grande de inteligencia artificial pura, de lectura, escaneo y reconocimiento de documentos para una empresa de banca española. Como hemos comentado, este es el proyecto más grande de Inteligencia Artificial en el sur de Europa, y utilizamos las salidas del OCR para extraer información de cada documento. Las imágenes que aparecen son, evidentemente, imágenes obtenidas de internet, no son del proyecto actual. También podemos unir diferentes documentos que provienen de distintas partes y unirlos a un mismo usuario o a una misma empresa. Luego, podemos dirigir cada documento a donde corresponde, porque sabemos lo que contiene y cuáles son los pasos siguientes para ese documento. Además, esto es algo que se puede escalar, no solo a la clase que estamos viendo ahora, sino que también somos capaces de escalarlo a otro tipo de tareas, todo esto partiendo de un sistema que detecte los caracteres, es decir, que detecte el texto de los documentos y la jerarquía también de los documentos de manera eficaz.

Efectivamente, esto es algo que interesa mucho actualmente, específicamente, pero no se limita solo a la banca, porque como hemos mencionado, es algo que se puede automatizar muy fácilmente, eliminando una gran cantidad de gastos humanos en recursos. Además, al eliminar esa parte humana, también se reduce considerablemente la latencia y se aumenta la seguridad. En este caso, ya hemos visto que todo el reconocimiento de caracteres y demás son algoritmos, por lo que es completamente anónimo y no se retiene ninguna información que, tal vez, un humano sí podría tener.

En el resto de la sesión, vamos a ver el Colab. Aquí tenéis el enlace que colocaré cuando estemos todos. En esta práctica, lo que he preparado es, digamos, un par de formas de leer el texto de un documento, tanto si el documento es un PDF como si es una imagen. Veremos que si es un PDF, a lo mejor tiene texto extraíble. Entonces, accederemos a la metainformación del PDF con una librería llamada PDFminer. Así podremos leer el PDF y extraer el texto. Lo bueno de esta librería es que, si hay texto extraíble, también puede ser capaz de extraer las imágenes, por ejemplo. Esto nos proporcionaría mucha información. Además, junto al texto, te devolvería su posición, así como el texto y otra información relevante. Es muy útil, ya que muchas veces no necesitamos un OCR si los documentos que nos proporcionan son en PDF. Sin embargo, también hay que tener en cuenta que no todos los PDFs tienen texto extraíble.

Para el resto de los casos, tanto imágenes como PDFs sin texto extraíble, instalaremos Tesseract en Colab y veremos rápidamente algunos ejemplos de casos en los que funciona muy bien, así como casos en los que no funciona tan bien, cómo corregir esos casos y, tal vez, aplicar alguna técnica de mejora, y ver cómo afecta eso a la salida del modelo. Nos encontramos aquí en Colab; ya he preparado todo por partes, está separado por los diferentes ejercicios que vamos a realizar. Así que, nada, todas las celdas se ejecutarán de arriba hacia abajo. Esto ya lo he ejecutado yo, pero vosotros tendréis que instalar las dependencias; seguramente os aparecerá un mensaje que tendréis que leer y hacer un reinicio de la rutina de Colab, así que le dais y ya estaría.

Aquí es donde vamos a importar todas las librerías. Si no estáis familiarizados con Python, no pasa nada; simplemente, todas las funciones que vayamos a usar deben estar dentro del programa, y si no, no sabréis dónde buscarlas. Todas las imágenes y los archivos y PDFs que usemos en esta demostración no estarán en Colab, ni tendréis que acceder a ningún sistema de archivos. Simplemente, utilizando la función wget, podemos acceder a cualquier documento que esté en la web y descargarlo para incorporarlo automáticamente al sistema de archivos que viene con él. En mi caso, ya he ejecutado algunas de estas celdas. Este, por ejemplo, es el PDF, si no recuerdo mal, el de "Atención". Así que nos lo descargamos y parece que todo está correcto.

Aquí, como es un PDF, podéis ver que puedo acceder al PDF y seleccionar el texto. Entonces, ¿por qué no intentar acceder a la metainformación que tiene el propio PDF y extraer la información que contiene? Porque a lo mejor no tengo que recurrir a un OCR si puedo acceder directamente a esa información. Aquí, comentándolo línea por línea, cargaremos el archivo; en este caso, utilizaremos la función open de Python y le daremos el flag de read bytes para que lo lea como un buffer de bytes. Utilizaremos la librería PyPDF2. Entonces, le daremos el objeto que ha leído los bytes a la función FileReader. En este caso, ya nos devolverá un objeto de la librería. En ese objeto, podemos acceder a sus atributos, como por ejemplo el número de páginas, pero también podemos acceder a la información que tenga ese documento. En este caso, lo guardaremos en una variable llamada "info", y luego lo mostraremos por pantalla. De esta forma, el número de páginas será el número de páginas, el título será el título, el autor será el autor, ¿vale? Así iremos viendo qué información podemos extraer directamente del PDF y lo comprobaremos con el propio PDF para verificar si es correcto. Así que vamos a ejecutarlo.

Ha sido muy rápido, evidentemente. Vemos que el número de páginas son 11 y podemos ver aquí que efectivamente tiene 11 páginas el documento. El título, o "title", es "Attention is all you need", efectivamente así es el título. El autor, pues varios autores que podemos ver que están aquí todos referenciados. El "subject", esto ya no lo pone en el PDF en sí, sino que es parte de la información que tiene el documento. El creador vemos que no está definido y el productor tampoco. Creo que automáticamente se asigna a la librería que hemos utilizado. Pero bueno, el resto de la información es muy útil, tanto el autor como el título del documento, como el número de páginas. Personalmente, la información que utilizo en el proyecto en el que estamos ahora, sobre todo, es el número de páginas para poder iterar sobre ellas.

Antes de comentar que hay librerías como PyPDF2 que extraen la metainformación del texto, también hay otras que convierten los PDFs a imágenes para luego poder procesar los textos. Por otra parte, si no hay suficiente texto extraíble en los documentos o no hay texto directamente, aquí voy a comprobar antes que se esté grabando todo. Un momentito, vale, disculpad. Aquí es donde vamos a utilizar justo la variable de número de páginas, que como hemos visto nos devuelve un número entero, en este caso 11, y vamos a hacer un bucle for que significa que, para cada página, vamos a iterar sobre ellas y leeremos el contenido. Disculpad el corte, he visto un pequeño fallo que había en el código y lo he corregido. Fallos del director. Entonces, una vez creado el objeto de la página utilizando el objeto que nos ha devuelto antes la función y la función getPage, le damos como argumento el número de la página y nos devolverá un objeto que contiene información de esa página. En este caso, nos interesa el texto, así que llamaremos sobre ese objeto a la función extractText. En mi caso, he decidido imprimir un salto de línea y unas cuantas barras para separar las páginas, que nos imprima también el número de la página y luego el contenido de la página. Después, cerraremos el PDF cuando hayamos terminado de leerlo. Vamos a ejecutarlo.

Vale, pues podéis ver que se ejecuta efectivamente todo. Entonces, vamos a ir al principio. Vale, pues tampoco vamos a leerlo aquí, pero bueno, página 0, vale, empieza a contar desde 0, así que acabará en la 10. Pues eso, "Attention is all you need", los diferentes autores, el abstract, vale, todo esto se corresponde a esto de aquí y digamos que ya tenemos un texto que es modificable por un ordenador. A partir de aquí, puedo buscar, por ejemplo, palabras clave, como haremos a continuación, y obtener información diferente que antes, evidentemente, no podía. Vamos a limpiar este output.

Ahora vamos a ir con imágenes. En este caso, evidentemente, una imagen no contiene texto extraíble ni legible por una máquina a priori. Entonces, vamos a decodificar ese texto, vamos a leerlo utilizando Tesseract para poder acceder a modificar ese texto o ver qué tal lo ha hecho. Esto es una pequeña función que utiliza la librería Matplotlib para mostrar imágenes por pantalla. No voy a profundizar en qué significa esto, pero básicamente, como estamos utilizando OpenCV, que es una librería de visión por computadora para leer las imágenes como matrices de números, OpenCV las lee en formato BGR (blue, green, red), pero luego, para mostrarlas por imagen, las presenta en RGB. Por lo tanto, hay que cambiar los colores, y esto es lo que significa. No mostramos el eje porque, en este caso, estamos mostrando imágenes y lo mostramos en pantalla.

Una vez ejecutada esta función, para que la meta en memoria, vamos a descargarnos la primera imagen, que es esta de aquí. Es una imagen bastante clara, se lee todo bastante bien, no es un escaneo ni nada, es una imagen completamente digital. No tiene muy buena resolución, pero es más que suficiente, así que esto es un caso perfecto. Aquí cargaremos las imágenes, utilizaremos la función de OpenCV para leer la imagen, la mostraremos por pantalla y luego le daremos esta configuración a Tesseract. La configuración, si lo recordáis de lo explicado en el PDF, es el Page Segmentation Mode 11, que significa que el PDF extrae el texto que pueda sin preocuparse de cómo esté distribuido por la imagen, por así decirlo, y luego le especificaremos que el idioma es el inglés. Aquí ya podéis hacer modificaciones para ver qué pasaría si le quito el idioma inglés, si funciona peor o mejor. No siempre funciona mejor, ni siempre funciona peor; depende de cada caso.

Lo que sí es obvio es que, si queréis detectar caracteres especiales de un idioma, siempre es bueno indicárselo, porque si no, será muy difícil o directamente no los detectará. Directamente, llevamos a la función, que es una función muy sencilla de Tesseract, donde le damos la imagen y utilizamos imageToString, que, como indica la función, convertirá una imagen a un string de palabras o texto. Ejecutamos, nos muestra la imagen y aquí nos muestra el texto que ha extraído. Podéis comprobarlo vosotros, pero está perfectamente extraído; no se ha equivocado en ninguna parte. Efectivamente, es un texto muy fácil y nos ha extraído sin mucha dificultad. Sin embargo, no solo devuelve el texto; hay otra función llamada imageToData, a la que le volvemos a dar la imagen y nuestra configuración como entrada, y vamos a ver qué devuelve este tipo de función.

Aquí vemos que devuelve un dataframe en el que nos proporciona información de nivel, número de página, número de bloque, número de línea, número de párrafo, número de palabra, left, top, width, height, confidence y text. Esto es un dataframe o un diccionario que nos devolverá mucha información por cada palabra. ¿Vale? Veis que la palabra "this", que es la primera de todas, nos indica que está un poco a la izquierda, de derecha a izquierda, y la confianza es del 96%, lo que significa que está muy seguro de que esa es la imagen. Vamos a abrir todo lo que hay aquí, porque esto ya son detalles internos de párrafos o caracteres especiales. La siguiente información nos dará la posición que tiene ese texto en la imagen, los píxeles en los que se encuentra ese texto. Todo esto nos indicará que fue parte de un nivel número 5, el número de página número 1, el bloque 1, el párrafo 1, y nos proporcionará información sobre la jerarquía que contiene más información sobre el texto, indicando que no solo ha detectado "this", sino que también te dice exactamente dónde está localizado y qué jerarquía forma con respecto al resto del texto. Vamos a limpiar un poco este output.

Ahora vamos a probar con una imagen de peor calidad. Vamos a ver qué imagen es. Ya nos lo he impreso aquí por pantalla. Es una imagen que podemos ver que también es texto, pero la calidad es mucho menor. Lo primero es que está un poco rotada, además no hay mucho contraste; no era como la imagen anterior, donde el negro y el blanco se diferenciaban mucho. Aquí hay unos cuantos grises en medio y, además, la calidad también es muy baja. Vamos a ver si es capaz de detectar. Bueno, aquí no lo he mencionado, pero volvemos a llamar a la misma configuración y a la misma función que antes, nada nuevo, e imprimimos el texto. Vemos que sí es capaz de detectar las primeras líneas, pero luego la palabra "dog" se ha perdido. Hacer un análisis de esto es un poco complicado, porque evidentemente todo esto es de alto nivel, pero por alguna razón no ha detectado la palabra "dog". En este caso, si era una abstracción casi perfecta, vemos que ha fallado en dos caracteres. También podemos ver la posición y la jerarquía que ha estudiado el texto, y podemos observar que, si antes esta jerarquía era la misma, ahora nos devuelve caracteres. Ups, disculpad, voy a cargar la imagen un segundo. La confianza que nos daba del texto era del 95%, 96%, 96%, 96%, lo que indica una precisión muy alta. Vemos que ahora esa nueva confianza se convierte, por ejemplo, en muchos 96, pero en la palabra "D", ya ha bajado a 80, y directamente "DOG", la "O" y la "G" y el punto no los ha detectado. Así que podemos ver que es bastante sensible a cualquier deformación de la imagen.

Ahora vamos a probar con una imagen manuscrita, que para nosotros es muy fácil de leer, pero vamos a ver si Tesseract es capaz de detectar texto manuscrito. Vale, y vemos que falla bastante. Esto es porque, como hemos mencionado en la presentación anterior, todo lo que se salga de documentos, por así decirlo, de uso, no es capaz de detectar, ya sea por rotación, mala calidad, manuscritos o cualquier tipo de letra que no sea la típica de un documento. Así que, todo eso no es capaz de detectarlo. Ha hecho su mejor esfuerzo, pero no ha sido capaz.

Vale, ahora vamos a hacer un pequeño ejercicio, por último, para finalizar la extracción de entidades en la que vamos a leer esta página y la vamos a imprimir. Es un documento legal, extraído de internet, que no significa nada, está en formato imagen, así que no podemos extraer ningún texto a priori. Pero digamos que quiero tener un sistema que automatice esto, que los usuarios me puedan enviar estos documentos en foto y yo pueda acceder al nombre, al número de teléfono y a dónde reside. Entonces, vamos a crear un pequeño flujo de imagen en el que hagamos exactamente esto. ¿Cómo haré ese flujo? Pues cuando extraiga el texto de la imagen, podremos ver que tanto la palabra "resident name" como "fake person" en este caso están muy juntas, es decir, están seguidas en la extracción del texto. Entonces, si busco la palabra clave "resident name", me llevará automáticamente al nombre de esa persona.

Lo primero, como hemos dicho, es convertirlo a texto. Aquí nos lo ha convertido todo a texto. El texto que ha detectado no es muy bueno en este caso, porque la calidad del documento no es muy buena para empezar, pero lo que es necesario sí que lo ha extraído bien. Ahora vamos a realizar unas cuantas funciones de Python. Voy a explicarlas un poco por encima. Esto nos ayudará a eliminar caracteres especiales, como saltos de línea o diferentes caracteres del propio string de Python. Luego, separaremos el texto por saltos de línea. Disculpad, el string no eliminaría saltos de línea, efectivamente. Aquí separaremos el texto por saltos de línea, es decir, cada una de las entidades, cada una de las líneas, formará parte de un índice diferente en una lista. Luego, cada uno de los índices de esa lista lo separaremos por espacios. De esta forma, ya tendremos una lista que contiene todas las palabras del documento. Esto simplemente es para convertirlo a la raíz de palabras.

Ahora vamos a filtrar, vamos a eliminar todas las palabras que no tengan texto, porque, por alguna razón, a lo mejor ha habido dos espacios seguidos, así que no hay texto en esos espacios. Vamos a ver cómo se limpia el texto. Como hemos dicho, ya tenemos un array con todas las palabras, un poco limpio. Entonces, quiero extraer, como hemos comentado antes, el nombre, el número de teléfono, la ciudad y el código postal. Podemos ver que en la imagen tenemos que buscar "resident name", "phone number" y también "zip", que estaría aquí. Así que vamos a hacer eso mismo, buscarlo, y la función que tiene para buscar es "index", que es una función de Python. Entonces, buscaré "name" y me quedaré con el índice de este array que contiene la palabra "name", y lo mismo con el resto. Ahora, sabiendo que el nombre me espero que tenga dos palabras, que con el teléfono me espero a lo mejor dos palabras también, con la ciudad de residencia también me espero unas cuantas palabras predefinidas y el código postal es solo una palabra, puedo acceder a esa información dentro de la lista. En este caso, el índice también está un poco hecho a propósito para que funcione bien, porque he comprobado que funciona correctamente. Pero no he extraído esa información de forma automática. Es decir, he proporcionado una imagen de entrada, completamente sin texto extraíble, y tendría que haber una persona detrás accediendo a esa información y leyéndola. Pero se le ha dado a Tesseract y, con un poco de programación básica en Python, he sido capaz de extraer las entidades que quiero. ¿Vale? Este es un ejemplo muy básico. Efectivamente, no es la mejor forma de hacerlo, pero es un ejemplo que demuestra la capacidad y la facilidad que tienen estos algoritmos, con muy pocas líneas de código, para crear un sistema automatizado de extracción de esas entidades.

Esto sería el laboratorio y con esto concluye la práctica de OCR. Cualquier duda o sugerencia que tengáis, me podéis escribir, como hemos comentado, tanto en LinkedIn como en GitHub, y también dentro de LinkedIn está mi correo electrónico si os resulta más conveniente.

### Generar Imágenes con Stable Diffusion
![[502.E1_Generar_Imágenes_con_Stable_Diffusion.mp4]]
[Generar Imágenes con Stable Diffusion](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion)
[Generar Imágenes con Stable Diffusion (PDF)](503.E1_StableDiffusion-230120-152902.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1KwcbPDiRL_RArlVdWvopIZ-_nEVICn1K?usp=sharing)

Hola y bienvenidos a un nuevo curso de W3. En este curso vamos a abordar Stable Diffusion y diferentes modelos de traducción de texto a imagen o de generación de imágenes a partir de texto. La idea que se había planteado para esta clase era comenzar con un poco de la trayectoria que han seguido estos modelos, ver un poco sus inicios, cómo han comenzado, las primeras redes que realizaban este tipo de tarea y luego también pasar directamente a dónde está el estado del arte en este momento, los modelos actuales, qué tipo de imágenes pueden generar y para qué se pueden usar.

Como ya sabéis, esto también interesa tanto a nivel de investigación como a nivel de deep learning e inteligencia artificial. Evidentemente, es un tema extremadamente interesante, con mucho futuro y que ha tenido un desarrollo significativo en estos últimos meses, sobre todo en los últimos dos años. Entonces, veremos un poco por qué existe ese interés y luego también cómo se puede usar y qué significa para el arte, sobre todo porque, claro, ahora que se está poniendo de moda, por ejemplo, la generación de arte digital, de NFTs, de arte digital y de compra y venta de estas piezas de arte, este tipo de modelos también ayudarían o cambiarían un poco ese tipo de mercados. De tal forma que cualquiera que tenga conexión a internet, y la mayoría de las veces de forma gratuita o pagando muy poco dinero, puede generar imágenes indistinguibles de imágenes reales o de piezas de arte generadas por cualquier autor en cualquier estilo a lo largo de la historia, básicamente con este tipo de herramientas.

Por lo tanto, son bastante útiles de conocer y evidentemente van a ser muy prácticas y muy utilizadas en el futuro. Por último, vamos a ver una práctica, unos pequeños ejemplos hechos por mí para vosotros en Collab. Está todo bastante preparado, simplemente iremos ejecutando y explicando cómo generar imágenes de forma gratuita desde Collab con muy buena calidad. Eso lo veremos al final.

Para empezar, como ya hemos comentado, vamos a ver un poco de dónde vienen este tipo de modelos. La verdad es que son relativamente recientes. Por ejemplo, uno muy famoso fue Stack GAN, que fue del 2017 y representó un hito, por así decirlo, para este tipo de modelos porque hasta ese momento no se habían generado imágenes de tanta calidad. Evidentemente, para hoy en día no es una calidad muy buena, pero para aquel entonces la verdad es que era impresionante. Como no sé si se puede ver por la imagen, digamos que el modelo tiene dos etapas. La primera etapa generaba un pequeño boceto muy borroso, con mucho ruido, del texto que se intentaba generar y luego, en una segunda etapa, lo que hacía era, con el boceto y con el texto, intentar realizar una especie de super resolución o mejorar la resolución y la calidad de la imagen, teniendo en cuenta también lo que se quería generar.

Digamos que, pensando desde un punto de vista estadístico, es un problema que se llama "one to many" porque un mismo texto puede generar una cantidad casi infinita o infinita de imágenes. Por lo tanto, en este modelo no hay una respuesta correcta. Sí que existen respuestas incorrectas o, mejor dicho, respuestas lejanas a la idea que se quería generar, pero no existe solo una respuesta correcta. Esto hace que al entrenarlo sea muy complicado y complejo. En este caso, se entrenó esta red neuronal, la Stack GAN, utilizando la metodología GAN, que es una forma de generar imágenes en la que se contrasta la generación con un discriminador que te dice si lo que se ha generado es realista o no, por así decirlo. Intenta determinar si lo que se ha generado es falso o verdadero y, con la salida de este discriminador, también hay un bucle en el que se entrena al propio generador para que aprenda a engañar al discriminador. Al final, este bucle y este juego lo que hacen es generar imágenes con mucha calidad y con no demasiados datos, ya que es casi imposible o muy difícil, o teóricamente no se podría hacer overfitting de este tipo de modelos, ya que no ven los datos originales.

Esto sería en 2017. Vamos a dar un salto de cuatro años al 2021. En enero de 2021, la compañía que comenzó un poco esta nueva moda de generación de imágenes a partir de texto fue OpenAI. OpenAI, en enero del año pasado, 2021, lanzó su primera versión de Dall-E, que era un modelo que utilizaba un tipo de generación de imágenes que se llama VQ-VAE, que es Vector Quantized Variational Autoencoder. Este modelo utiliza una representación discreta de los datos en lugar de continua, como por ejemplo pueden ser las GANs o otros tipos de modelos más utilizados para este tipo de generación de imágenes. Esto lo que hace es que tenga muchísima más representación en ese espacio embebido y, además, que dé capacidad a muchas más variedades diferentes.

Entonces, gracias a esta nueva representación, digamos que fue más, entre comillas, fácil o fue un poco el desencadenante de que la calidad de las imágenes fuera mucho mejor. En este caso, se demuestra en la imagen de la izquierda, que es la del 2021, y la de la derecha se correspondería a su variante más actual en abril de 2022. Creo que es un poquito anterior, pero bueno, el paper revisado, la última revisión que se hizo, fue en abril de 2022, en el que OpenAI lanzó su segunda versión. El salto de calidad fue increíble, no solo en términos de calidad, sino también en tamaño y en entendimiento del texto. Un poco lo que cambió de un modelo al siguiente es que el siguiente no utiliza la metodología que hemos hablado de los VQ-VAE, sino que utiliza un proceso de generación de imágenes, en este caso o de datos en general, que se llama modelo de difusión.

El modelo de difusión es una técnica en la que a una imagen conocida se le añade ruido poco a poco, paso a paso, en el que en cada paso se le va añadiendo más ruido. En cada paso que se realiza, lo que se crea es una renderización que ve un poco cómo se ha añadido el ruido y que aprende a deshacer ese ruido, todo esto por pasos. Entonces, al final, por ejemplo, tenemos 700 pasos en los que se ha añadido ruido 700 veces. ¿Qué pasa? Que al final, después de añadir ruido 700 veces, te queda una imagen con muchísimo ruido, tanto ruido que es indistinguible de una imagen simplemente de ruido, de tal forma que la imagen original queda completamente oculta y eclipsada por el ruido. Pero como hemos tenido una renderización que ha aprendido o le hemos enseñado a deshacer cada paso, lo que vamos a hacer es empezar desde el paso 700 y deshacer, creando de esta forma hasta que deshaga todo el ruido, vaya quitando el ruido en estos 700 pasos. Entonces, al final nos queda una imagen sin nada de ruido, como las que vemos en la pantalla. Esto es lo que nos permite, dado que después de todos esos 700 pasos te queda ruido, básicamente porque la imagen ya ha sido completamente destruida. Lo que vamos a hacer es simplemente coger ruido aleatorio y pedir que deshaga los pasos.

Este proceso, a partir de ruido aleatorio, puede ser capaz de generar imágenes que no existen en nuestra base de datos, que no ha sido entrenado con ellas, por así decirlo. Entonces, lo que hacemos al realizar este proceso inverso de difusión, que se llama "quitar ruido", es condicionarlo con contexto. Si simplemente le ponemos una condición textual a este proceso de difusión inverso, lo que hacemos es condicionar que, digamos, genere o quite el ruido de esta imagen sabiendo que en esta imagen hay, por ejemplo, un cerro en un campo en el estilo de Monet, por ejemplo. Por lo tanto, es capaz de generar datos con muchísima más calidad y con un proceso de generación de imágenes mucho más estable que utilizando, por ejemplo, las GANs, como hemos visto en la diapositiva anterior. Esto permite también trabajar con datasets mucho más grandes, como por ejemplo el que fue entrenado Dall-E, que son billones de imágenes, lo que también permite tener información de una gran cantidad de diferentes objetos, artistas, animales, situaciones, y todo un poco.

Sin embargo, menos conocido, en mayo de 2022, Google también lanzó un par de modelos de este estilo. Google lo que hizo fue utilizar otro tipo de arquitectura que no vamos a detallar, pero que es un poco parecida a la anterior que hemos visto de VQ-VAE combinada con GANs, aunque era muchísimo más robusta en algunos problemas que tenía OpenAI. Dall-E tiene un gran problema, por así decirlo, que es que no sabe escribir. Como se puede ver en la imagen de la izquierda, estos serían textos generados por OpenAI y por Dall-E, que podéis usar todos pagando una pequeña cuota en su API. Digamos que si le pones un cartel que, por ejemplo, genere un cartel en el que se escriba "deep learning", ya veis que genera un poco de todo menos lo que le estamos pidiendo. Mientras que, por ejemplo, si a la imagen de Google le pedimos que genere una tienda en la que se pueda leer un letrero que diga "texto imagen", es bastante legible de forma humana. Mientras que en esta parte de aquí sería también la generada por Dall-E, en la que no ha conseguido escribir el texto y a veces parece como una especie de combinación rara de letras o que intenta hacerlo, pero no lo consigue. Esto es un poco de la potencia del modelo, que no permite tener tanto entendimiento de geometría. Sobre todo, le cuesta bastante, y es algo que también le cuesta a la mayoría de los modelos. La imagen también, que es la geometría del entendimiento de una escena a nivel geométrico, es algo extremadamente complicado que, obviamente, sabemos desde el nacimiento. Pero digamos que al decir, por ejemplo, "quiero una foto en la que haya 35 pelotas de ping-pong", a este tipo de redes le cuesta muchísimo porque tiene que entender, por ejemplo, dónde empieza y dónde acaba una pelota de ping-pong a nivel de píxel o a nivel de concepto, que se vean todas por la imagen y que no haya ningún tipo de contaminación entre conceptos. Entonces, claro, a partir de creo que eran de 7 u 8 objetos, se lía. Si le pedimos, por ejemplo, a partir de 10 pelotas de tenis o lo que sea, en principio se va a empezar a liar. Pero bueno, en texto pasa algo parecido, en el que, digamos, que en texto tienes que entender el orden de lo que se está escribiendo, aparte de también entender que luego un humano lo pueda leer. Es algo que OpenAI, Dall-E, perdón, no lo entendía y, por ejemplo, el de imagen, en este caso, sí que lo consigue entender.

Luego, Google sacó otra versión de un modelo de texto-imagen. Estos, como os digo, no están públicamente accesibles y no parece que los vayan a lanzar en ningún momento. Son modelos mucho más grandes que Dall-E, pero aparte de eso, también, digamos que la calidad de las imágenes, en principio, es mejor. Luego, bueno, ya podéis ver por las imágenes que no solo conceptos un poco más abstractos, como una cobra hecha de sushi o lo que sea, que a lo mejor también Dall-E podría llegar a generar, o por ejemplo, un wombat tomándose un cóctel con camiseta hawaiana, y todo eso también Dall-E lo podría llegar a generar. Pero en el texto, como vemos en la de la derecha, el texto es de muy buena calidad y cumple con todos los requerimientos de diseño y, sobre todo, de estilo que se le pide. Entonces, la verdad es que es un buen paso hacia esa dirección de entendimiento de la escena. Sobre todo se ve al final que el texto que se escribe es una excusa para saber si o determinar si un modelo es capaz de entender cosas muy complicadas, como por ejemplo en la escritura. Entonces, si es capaz de escribir texto que nosotros podamos leer, es una muy buena señal.

Una cosa a tener en cuenta también, que ha sido bastante comentada a la vez que se iban publicando este tipo de papers, es que, evidentemente, todo este research se está haciendo desde América principalmente. Por lo tanto, los datos de internet en general están un poquito, o sea, bueno, tienden hacia la cultura americana y europea. Entonces, es razonable decir que existe un sesgo en este tipo de modelos, dado que, evidentemente, hay un punto de referencia en el que se crean estos modelos. Por lo tanto, evidentemente, siempre que quieras, yo qué sé, por ejemplo, medir la palabra "exótico", en cualquier parte que te vayas de la tierra, la palabra "exótico" va a significar cosas completamente diferentes. En la imagen que vemos a continuación, la palabra "exótico" tiene una definición muy europea y muy, digamos, americana. Una persona exótica tampoco te va a salir como las que estamos acostumbrados a ver, y sale desde un punto de vista muy diferente al de, por ejemplo, cualquier otra persona de otro país. También, este por ejemplo, el punto de vista de una persona exótica puede ser uno de los menos perjudiciales, pero sí que otras sí que fomentan los estereotipos que tenemos en este tipo de países, lo cual es un poco evidente. Lo que estaba comentando en la diapositiva anterior es que, evidentemente, si los datos son basura, si le das basura, te da basura. Entonces, si los datos que le hemos dado son internet entero y, por lo que sea, está plagado de este tipo de tendencias americanas y europeas, evidentemente lo que saca este modelo va a ser ese tipo de cosas que ha aprendido. Esto puede ser, evidentemente, perjudicial. También, evidentemente, si a este tipo de modelos, en vez de simplemente decirle "quiero un terrorista", le dices "quiero un terrorista rubio con ojos azules", evidentemente te va a sacar un terrorista rubio con ojos azules. Entonces, esto es muy fácil de mitigar; cualquier persona lo puede evitar. Pero bueno, esto simplemente es lo que saca el modelo de Dall-E en este caso con palabras o con conceptos muy generales.

Hemos hablado mucho de Dall-E, que es de pago, y hemos hablado también de los modelos de Google, que no son accesibles para el público en general. Ahora lo que vamos a ver son modelos que son gratis o también otros modelos que son más baratos. En algunos de ellos veremos que tienen una prueba gratuita, pero que luego, si se quiere generar más, habrá que pagar un poco después. Otros no dejan prueba gratuita y otros son 100% gratuitos. De estos, vamos a ver un poco el estilo de imágenes que generan, los pros y contras de cada uno y un poco, pues también para que, si en algún momento alguien quiere hacer algún tipo de proyecto sobre esto, también sepa qué puede esperar de ejecutar un mismo texto en varios de estos sistemas.

Primero, mencionar otra vez más a Dall-E. Dall-E no tiene prueba gratuita, pero puedes generar 115 imágenes por 15 dólares y permite hacer una gran variedad de herramientas dentro de su página web. Este ejemplo de aquí sería simplemente poner texto, decir, en este caso, yo qué sé, "un par de ositos de peluche químicos en óleo", y generaría unas 4 o 5 imágenes. Pero también le podemos dar una imagen, como por ejemplo la de Vermeer, la de la mujer con el pendiente, y te genera variaciones de la imagen que le vayamos a dar. Esto se puede utilizar posiblemente para logos, para retratos, para otro tipo de arte, para que te genere, por ejemplo, piezas similares a las que tú quieres o fotografías, por ejemplo, de un diseño de una habitación y todo eso. No solo eso, sino que también sobre la misma imagen, otra herramienta que tiene Dall-E es que podemos estirar la imagen de forma coherente. Entonces, al estirar la imagen, esta es el cuadro original. La pieza que está aquí, también en el recuadro, es capaz de extender la imagen en todas las direcciones de forma arbitraria. O sea, no tiene final, simplemente el número de dinero que os queréis gastar en la que se puede ir generando parches por parche, viendo hacia dónde va la imaginación, por decirlo de alguna forma. Dall-E copiará el estilo, asegurará que haya una transición buena entre la imagen que le vayáis a dar y el resto, y que tenga sentido. Todo eso, lo dicho, es una herramienta muy potente. La calidad de las imágenes que genera es muy buena, pero es de pago y el modelo no es libre, por lo que no podemos acceder al modelo y ver qué pasa dentro del modelo. Simplemente podemos acceder a las salidas, lo cual, evidentemente, para la mayoría de las personas es más que suficiente. Pero a la hora de crear en la comunidad, por ejemplo, no hay ninguna comunidad de esto.

Otro modelo disponible es MidJourney, que es también muy famoso. Creo que es, en mi opinión, el segundo más famoso después de Dall-E. En este caso, se utiliza solo por Discord, en el que tú, como un bot de Discord, puedes pedirle que genere imágenes. En la forma gratuita, creo que se pueden generar unas cuantas, unas 20 o 30, no recuerdo el número exacto, y a partir de ahí son 10 dólares por 200 imágenes, bastante más barato que Dall-E. Al final, por imagen son unas cuantas fracciones de centavos. Por lo tanto, es interesante para la gente que lo necesite. La diferencia con Dall-E es que ha sido entrenado con artistas famosos de todo tipo, tanto artistas clásicos como artistas actuales y artistas, sobre todo, muy famosos de arte digital. Por eso también tiene una tendencia muy clara a sacar siempre una salida muy artística, mientras que Dall-E tiende a sacar algo más realista. Por ejemplo, la pieza de la izquierda es un poco una exageración de lo que saca de normal Dall-E. MidJourney tiende a crear muchas fractales, pero muy artísticas, mientras que también es capaz, como se ve en la pieza de la derecha, de generar algo que es muy imaginativo, por así decirlo, y no existe en la vida real, pero de forma más o menos realista.

MidJourney, también dentro de la aplicación de Discord, se le puede pedir que, una vez que genera cuatro imágenes parecidas a lo que tú le dices, puedes elegir una de ellas, por ejemplo, y aplicarle un algoritmo de super resolución. Por eso es capaz de sacar este tipo de imágenes. Aparte de que Dall-E solo puede generar imágenes de 1024 x 1024 píxeles, MidJourney puede generar imágenes de una resolución arbitraria. Luego, además, si se quiere más resolución, se le puede aplicar un algoritmo de super resolución varias veces hasta poder multiplicar por 8 la resolución y tener una imagen, creo que en 2K. Se puede llegar a tener una calidad suficiente como para hacer un póster o cualquier cosa que tenga buena calidad.

Por último, vamos a ver Stable Diffusion, que es un modelo que es 100% gratis y además está públicamente accesible. Bueno, se me olvidó comentar que MidJourney tampoco tiene acceso público al modelo. En este caso, sí. En este caso, nos dan el modelo, los pesos del modelo, el código suficiente como para ejecutar el modelo y ya está. Nos dicen que hagamos lo que queramos con eso. No está capado de ninguna forma, puede generar cualquier tipo de imágenes que le digamos. No tiene ningún filtro, ya que lo estamos ejecutando de forma local. Otra cosa es que elijamos ejecutarlo utilizando alguna API o utilizando Hugging Face, que en ese caso sí que va a estar capado. Pero si nosotros somos los que nos descargamos los pesos, no tiene ningún tipo de filtro de ninguna forma. Está entrenado con artistas modernos e imágenes de fotografía de imágenes normales y, por la experiencia que tengo, es muy bueno generando, sobre todo, piezas realistas, especialmente imágenes de personas. Como ya estáis viendo en las imágenes, estas son increíbles y cuesta mucho saber que no han sido hechas por inteligencia artificial. De hecho, en la mayoría, bueno, sobre todo en la de la izquierda, a menos que se te diga, es muy complicado. También, conceptos como, por ejemplo, el del medio, que evidentemente no es un retrato, por así decirlo, es un concepto imaginativo y la verdad es que también lo defiende muy bien. Este es el que vamos a usar luego en la práctica, así que tampoco voy a entrar mucho en detalle de cómo funciona. Evidentemente, se llama Stable Diffusion porque utiliza el modelo o la técnica de difusión que hemos comentado anteriormente, que también utilizaba Dall-E. Luego lo veremos, generaremos unas cuantas imágenes utilizando este modelo de aquí.

Stable Diffusion es gratis y está disponible. Lo que ha hecho la gente es crear una comunidad alrededor de este algoritmo o esta red. Por lo tanto, la gente, friki como puede ser cualquier programador muy interesado en la inteligencia artificial y que sepa de esto, coge ese modelo y, por ejemplo, ya que lo puede utilizar gratis desde donde quiera y como quiera, lo que voy a hacer es, por ejemplo, crear una especie de red social como Lexica. Es una red social de gente que le gusta mucho hacer ingeniería de texto para ver qué tipo de imágenes se puede llegar a generar. En esta red social, la gente sube textos y las imágenes que han llegado a generar con ese texto. Entonces, la cosa es que yo puedo ir ahí y coger una imagen que me gusta muchísimo y decir, en plan, a ver qué texto han utilizado para esto y me lo quedo. Pero esta imagen también me gusta, entonces, por ejemplo, puedo combinar ese texto con el que tengo para llegar a uno nuevo y generar algo súper chulo. También he usado esto para si tengo muy claro el estilo que quiero para una generación de una imagen, pero no sé muy bien qué ponerle o no sé exactamente a qué artista se puede atribuir este tipo de estilo o no conozco exactamente qué ingeniería hay que aplicar para escribir el texto exacto para que salga algo que yo quiera, algo muy decente o de muy buena calidad. Entonces, me voy a esta red social, como el que se mete en Instagram, y estoy un rato scrolleando buscando exactamente qué es lo que más se parece a lo que quiero generar. Digamos que también ayuda mucho a la inspiración, a coger ideas y todo eso.

Luego está Dream Studio, que creo que es la más famosa de todas, que es la que nos permite utilizar este modelo de forma online. No solo es la generación de imágenes a partir de texto, sino que también te permite hacer la edición de imágenes. Dream Studio, si no recuerdo mal, tiene una aplicación para poder utilizar el modelo de forma local. Es muy recomendable usarlo si se tiene una GPU muy potente; si no, evidentemente no se va a poder generar imágenes o va a tardar muchísimo tiempo en generar cualquier imagen. Pero digamos que lo bueno de utilizarlo de forma local es que no tenemos el filtro que sí que puede tener Dream Studio en su modalidad online. Creo que la aplicación para ser descargada, si se quiere usar en local, se llama Art Room. Creo que Dream Studio no es la compañía, es la página web, pero la compañía, si no me equivoco, se llama Stability AI. Entonces, si se quiere utilizar online, sí, porque por lo que sea no me apetece descargarme un modelo y usarlo en local, ya sea porque no tengo tiempo o porque no tengo un ordenador suficientemente potente, se puede utilizar en su modalidad online con Dream Studio. Y si, por lo que sea, sí que me interesa quitarme el filtro y que no tenga ningún tipo de límite ni tener que depender de la conexión a internet, nada de eso, Art Room nos permite descargar el modelo y utilizarlo sin ninguna de esas limitaciones.

Por tercera, una tercera creación que he visto por ahí es Plant Mania, que permite crear textos para generar buenas imágenes. Digamos que tú le escribes un texto como lo podría escribir yo o otra persona que no sabe cómo escribir buenos textos para este tipo de modelos. Por ejemplo, yo quiero una imagen de una fotografía de un gato. Entonces, una vez que le das esta idea, te escribe un texto muchísimo mejor y mucho más capacitado para que, una vez que luego lo pongas en Stable Diffusion, la imagen que genere tenga muchísima más calidad que si no le dices solo una cosa en general. Porque una cosa que vemos en estos modelos, sobre todo en este tipo de difusión, es que cuanta más información le des, es mucho mejor. En cambio, si le das muy poca información o algo muy general, las imágenes que genera son de peor calidad. Por lo tanto, aplicaciones como esta son muy útiles porque saben exactamente qué tipo de texto escribirle, en qué formato, en qué orden, porque son cosas que afectan bastante a la generación final de las imágenes. Esta herramienta habrá más, seguramente. Yo sí que conozco alguna más, pero esta es la más famosa y una que yo he usado bastante y he tenido muy buenos resultados, así que la recomiendo.

Por último, Photoshop sí que ha tenido ya implementaciones o ideas o conceptos, no sé si del propio Adobe de Photoshop, pero para implementar este tipo de herramientas en Photoshop. Esto sería, evidentemente, para aquellos que hagan algo de arte digital o edición de fotografía, el poder, por ejemplo, quitar cualquier cosa de la imagen utilizando este tipo de modelos o completar una imagen si falta un trozo. O, por ejemplo, simplemente decirle a esta persona "quiero que tenga el pelo rubio ahora" o "esta persona quiero que tenga un traje en vez de camiseta", y todo eso lo genere de forma automática en segundos y que quede tan realista como si hubiera sido una foto de verdad. Es algo muy interesante que está a punto de venir. También, un poco gracias a esto, para finalizar esta diapositiva, explicar un poco el hecho de que estas ideas han sido gracias a que Stable Diffusion ha sido lanzado al público sin ningún tipo de restricción. Al final, MidJourney o Dall-E no tienen este tipo de ventajas, ya que no han lanzado el código. Claro, el resto de personas que queremos crear algo con esos modelos no se nos permite, ya que este código está cerrado. Pero en cambio, los de código abierto sí que al final crean una comunidad que explota. Evidentemente, esto ha convertido a este modelo en algo muy famoso y muy utilizado, que la gente ha ido mejorando. Al final, en este caso, es la comunidad lo que ha hecho que este modelo sea muy grande.

Por último, simplemente como curiosidad, digamos que por el auge de generación de imagen a partir de texto también han surgido otras ramas, por así decirlo, que surgen de esta nueva línea de investigación o de esta línea muy avanzada de investigación que generan otro tipo de datos sintéticos a partir de texto. Se ha visto un poco también, a partir de imágenes, que también fuera del ámbito del arte y de la creación artística ha habido, por ejemplo, generación de imágenes de rayos X para aumentar datasets de reconocimiento de tumores o de otro tipo de problemas médicos. También se ha utilizado en otro tipo de generación de imágenes, pero que también se puede generar otro tipo de datos con metodologías muy similares a las que hemos visto con imagen, pero por ejemplo en audio, en vídeo, en imágenes sensoriales tridimensionales, etc. En este caso, solo vamos a verlas porque son también más interesantes, más curiosas, pero también más famosas.

En audio, Meta, en la antigua Facebook, creo que creó un modelo hace muy poco, hace creo que un mes, que se llama AudioGen, que como su nombre indica, es un modelo que genera audio a partir de texto. Esto está muy bien porque, claro, imagina que si, por ejemplo, yo soy un productor de sonido y quiero exactamente meter un sample en algún audio que esté creando, va a haber una página web en un futuro no muy lejano en la que yo puedo escribir, yo qué sé, "sonidos de tacones de alguien caminando sobre parque" y "un bebé llorando de fondo", y que te genera un audio de una longitud que tú quieras sobre eso. Pero no solo eso, sino que también este tipo de modelos, no recuerdo si era este modelo en particular, pero que le podemos dar los primeros tres segundos de audio de una canción y te la continúa hasta el infinito, lo cual es una pasada. Evidentemente, a nivel de composición, ya no solo es capaz de hacer lo mismo que hemos visto con imágenes, que es prolongar una imagen hasta el infinito, sino que también con audio. Y, evidentemente, una combinación de ambas, decir en plan "vale, simplemente contexto", decirle "quiero que me genere una canción de cuatro minutos y medio que sea folk mezclado con electrónica y que en el estribillo haya un piano". En un futuro, esto no sé si existe ahora, creo que no, pero no es ninguna tontería y es algo que vamos a ver, si no el año que viene, en el siguiente. Pero me extrañaría mucho que no lo veamos ya a principios del año que viene. Bueno, por contexto, esto se está grabando en diciembre de 2022. Entonces, la gente que lo esté escuchando en el futuro ya me dirá si tengo razón.

Por último, en vídeo, como también como curiosidad, Google creó Fenaki, que es un modelo de generación de vídeo a partir del texto, lo cual ya es muy fuerte porque al principio crearon uno que podía generar unos GIFs de una duración determinada. Tú lo escribías, por ejemplo, "que sea un oso panda volando en el cielo" y te creaba un GIF de unos, creo que eran unos tres o cinco segundos de eso. Pero luego crearon uno que se llama Fenaki, que lo que podía hacer era seguir un guion, de que el texto fuera cambiando y, digamos, que el vídeo iba cambiando de forma adaptativa. Todos los frames tenían sentido entre ellos. Bueno, yo quería entrar para enseñaros luego aquí. Luego podéis estar, obviamente, toqueteando todo lo que queráis, pero un poco por encima, podéis ver que el texto va cambiando. En este ejemplo de aquí, por ejemplo, te dice "un oso panda fotorrealista que está nadando en el océano en San Francisco". Luego dice que se va debajo del agua, luego que ve peces de colores y de repente que es un panda el que está debajo del agua. Entonces, podemos ver cómo está por encima, se sumerge, aparece un pececillo por ahí y de repente se convierte en un panda. Podéis ver que la transición es perfecta, o sea, es extremadamente buena. Lo mismo con otros, como puede seguir en plan de que primero esté buceando en el océano, que luego pase a la tierra a andar por la playa y luego que aparezca, por último, una hoguera mientras que la cámara se aleja de él, hace un plano alejado y cómo sigue exactamente el guion. Con esto, por ejemplo, han creado un vídeo de dos minutos, que es el vídeo más largo de la historia, siguiendo todo este guion de aquí, que podría ser, por ejemplo, la introducción a una película. Las imágenes, como podéis ver, no son extremadamente buenas, así que se ven muchos efectos por ahí, pero bueno, yendo un poco a la diapositiva de las primeras que hemos visto, ya habéis visto que Dall-E, desde la primera versión hasta la actual, tardó un año, menos de un año. Entonces, por lo tanto, no me parece una locura pensar que todo esto, dentro de menos de un año, podremos darle el guion de una película y que pueda generar una película entera en HD o incluso en 2K, en la que solo falte poner la voz de los actores o la música y ya está.

Esto es un poco lo que quería comentar. Ahora sí, si queréis, nos vemos en el Collab y ejecutamos unos cuantos ejemplos para poder ver las imágenes que podemos llegar a generar utilizando este modelo de difusión.

Hola de nuevo, aquí estamos en el Collab. Aseguraos, cuando entréis en Google Collab, que cogéis una sesión con GPU. Yo ya me he conectado de antemano. Lo de la GPU es importante, ya que no es limitante, pero sí que nos va a permitir generar imágenes más rápidas y de mejor calidad que si utilizamos la CPU. Una vez que instalemos las librerías, vamos a loguearnos con nuestra cuenta de Hugging Face. Esto es importante porque vamos a utilizar la librería de Stable Diffusion de Diffusers, que es una que va a cargar un modelo de Hugging Face. Por lo tanto, vamos a tener que tener acceso a la API de Hugging Face, que es súper fácil. Simplemente os tenéis que loguear. Ahora veremos cómo hacerlo. Simplemente vamos a ejecutar de momento las celdas en orden y aquí nos va a pedir que escribamos el token. Va a ser muy fácil. Para crear el token, yo lo voy a tener aquí. Yo tengo mi cuenta de Hugging Face hecha. Simplemente, clicando una vez que tengáis la cuenta hecha, clicando en el link que aparece, os va a salir directamente la clave del access token. Yo simplemente con copiarlo lo tenéis aquí, le dais a login y dicen que el login fue exitoso.

Una vez creado el login exitoso, vamos a cargar el modelo directamente desde Hugging Face. Aquí, muy importante ver, o una curiosidad, es ver que estamos utilizando el modelo de FP16 o de float 16. Normalmente, el modelo es de float, float es un tipo de variable en Python que representa números con decimales. En este caso, utilizamos una precisión de 16 bits. Lo normal es utilizar el modelo de, digamos, que el modelo base es de 32 bits, pero para quitarle un poquito de carga de computación a Google, quitando muy poca, un poquito de precisión a la generación de imágenes, nos va a permitir generar imágenes muy buenas con requisitos de software un poco más ligeros. Vamos a cargar el modelo, va a tardar un ratillo mientras se cargan todos los pesos de la nube. Son 16 archivos. Un poco por hablaros de estos archivos, es tanto el modelo de reconocimiento de texto, el modelo de generación de imágenes y luego también hay un modelo que es de detección de imágenes que no sean "not safe for work" (NSFW). Esto lo que nos va a detectar son, pues lo que hablábamos antes de cuando utilicemos Stable Diffusion a partir de otra compañía, en este caso Hugging Face, están obligados también a tener un filtro de, digamos, de cosas que generaría la gente si no le dieras ningún tipo de filtro. En este caso, es un modelo que detecta si en algún caso generamos una imagen que tenga algún tipo de contenido explícito de cualquier forma.

Disculpad que había habido un problema con la versión de la librería de Diffusers. Ya está solucionado. Vosotros vais a tener el repositorio, perdón, el Collab como toca. Yo tenía un pequeño fallo. Si por lo que sea no os funcionara, os pasa el mismo error que a mí, simplemente ejecutando esta línea aquí, lo que va a hacer es instalar una nueva versión de estas librerías. Por tanto, en principio, el error a mí me ha desaparecido. Lo dicho, hemos cargado el modelo a partir de la librería de Diffusers que se conecta a Hugging Face introduciendo nuestras credenciales de Hugging Face y nos hemos descargado ya la pipeline que contiene una serie de modelos ya en el orden que toca. Entonces, lo único que vamos a hacer es meter la pipeline en la GPU para que, como hemos dicho, tarde menos tiempo en realizar estos cálculos. Entonces, por tanto, esto simplemente diciéndole que la pipeline entera la pase a la GPU.

Ahora, simplemente escribiendo un texto en el que queramos, en este caso el clásico de este tipo de modelos es escribir "este". Para empezar, simplemente lo vamos a llamar a la pipeline para que, dado el texto y un número de pasos, la altura y el ancho de la imagen y un número de aquí. Ahora vamos a ver lo que significa que te saque la primera imagen y luego la mostraremos por pantalla. Lo que hace esto de aquí, el número de steps, es la cantidad de pasos que se hace para quitarle el ruido. Cuanto más pasos, mejor calidad, pero también va a tardar mucho más tiempo. Aunque ponerle más de ciento y pico es demasiado, 50 es la verdad que genera imágenes de muy buena calidad y tarda bastante poco. Entonces, por tanto, eso. Luego, el tamaño 512 por 512 es un buen tamaño para una imagen y luego el guidance scale es, digamos, cuánto quieres que se parezca la imagen al texto que tú le has dado. Si es muy alto, por ejemplo, 14 o 15, te va a generar imágenes muy raras. Si se pone alto, tipo 12, va a tener menos capacidad de imaginación, por así decirlo. Y, evidentemente, también va a pasar lo mismo por el otro lado, pero al revés. Digamos que 7.5 es un buen número para decirle "escríbeme lo que te he dicho, pero también con un pelín de imaginación".

Si le damos, por ejemplo, para esta imagen, vamos a ver que va a tardar 50, que es el número de pasos que le hemos dicho. Va bastante rápido dentro de la generación, como ya hemos visto, como tiene que quitarle el ruido de forma paulatina. Es lo que tiene que ir poco a poco. En este caso, bueno, aparece el astronauta y el caballo, pero no ha sido una buena. Vamos a intentarlo otra vez. Así genera una imagen con un poquito más de sentido. Como hemos dicho, cada vez que lo ejecutamos, lo que hace es coger ruido aleatorio, por lo que cada generación va a ser completamente diferente. Esta es mucho mejor. A partir de ahora, si se quiere, por ejemplo, decir "en el estilo de Monet", vamos a ver si consigue, por ejemplo, captar el estilo de Monet. Bueno, es mucho estilo de Monet, pero seguramente si le damos otra vez a esto, generamos un texto mejor, es capaz de entenderlo. Esto también lo que hemos comentado, también hay un buen trabajo de saber exactamente qué tipo de texto preguntar. Esta imagen, bueno, es un poquito Van Gogh, pero no está mal. Pero eso, que es muy importante saber también cómo escribirle este texto para que genere exactamente lo que queramos. Pero bueno, esto simplemente es una pequeña práctica, una pequeña prueba, una explicación de cómo se puede utilizar de forma gratuita, completamente gratuita, este tipo de modelos y, a partir de ahí, hacer cualquier proyecto que os apetezca. Al final, la imaginación también, en este caso, ahora sí que no hay ninguna excusa, ya que la imaginación es el límite, ya que esto es más fácil de implementar y más chulo no puede ser.

Bueno, muchas gracias por asistir a esta sesión y nos vemos en otra.


### Estado del Arte (Video)
![[504.E1_Estado_del_Arte.mp4]]
[Estado del Arte](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41975345-p6-4-estado-del-arte-diego-bonilla)
[Estado del Arte (PDF)](505.E1_DeepLearningStateoftheArt-230120-153018.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1HBSP9yyfvaOqG3FebpSP7VrSFEuR7kUv?usp=sharing)

Bienvenidos a la sesión de "State of the Art". En esta sesión, veremos de manera general cuáles son las técnicas de "State of the Art" de las redes neuronales en la actualidad. Algunas de ellas tienen unas pocas semanas de antigüedad al momento en que se está grabando esta sesión, que es en diciembre de 2022, y otras son muy conocidas. Si ustedes han realizado algún tipo de tratamiento de imágenes con redes neuronales, seguramente les sonarán. Partiremos de las más conocidas hacia algunas más novedosas que quizás no sean tan familiares. Iremos viendo esto en orden de precisión, comenzando con las más antiguas y las que menos precisión tienen hoy en día, y luego iremos subiendo en esa escala.

Es importante notar que muchas de estas técnicas tienen más precisión que la anterior, aunque a veces solo sea por unas pocas décimas. No por ello son una revolución, pero sí es cierto que la arquitectura cambia lo suficiente como para considerarlas redes completamente diferentes o arquitecturas muy distintas a las anteriores. Por último, saltaremos a Google Colab, donde tengo preparadas algunas prácticas para poder ejecutar estos modelos de forma sencilla utilizando la librería de PyTorch. La verdad es que facilita bastante el uso de estos modelos, incluso los preentrenados. Simplemente cargaremos los modelos y ejecutaremos unas cuantas imágenes de internet para ver su precisión, comprobar cuál puede funcionar mejor, cuál es más robusto, y que, si luego deciden usar alguno de estos modelos internamente, tengan más claro cómo utilizarlos y que esta sesión les sirva también de referencia.

Lo primero que vamos a necesitar para comparar cualquier tipo de modelo es una base. En este caso, la base es un dataset llamado ImageNet, que quizás les suene, ya que es muy popular. Es la base más conocida hoy en día y contiene más de 14 millones de imágenes que han sido anotadas a mano. Esto significa que ha habido personas anotando durante muchas horas esas imágenes. Algunas de ellas contienen la clase a la que pertenece la imagen y otras también incluyen la posición que ocupa esa clase en la imagen. Es decir, no solo se indica que hay, por ejemplo, una persona, sino que también se especifican las coordenadas de la imagen que ocupa esa persona.

En cuanto a categorías, hay más de 20,000. Estas categorías son muy diversas, incluyendo vehículos, animales, razas dentro de esos animales, comida, objetos cotidianos, entre otros. Al final, son 20,000 categorías. Lo más común es que los modelos que un equipo de científicos desarrolle, o que ya hayan desarrollado, para medir la precisión o cómo se comparan con el resto, utilicen mil imágenes, es decir, las mil imágenes de mil categorías de este dataset en lugar de las 20,000, y midan la precisión de su modelo en esas mil imágenes. Hay un benchmark, una tabla donde cada modelo tiene su precisión, lo que facilita la comparación entre ellos.

Aquí podemos ver una de estas tablas, donde se indica la precisión en la tabla de la derecha. En este caso, como podemos observar, se hace con mil clases. Esta es la más reciente que he encontrado; no están todos los modelos que vamos a ver aquí, desgraciadamente, pero sí están la mayoría y hay otros que no están. De izquierda a derecha, normalmente se puede ver el nombre del modelo que se está tratando. Cada tipo de modelo tiene diferentes variaciones, de más pequeño a más grande, en este caso, y el número de parámetros en millones. Luego, los FLOPS son una unidad de computación que necesitas para ejecutar esa red neuronal, expresada en gigaflops. Finalmente, se muestra el número de imágenes por segundo que puede procesar y, por último, el "top 1", que significa que efectivamente la clase que sale con más precisión de tu modelo es la clase en la que está etiquetada esa imagen. Por lo tanto, eso sería un acierto, y el número de aciertos, el porcentaje de aciertos, se refleja en la tabla de la izquierda.

Toda esta tabla se divide en dos tipos de arquitecturas principalmente: las convolucionales y los vision transformers. Es cierto que todas estas se aplican bastante a visión, pero la arquitectura transformers también se aplica en la actualidad en temas de reconocimiento de texto. Las convolucionales también se aplican a temas de audio u otros tipos de datos. Tratarlo con imágenes es una forma de universalizarlo todo, pero no está limitado solo a imágenes; la mayoría de estas arquitecturas son muy versátiles.

En la tabla, podemos ver cómo a lo largo de los años ha ido aumentando la precisión en este dataset. Efectivamente, como se puede entender, cuanto más avanza la investigación en estos campos y cuanto más avanza la capacidad de cómputo de los ordenadores, que son más accesibles que nunca, también va aumentando la precisión de los modelos. Esto es directamente proporcional. Hoy en día, el último modelo que ha salido, al momento de grabar esto, es el FocalNet, que veremos al final, con una precisión que ronda el 83.5% a 84%, lo que lo convierte en el "State of the Art" en la actualidad.

Para empezar, como hemos visto en el índice, una red neuronal que en su día fue una revolución se llama ResNet. La ResNet tiene ese nombre porque "Net" se refiere a "network" y "Res" hace referencia a las conexiones especiales que incorpora, llamadas conexiones residuales. Las conexiones residuales permiten que, en una red neuronal normal, que como ya saben está formada por capas conectadas una detrás de otra, se salten algunas conexiones. Esto facilita que la información fluya más fácilmente a través de las capas y mitiga muchos problemas que tiene una red neuronal, como por ejemplo, hacer que sea más fácil de entrenar y que se pueda aumentar mucho de tamaño sin que haya problemas en el entrenamiento. Esto la hace mucho más robusta en general. Efectivamente, una vez que se introdujo, fue una revolución, ya que aumentó considerablemente la precisión en el reconocimiento de imágenes.

Aquí se refleja en la imagen de ResNet 50, que también veremos luego en la práctica. Cada dos capas hay una conexión residual. Es importante aclarar que las conexiones residuales, una vez que se conectan, pueden sumar las activaciones o concatenarlas. Lo más común es que se sumen las activaciones. La ResNet en general ocuparía esta parte de la gráfica que hemos visto, siendo de las primeras, ya que se sigue utilizando bastante. Es una de las más antiguas en esta tabla y tiene una precisión alrededor del 80% en ImageNet.

Ahora pasaremos a un salto tanto en arquitectura como en historia hacia los Vision Transformers, que fueron creados después de observar la precisión de los transformers, que inicialmente fueron diseñados para el procesamiento de texto. Al ver la precisión que tienen con el texto, se intentó aplicar esta técnica a imágenes y se obtuvieron muy buenos resultados. Así se creó un Vision Transformer, que es la versión de imagen del transformer. A partir de aquí, el modelo es exactamente igual que los transformers aplicados a texto.

Esto nos lleva a la idea de que, tal y como funciona nuestro cerebro, la información se trata de forma muy similar, aunque provenga de diferentes dominios. Si tu cerebro tiene suficiente capacidad de cómputo, se puede adaptar a cualquier tipo de datos que reciba, siempre que sean representativos de algún tipo de información. En el caso del procesamiento de texto, es más directo separar, por ejemplo, una frase en palabras, aunque en realidad se separan en entidades más básicas llamadas tokens. Sin embargo, en imágenes no había una forma clara de hacerlo hasta que se les ocurrió, en el paper de Vision Transformer, separarlas en parches de 16 por 16 píxeles. De esta forma, se trata la imagen como diferentes palabras que forman una frase completa, que sería la imagen entera, si se quiere entender de esa manera. Esto reduce considerablemente la carga computacional en comparación con pasarle un píxel como un token de la imagen, lo cual se ha intentado y resultó ser inviable, sobre todo porque el transformer, y ahora vamos a explicar un poco lo que significa, es una arquitectura que intenta prestar atención a cada parche extraído, observando cómo se relaciona con los demás parches.

Esto crea una atención global donde cada parte de la imagen presta atención a la parte que le interesa, generando un mapa muy global de las características. Esto es mucho más potente, por así decirlo, que las convolucionales, que en principio procesan por partes de la imagen. El obtener ese mapa global no se llega a hacer o se realiza en etapas muy finales de la red neuronal. Comparando los Vision Transformers con las convolucionales, las convolucionales son mucho más fáciles de optimizar, pero una vez que optimizas los Vision Transformers, que requieren mucho más tiempo de entrenamiento y más datos, sí es cierto que han demostrado tener mucha más precisión y robustez que las convolucionales.

También es importante mencionar que tanto los Vision Transformers como las redes convolucionales tienen una capacidad llamada "inductive bias". Esto significa que cada red, tal y como está formada, tiene una capacidad intrínseca. Se ha demostrado que, por ejemplo, los Vision Transformers prestan más atención a la forma de los objetos, mientras que las convolucionales prestan más atención a la textura de esos objetos. Esto es algo a tener en cuenta. Además, es cierto que las convolucionales tienen más variaciones que los Vision Transformers, por lo que, en general, los Vision Transformers son más robustos a las distorsiones de imágenes y a las permutaciones que pueden tener los parches de las imágenes. Sin embargo, a pesar de la universalización de los beneficios de una arquitectura sobre la otra, al final depende de cada tarea aplicar una arquitectura u otra.

Ahora vamos a ver los Swin Transformers. Una vez que hemos visto los Vision Transformers, son muy fáciles de entender. Lo que hacen es que, en la parte que hemos visto de los Vision Transformers, se separa la imagen en parches de 16 por 16, y en ningún momento la atención se computa entre los parches de la imagen de forma global. En el Swin Transformer, por otro lado, esos parches se dividen en otros pequeños parches. Así, la atención primero se computa a nivel local dentro de estos parches y luego se acumulan para calcularla a nivel global. Esto hace que la arquitectura sea más densa y que la atención sea más fina, por lo que se atiende a parches mucho más pequeños que en la arquitectura de Vision Transformer que acabamos de ver.

No solo eso, sino que además tiene otra complejidad computacional diferente a la de Vision Transformer que la hace más eficiente. Gracias a esto, se utiliza tanto en clasificación de imágenes como en detección de objetos, que consiste en dibujar un cuadro alrededor de los diferentes objetos que forman una imagen. También se utiliza para segmentación, que es a nivel de píxel, donde se determina a qué clase pertenece cada píxel. Por ejemplo, en una imagen donde aparece una persona y un coche, se sabe qué píxeles forman parte de la persona y cuáles forman parte del coche. Esto se puede observar fácilmente si han hecho alguna videollamada últimamente, ya que lo que segmenta a la persona del fondo de pantalla utilizaría esta arquitectura. En la tarea de segmentación, se identifica qué píxeles forman parte de la persona, no solo de la piel, sino también del pelo o la ropa, y el resto se identifica como que no forma parte de la persona.

En la tabla que hemos visto anteriormente, podemos destacar la parte donde están estos transformers. Vemos que tenemos el Tiny, el Small y el Base. Como hemos mencionado, cada uno de estos tiene un incremento de casi el doble de parámetros que el anterior, por lo que su complejidad de ejecución aumenta considerablemente. La velocidad a la que se procesan las imágenes disminuye bastante y, por último, también es cierto que, cuanto más grande es la arquitectura, en un orden más normal y lógico, también aumenta la precisión general. Es verdad que, como hemos visto, parece poco, porque al final ResNet es una arquitectura más simple, entre comillas, pero tiene un par de puntos solo por debajo. Sin embargo, ese par de puntos significa mucho en estas tareas. Por lo tanto, aunque parezca poco a nivel de números, es un buen salto.

Luego vamos a ver las ResNeXt, cuyo nombre evidentemente se relaciona con las ResNet que hemos visto anteriormente. Esta es una nueva generación, en este caso, es una arquitectura un poco curiosa porque no cambia nada de ninguna capa en general de las convolucionales que hemos visto antes. Siguen siendo redes convolucionales; lo único que hace diferente es crear unos stacks de bloques que tienen la misma topología. Por lo tanto, la misma topología, como hiperparámetros de los filtros que se aplican, es compartida. Así, de una capa residual, que es de las que hemos visto anteriormente, donde simplemente pasamos la red normal, tenemos una conexión que se salta a todas las conexiones y se suma a lo que se procesa por otra parte. En este caso, se divide mucho más, incluso en 32 pads, como podemos ver, con la misma topología, y luego se suman cada uno de ellos. Además, tenemos otra conexión residual que se suma a todos. Esto hace que la precisión que puede llegar a tener sea más robusta.

En la tabla que hemos visto, que es diferente a las que hemos visto anteriormente, podemos observar que es una arquitectura bastante reciente al momento en que se está procesando esta clase. Podemos ver que tiene unos parámetros que no son muchos y unos FLOPS que tampoco son demasiados, pero sin embargo, tiene una precisión muy alta. Según esta tabla, no sería mejor que el Swin Transformer, pero en general se ha demostrado en otras que sí lo supera. Sin embargo, no consigo encontrar estas tablas; seguramente en el paper de ResNeXt estarán reflejadas. Además, se ha demostrado que ResNeXt es mucho más robusta que el Swin Transformer, según explican en su paper. Esto también supone un buen salto desde la ResNet anterior sin cambiar la arquitectura. La verdad es que facilita mucho a las personas que han trabajado más con convolucionales que con transformers, que tienen menos tiempo de antigüedad. Esto también nos facilita el salto, ya que no es necesario cambiar a transformers y, simplemente utilizando las capas que conocemos de toda la vida, podemos obtener resultados similares a los transformers.

Por último, y más como curiosidad que como algo que vayamos a explicar en profundidad, aunque tampoco es muy complicado, vamos a hablar de las Focal Nets. Digo como curiosidad porque, al momento en que se está grabando esta clase, que recuerdo que es diciembre de 2022, tiene unas pocas semanas de antigüedad. Es una red desarrollada por Microsoft que cambia el mecanismo de atención de los transformers por uno basado en convolucionales. Como pueden ver, hay una pequeña guerra entre transformers y convolucionales, donde los transformers superan fácilmente a las convolucionales, pero luego, si se aplican esas mismas técnicas a las convolucionales, vemos que vuelven a resurgir. Al final, no hay mucha diferencia entre ellas, y menos si se copia un poco la arquitectura, simplemente cambiando algunos detalles de cómo se perciben los datos de las imágenes.

En este caso, el self-attention, sin entrar mucho en detalle, crea matrices de key, query y value, y aplica una atención entre el query y el key. Utiliza la función de atención, que luego se multiplica por el value para crear el output. En este caso, lo que hacen es una arquitectura un poco similar; si pueden ver una similitud entre los gráficos, la agregación de contexto se hace mediante redes convolucionales que utilizan kernels cada vez más grandes. Aunque parezca complicado de entender, son unas pocas líneas de código para implementar esta convolucional. Luego, también se concatena esto con el query para obtener el output, como se hace de manera similar en el self-attention.

Para comprobar esto, evidentemente, la tabla la he sacado del propio paper, donde se puede ver que FocalNet está resaltado en negrita. Tiene diferentes arquitecturas, como hemos visto, para saber cuáles son más potentes. Simplemente, fíjense en el número de parámetros: 28 millones de parámetros es la más pequeña y llega hasta 88 millones de parámetros, casi 89 millones, que sería la más grande. Efectivamente, la más grande es la que más precisión tiene y, hoy en día, es el "State of the Art" en clasificación de imágenes y reconocimiento de imágenes. Como ya hemos comentado, el ser capaz de reconocer imágenes también indica que esta red se podrá adaptar a otro tipo de datos.

Además, resulta que la FocalNet, al igual que el Swin Transformer, tiene ese "State of the Art" en segmentación de píxeles, no solo en clasificación de una imagen entera. Así que, si quieren un resumen de lo que hemos visto, el FocalNet es la red más utilizada. Si bien es cierto que no hay ninguna implementación fácil, está en GitHub; la pueden buscar fácilmente buscando por el nombre de FocalNet. Está desarrollada por Microsoft y es de código abierto. Como ya comenté, el modelo es muy fácil de entender porque son unas pocas líneas, pero no la veremos en la práctica porque es tan reciente que no tiene una implementación fácil, como las que yo estaba buscando para esta práctica.

El "State of the Art" hoy en día, como otra conclusión, es del 83.9% en ImageNet 1K. Como ya hemos comentado, vamos a pasar ahora a Google Colab, donde veremos cómo cargar unas cuantas imágenes utilizando la librería Pillow para cargar las imágenes como un objeto de Python. Utilizaremos la librería de PyTorch, que tiene internamente una sublibrería llamada TorchVision, dedicada a temas de visión. Usaremos los modelos que han sido entrenados con ImageNet 1K, lo que significa que simplemente cargaremos los pesos de los modelos, que se descargarán automáticamente en una sola línea. No vamos a tener que programar nada y, automáticamente, le podemos dar una imagen. Si en esa imagen hay alguna de las mil clases de las que hemos comentado, nos mostrará la salida junto con la precisión, o mejor dicho, la confianza que tiene en esa clase.

Muchas gracias por el momento por la parte teórica. Ahora nos vemos en la práctica.

Hola de nuevo, estamos ahora en Google Colab, donde vamos a ejecutar lo que hemos comentado. Una vez más, vamos a seguirlo de arriba abajo, sin saltar ninguna celda, ya que están todas en orden, e intentaremos ejecutarlas todas juntas, así les iré explicando un poco lo que hace cada línea por si acaso no está bien comentado o no se entiende. Vamos a importar los modelos automáticamente. Google Colab, por defecto, viene con la librería TorchVision instalada. Por lo tanto, simplemente desde TorchVision vamos a importar los modelos y vamos a imprimir en pantalla qué modelos vienen incluidos con esta librería.

Vemos que no son pocos. Estos son modelos que vienen de código abierto; pueden acceder al código en la página oficial en el repositorio de PyTorch. Están ordenados alfabéticamente, así que no se fíen de este orden en cuanto a precisión histórica. Es cierto que AlexNet, en este caso, creo que es la más antigua que hay aquí, o DenseNet, pero aquí vemos las ResNet, que también hemos visto, y los Swin Transformers, Vision Transformers. Aquí están los Vision Transformers, pero hay muchísimos más que no he mencionado, porque evidentemente estaríamos un curso entero para verlas todas. Sin embargo, están aquí para probarlas y todas ellas están preentrenadas.

Vamos a limpiar este output. En este caso, he decidido simplemente tomar cuatro clases, un poco básicas, en este caso de un gato y un perro. Podemos ver, si quieren, el enlace a las imágenes: este gato de aquí y este perro de aquí. También he cogido, si no me equivoco, una mangosta; he tenido que buscar porque no me acordaba. Una mangosta es un animal que, la verdad, no conocía mucho, pero es una clase un poco más rara. Luego, un fagot, que es un objeto poco conocido. Estas son clases que efectivamente están dentro del 1K, las clases que hemos visto.

Pueden ver que cada una es muy diferente a la anterior. Son animales muy conocidos, un animal un poco menos conocido y un objeto poco conocido. Vamos a descargar estas imágenes. El hecho de descargarlas es para que no tengan que conectarse a ninguna base de datos ni a ningún drive ni nada de eso. Simplemente las descargamos automáticamente y nos las dejará en nuestro sistema de archivos de la ejecución de este Colab. Utilizando la función `wget`, podemos especificar el nombre que queramos y simplemente el enlace a la imagen. Con éxito, nos han descargado todas.

También nos vamos a descargar esto de aquí. Este es un documento que contiene las mil imágenes y el nombre de las mil clases que hemos visto. Podemos ver que tiene muchas clases muy diferentes. La verdad es que no sé lo que son todas, pero algunas son un poco más raras. Sin embargo, no todas son animales; también hay muchos objetos dentro de estas clases. Ya las pueden ver ustedes si quieren, pero no son muy variadas. Al final, son mil clases y hay de todo. Por ejemplo, aquí hay un cañón, que sería un objeto, o una rueda de coche. Todo esto es capaz de reconocerlo.

Descargamos todo esto porque, al final, el modelo lo que nos va a devolver es simplemente un índice. Por ejemplo, 8. Entonces, claro, si el índice nos devuelve un número, tendremos que venir a esta lista y buscar en plan 1, 2, 3, 4, 5, 6, 7 y 8 para ver que es un gallo. Así que, si cargamos todo el nombre de estas clases, es mucho más fácil.

Evidentemente, lo que vamos a hacer no sirve para evaluar estos modelos, porque lo único que hemos hecho es cargar cuatro clases y ya está. No es una buena evaluación de un modelo simplemente con cuatro clases, pero es una demostración de cómo utilizar estos modelos, cómo cargar imágenes en estos modelos y cargar los pesos preentrenados utilizando esta librería. Verán que es extremadamente fácil.

Me he descargado de aquí el archivo que hemos visto antes con las clases y lo que voy a hacer es abrirlo. Utilizo la función `open` que viene con Python y simplemente lo que voy a hacer es abrirlo y, a la vez que lo abro, también lo voy a separar por líneas utilizando la función `readlines`. Esto nos devuelve una lista con las diferentes líneas, así que simplemente voy a guardar en una lista que se llama `labels`. Por lo tanto, `labels` tendrá una lista con todas estas imágenes. De hecho, vamos a echarle un ojo. Vemos que todas las clases que hemos visto anteriormente están aquí, las mil clases. De hecho, si vemos el `length of labels`, esto nos va a devolver el tamaño que tiene. Vemos que son mil clases, como en otras prácticas que hemos hecho.

Vamos a importar la librería de Matplotlib para dibujar las imágenes que vayamos viendo. Entonces, me creo una función de `show_image` para mostrar las imágenes en la resolución que yo quiera. Vamos a utilizar la librería Pillow, que ya hemos importado anteriormente. No hace falta importarla de nuevo, porque viene con Colab. Vamos a subirla hacia arriba, donde ya la hemos importado. Dentro de la librería, utilizaremos la clase `Image`. En TorchVision, vamos a importar `transforms`, que veremos qué significa eso, y luego, por supuesto, PyTorch.

Lo que vamos a hacer en todas las imágenes es cargarlas a partir de las que hemos descargado anteriormente y convertirlas en formato RGB, por si acaso. Las guardaremos simplemente en variables y las mostraremos por pantalla, cosa que ya hemos hecho antes, pero simplemente para ver que se han cargado todas efectivamente.

Aquí es donde vamos a crear el flujo de preprocesamiento de las imágenes. Esto es porque las imágenes que les vamos a dar a los modelos tienen un tamaño fijo. En este caso, el tamaño universal que se utiliza es de 224 por 224. Luego, las convertiremos en tensores para poder dárselas a PyTorch y las normalizaremos. Esto son datos de normalización estándar de media y desviación estándar de la librería de ImageNet.

Ahora vamos a crear una función que sea para clasificar una imagen. Como vamos a clasificar muchas imágenes con muchos modelos, no queremos tener que escribir todo esto todo el rato. Así que crearemos una función que simplemente le llamaremos y ya está. En esta función, le daremos la imagen y el modelo. Lo que significa esta función es que sacará las labels de una imagen utilizando un modelo. El modelo lo pasamos a modo evaluación. Esto significa que, internamente, PyTorch lo que hará es decirle que no vamos a entrenar el modelo, simplemente lo vamos a evaluar. Entonces, se hacen algunas modificaciones en las capas necesarias y no se pasa ningún gradiente, o digamos que no se almacena ningún gradiente.

Preprocesaremos la imagen con el flujo que hemos visto anteriormente y, entonces, ya la tendremos en tensor. Con esto, la convertiremos en un batch de una imagen y la pasaremos al modelo. El modelo te devolverá un vector de mil dimensiones, que corresponde a las probabilidades de que esa imagen pertenezca a cada clase. Como queremos la clase con más probabilidades, cogeremos el índice de la clase con más probabilidades y lo convertiremos en porcentaje utilizando la función softmax, que tampoco vamos a detallar, pero que hace que todas las probabilidades sumen uno.

Por lo tanto, esto nos dará la clase con más probabilidades y la mostraremos por pantalla, junto con el nombre que tenga dentro de la lista que hemos sacado de nombres de clases, junto con el porcentaje que tiene o que ha sacado el modelo de seguridad de que esa imagen pertenece a esa clase. Simplemente vamos a crear esta función y la metemos en memoria. Vamos a probar primero con ResNet. Dentro de ResNet, ya hemos visto que hay diferentes versiones, como ResNet 50, ResNet 101, y nosotros vamos a probar de momento simplemente con la ResNet 101 como un test.

Es muy importante ver que le estamos dando el flag de `pretrained` como `True`. Eso significa que va a descargarse los pesos de ImageNet 1K y los va a meter dentro de la red neuronal. También se puede poner como `False` si, por ejemplo, queremos entrenar la red nosotros desde cero. Una vez entrenada, podemos quitar algunas capas y hacer las nuestras para realizar transfer learning. Todo esto es muy fácil con la librería de PyTorch. Ahora nos va a descargar los pesos del modelo y ya tenemos el modelo cargado en memoria.

Lo que vamos a hacer es utilizar la función que hemos creado antes para imprimir y pasarle las diferentes imágenes, el modelo que queremos usar, y que nos imprima lo que cree. La primera es un gato. Fíjense que no solo nos dice que es un gato, sino que además te dice la raza de gato que es, que en este caso es un gato tigre. Con el perro, lo mismo; te dice que es un golden retriever con cierta porcentaje de seguridad. Aquí, efectivamente, te dice que es una mangosta, creo que se llamaba, con un 99.87% de certeza. Luego, el fagot, que es un objeto en inglés, tiene una mala traducción, pero también nos lo ha sacado con muy buena probabilidad.

Ahora vamos a ver muy rápidamente el resto de modelos. Aquí ven que cambia un poco la forma de escribir. Aquí era `pretrained` igual a `True`, aquí es `weights` igual a `True`. También ven que aquí los pesos ocupan 171 megas, mientras que aquí ocupan 1.14 gigas. Por lo tanto, ya es una demostración de que este modelo va a ser mucho más grande que la ResNet. Vamos a ver si tiene más precisión, pero tampoco, como lo he dicho, aunque falle en algunas de estas imágenes, no es una medida de la robustez de estos modelos.

Aquí, por ejemplo, saca otra raza de perro. Con el gato es lo mismo. Los últimos dos modelos bajan un poco la seguridad que tienen de que son esa clase, pero también las categoriza correctamente. Si quieren, luego pueden buscar en internet la imagen del perro y la raza que pone aquí para ver cuál es la que creen que se parece más.

Por último, el Vision Transformer. Bueno, por último, perdón, luego van los Swin Transformers, que son mucho más pequeños. Aquí también entiendan que estamos utilizando el `vt32`, que es un modelo medio grande, y luego el `swin transformer tiny`, que es el más pequeño. Aquí, los weights ya tienen que ser especificados, que quiere decir que son de ImageNet 1K y versión 1. Por lo tanto, son 108 megas, que es más pequeño incluso que la ResNet. Por lo tanto, también vamos a ver que va a tardar muchísimo menos que la ResNet.

Tampoco es que aquí estemos midiendo tiempos, pero parece que tiene mucho que ver con el Vision Transformer. Ha sacado la misma clase con unas probabilidades bastante parecidas, siendo una red muy diferente. Pero bueno, eso estamos viendo cómo clasificar estas imágenes. Ahora sí, por último, vamos a cargar el ComNet, que estaría un poco entre ResNet y Vision Transformer en cuanto a tamaño del modelo. Vamos a clasificar estas imágenes. Parece que la mayoría están de acuerdo en que es esta raza de perro, pero no todas. En general, han clasificado, aunque sean diferentes razas de perro, que es un perro. Tienen mucha seguridad en eso.

Con el gato no lo ha dudado en ningún momento, pero también ha detectado que el objeto que existe en esa imagen es un gato. Luego, algunos que son menos comunes también ha sido capaz de entenderlo. Por último, el FocalNet, como hemos comentado, no está la librería aún metida en esta librería, pero sí que en el repositorio oficial de Microsoft está todo el código abierto. Pueden descargar los pesos preentrenados también con ImageNet y para diferentes otras tareas.

Esto sería un poco la práctica de esta sesión. Hemos visto cómo coger una imagen, reprocesarla para cargarla en un modelo preentrenado y obtener las salidas de ese modelo, convirtiéndolas a porcentajes de seguridad de la red y sacando la clase con el porcentaje más alto, todo en muy pocas líneas de código.

## U7. El futuro del Machine y Deep Learning
### El futuro del Machine y Deep Learning (Video)
![[506.E1_El_futuro_del_Machine_y_Deep_Learning.mp4]]
[El futuro del Machine y Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris)

Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no mencionar un concepto que se llama Automachine Learning. Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolla inteligencia artificial. A priori, puede parecer una amenaza para la humanidad, pero no tiene nada que ver con esto. Simplemente, estamos hablando de que, como dijimos, la razón de ser de este campo de la IA y del Machine Learning es eliminar tareas repetitivas de bajo valor. Por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías.

De una forma muy rápida, tenemos plataformas que nos permiten realizar tareas que en otras ocasiones duraban meses. Debemos ponernos un poco en la piel de un científico de datos; tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. La primera sería lo que se llama desarrollo a bajo nivel, donde estaríamos hablando de código, específicamente de Python, que es el lenguaje oficial, por decirlo de alguna forma, de esta comunidad, aunque están apareciendo otros, como Go, que también pretenden ganar terreno. También tenemos R, que se utiliza más en el ámbito de la bioinformática. En definitiva, Python es el que tiene la mayor comunidad.

Pero también tenemos otro grupo, que sería el de los programas de alto nivel. Esta es una pregunta muy común entre los estudiantes: ¿qué son los programas de alto nivel? Son aquellos que son user-friendly, es decir, son interfaces de usuario que son amables y fáciles de entender. En pocas palabras, estamos hablando de escribir código versus tener herramientas que son prácticamente drag and drop, que son más familiares y que trabajan con el concepto de nodos.

¿Qué herramientas tenemos que usar en este segundo grupo de drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial del aprendizaje? Tendríamos herramientas como KNIME, ORANGE y BICML. ¿Qué podría comentarles sobre estas herramientas? Pues bien, básicamente KNIME está muy bien para el aprendizaje en fases tempranas y para poder prototipar, defenderse y entender de una forma esquemática lo que son los flujos de un algoritmo respecto a la ETL y en la fase de prueba, de ensayo y error con diferentes algoritmos. Estamos hablando de un concepto de nodos.

Obviamente, no son programas profesionales, sino que nos sirven de puente para poder llegar a pensar y aplicar todos los conceptos tanto teóricos como prácticos. Después tendríamos programas como Orange, que es del paquete Anaconda, donde se ubica Python. Han creado una interfaz mucho más amable y agradecida, que también nos permite prototipar muy rápido y de una forma muy intuitiva. En definitiva, nos permite acortar tiempos y, sobre todo, eliminar barreras al aprendizaje.

BKML sería un concepto que ya se sale de lo que son los nodos y realmente es un concepto más por ventanas. Estas serían las tres herramientas; las dos primeras son gratuitas, mientras que BKML es de pago. Sin embargo, este concepto, sobre todo el de nodos, es el que están utilizando ya plataformas como Azure o IBM Watson, en este caso Azure de Microsoft y IBM Watson, que son plataformas drag and drop.

¿Qué significa esto de conectar nodos? Conectar nodos implica que nos evitamos muchos errores de sintaxis de código, pero también significa que no tenemos tanta capacidad de edición como en el código. Es decir, al final, el código será la tecnología que siempre irá más rápido, será la más profesional y la más escalable de cara a ponerlo en producción. Pero sí es cierto que con el nacimiento de Azure o IBM Watson se abrió una puerta para trabajar con drag and drop que nos puede dar mucha agilidad a la hora de trabajar.

Desde este punto, es cuando empezaron a surgir nuevas tecnologías como Data Robot, H2O, Data IQ, Data Bricks y Rapid Miner, donde se centraron nuevamente en los nodos, pero yendo un paso más allá, aplicando Automation Learning. Automation Learning, al final, como hemos visto, para que tengan una idea muy sencilla, implica hacer una serie de comprobaciones y conceptos clave, como los missing values, los outliers y qué correlaciones hay entre diferentes variables. Esto en código lleva bastante tiempo, siempre teniendo en cuenta que debemos visualizar los datos, tomar decisiones y ver qué factores hay que corregir, que es el proceso de ETL.

Esto lleva mucho tiempo en código, dependiendo del nivel que tengas a la hora de saber escribir y depurar código. Por tanto, aquí surge una solución que es el Automation Learning. Podríamos decir que el líder a nivel mundial es DataRobot, pero ya existe un grupo bastante nutrido; RapidMiner, por ejemplo, también está haciendo bastante sombra. El AutoML lo que hace es que todos estos insights, porque al final estas plataformas te revelan insights que están en los datos, ya te están marcando y sugiriendo qué columnas habría que eliminar potencialmente, qué tipo de filas y qué correlaciones tienen entre ellas, todo esto de forma automatizada.

Esto, al final, lo podemos traducir en tiempo, y al final el tiempo es dinero. Claro, ¿qué problema presenta esto para el usuario? Básicamente, el problema es que las licencias aún son bastante caras. Estamos hablando de que una licencia de AutoML, de estos softwares de pago, puede oscilar entre 50.000 euros por usuario hasta 150.000 euros anuales. Este es un coste que no todas las empresas se pueden permitir, pero sí es cierto que lo que a priori era un mundo bastante cerrado, del AutoML, tiene la ventaja de que permite realizar de forma automatizada el EDA, lo que es el Exploratory Data Analysis, y la ETL, junto con la selección de algoritmos.

Lo que ha sucedido es que han aparecido ya versiones mediante las cuales, con código y de forma gratuita, podemos utilizar AutoML. En esta sesión de hoy, vengo a exponerles las principales herramientas de código gratuitas que podemos utilizar para trabajar con AutoML. Cuando hablamos de AutoML, debemos pensar que, como les he dicho siempre, la primera parte es entender el contexto, el negocio y la naturaleza del problema. Pero también debemos visualizar los datos; debemos pintarlos. Esta parte es muy importante porque si no visualizamos los datos, no llegaremos a entender si hay outliers, si hay correlaciones, y hablábamos también de matrices de correlaciones, por ejemplo, de relaciones entre variables.

¿Qué ocurre? Que este proceso es bastante lento también porque, a nivel de código, hay que ejecutar muchas sentencias y, a día de hoy, ya tenemos herramientas que nos permiten hacer AutoEDA. AutoEDA sería Automachine Learning aplicado al Exploratory Data Analysis. ¿Qué herramientas tenemos? En Python, tenemos la librería Pandas Profiling, que desde mi punto de vista es la más potente y la mejor resuelta. ¿Y por qué? Porque estamos hablando de que, básicamente, en 10 líneas de código, cargando nuestro dataset, nos va a ejecutar, nos va a pintar todos los datos referentes, nos va a visualizar todas las correlaciones y todas las estadísticas de este conjunto de datos, y además nos lo va a generar en HTML.

Con lo cual, estamos hablando de que en cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos. No solo tenemos Pandas Profiling, también tenemos Sweetviz, que de una forma diferente, con otro planteamiento, nos arroja prácticamente la misma cantidad de datos. El objetivo del AutoEDA, así como del Automachine Learning, será siempre ahorrar tiempo. Les dejamos aquí dos notebooks para ejecutar con Google Colab, donde pueden probar estas herramientas de forma muy sencilla, simplemente subiendo sus datasets. Verán que es una forma muy ágil de comenzar a trabajar con una base sólida.

Respecto al Automation Learning, creo que cualquier persona que haya trabajado en este sector como científico de datos, este es el sueño de cualquier científico: poder utilizar con código Automation Learning. ¿Por qué? Porque, básicamente, diríamos que es la máxima expresión de la practicidad. ¿Por qué? Porque al final, en código, tenemos acceso y podemos utilizar todas las herramientas que queramos, que van evolucionando de forma muy rápida. Pero si podemos utilizar Automation Learning, aprovechar las ventajas de otro tipo de software, que en este caso son corporaciones muy potentes con mucha inversión, y poder utilizar características similares a las que ellos proponen, esto realmente es algo muy práctico.

Con Automation Learning, hay consideraciones a tener en cuenta. Al final, Automation Learning nos va a hacer una... En el caso del código, no tenemos tantas herramientas para utilizar en el proceso de ETL, pero en el momento en que tenemos que probar diferentes algoritmos, debemos pensar que hacerlo prácticamente uno a uno sería un proceso muy lento, porque hay que invocar el algoritmo, probarlo y comprobar resultados. Aquí está la ventaja que nos da Automachine Learning: simplemente subimos los datos, realizamos la ETL, trabajamos las variables y automáticamente decimos que nos prueben el mejor algoritmo. Por ejemplo, le podemos pedir que nos pruebe 100 algoritmos y que nos ordene en un ranking los que mejor rendimiento tienen sobre los datos de prueba, ordenándolos por accuracy y por predicción. Esto representa un avance total, además de que ya te ofrece y te propone los hiperparámetros, es decir, nos soluciona dos partes muy pesadas.

Al final, probar diferentes algoritmos lleva un tiempo considerable en programación, pero una vez hemos seleccionado uno, elegir los hiperparámetros es la parte más abstracta, técnica y compleja; al final, estamos hablando de hilar fino. Cuando hablamos de ajustar hiperparámetros, simplemente, para que tengan una idea, es cuando tenemos una precisión del 90% y queremos subirla hasta el 95%. Este tramo es el más complejo de todos, porque parece que ya estamos cerca, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando de que estas herramientas nos pueden resolver muchísimos problemas.

Entre las más comunes, hay que saber que el Machine Learning cambia constantemente; es un campo que está totalmente vivo. Aquí les presentamos herramientas de AutoML en código gratuitas, como puede ser, por ejemplo, TPOT. También mencionaríamos H2O o H2O, dependiendo de cómo lo queramos leer, y AutoKeras. AutoKeras lo que está haciendo es auto-deep learning. Por decirlo de alguna forma, Keras es un framework de deep learning. Normalmente, cuando nos enfrentamos a crear una red neuronal, y para retomar un poco los conceptos anteriores, cuando tenemos muchos datos, cuando tenemos un dataset muy grande, podemos utilizar deep learning. Nuestro principal reto será decidir cuál será la arquitectura de esta red neuronal, ya sea por capas o por neuronas.

Lo que nos ofrece AutoKeras es realizar AutoML y revelar cuál es la mejor arquitectura posible para nuestro problema. También tenemos otra herramienta, Scikit Learn, que es una librería de Machine Learning de Python muy potente. Además, tenemos PyTorch y, seguramente, para el momento en que vean estas clases, habrá salido alguna más. Por nuestra parte, les adjuntamos la documentación para que puedan trabajar con ellas. A mí, personalmente, AutoH2O es la que más me gusta. Y el motivo es que, además, ofrece una serie de librerías que te muestran la explicabilidad del modelo, como los SHAP values, por ejemplo.

La explicabilidad del modelo es muy importante, porque no solo se trata de decir que este modelo es el que mejor predice, sino que también nos va a explicar, en nuestro caso de negocio, cuáles son las palancas o variables clave y en qué porcentaje para poder pasar a la acción y mejorar desde el punto de vista empresarial. Por tanto, les animamos a que prueben los notebooks que les dejamos aquí y que disfruten del concepto de Automation Learning.

### Auto ML, El futuro de la IA
**Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no hablar de un concepto que se llama Automated Machine Learning.**

Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolle inteligencia artificial. 

A priori, puede parecer una amenaza para la humanidad, pero no tiene nada que ver con todo esto. 

Simplemente, estamos hablando que la razón de ser de este campo de la AI y del Machine Learning es eliminar tareas repetitivas de bajo valor, por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías. 

¿Qué significa esto? Significa que de una forma muy rápida, tendremos plataformas que nos permiten hacer tareas que en otras ocasiones duraban meses. Debemos ponernos un poco en la piel de un científico de datos. Tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. 
1. Desarrollo a Bajo Nivel
2. Programas de Alto Nivel (Drag & Drop)

#### 1 | Desarrollo a Bajo Nivel.
Estaríamos hablando de código, de Python, que es el lenguaje oficial, por decir alguna forma, de esta comunidad, aunque están apareciendo otros como GO que también pretenden ganar terreno.

También tenemos R, que se utiliza más en el ámbito de la bioinformática.

#### 2 | Programas de Alto Nivel
Aquellos que son user friendly, interfaces de usuarios que son amables, fáciles de entender. 

En pocas palabras, estamos hablando de picar código versus tener herramientas que son, prácticamente, Drag&Drop, que son ya más familiares y que trabajan con el concepto de nodos. 

![[507.E1_Auto_ML_-_El_futuro_de_la_IA_1.png]]

##### Herramientas Drag & Drop
¿Qué herramientas tenemos en este segundo grupo del drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial de aprendizaje? Tenemos herramientas como:
- Knime (Gratuita)
- Orange (Gratuita)
- Big ML (De pago)

Knime está muy bien para el aprendizaje en fases tempranas y para prototipar, poder defenderte y comprender, de una forma esquemática, lo que son los flujos de un algoritmo respecto en la ETL en la fase de prueba, de ensayo y error con diferentes algoritmos.

> Obviamente, no son programas profesionales, pero nos sirven de puente para poder llegar a aplicar todos los conceptos, tanto teóricos como prácticos.

Después tendríamos programas como Orange, que es del pack Anaconda y, a su vez, Anaconda es el grupo donde se ubica Python, donde han hecho una interfaz mucho más amable que también nos permite prototipar muy rápido y de una forma muy intuitiva. 

Nos permite, en definitiva, acortar tiempos y sobre todo no ponernos barreras al aprendizaje. 

Big ML sería un concepto que ya se sale de lo que son los nodos, realmente es un concepto por ventanas. 

Estas serían las tres herramientas. Bueno, las dos primeras son gratuitas, Big ML es de pago, pero, en definitiva, este concepto de nodos, es el que están utilizando ya plataformas como Azure (Microsoft) o IBM Watson, que son ya directamente plataformas Drag & Drop. 

**¿A qué nos referimos cuando estamos hablando de conectar nodos? **
- Conectar nodos lo que implica es que nos evitamos muchos errores de sintaxis de código. 
- Pero lo que implica también es que no tenemos tanta capacidad de edición como tenemos en el código.
- El código va a ser la tecnología que siempre va a ir más rápido, la más profesional y la más escalable de cara a ponerlo en producción. 
- Con el nacimiento de Azure o IBM Watson, se ha abierto una puerta a trabajar Drag & Drop que nos puede dar muchísima agilidad a la hora de trabajar. 
- A partir este punto es cuando empezaron a salir nuevas tecnologías como Data Robot o H2O o DataIQ, DataBricks, RapidMiner, donde se centraron ya otra vez en los nodos, pero yendo un paso más allá, aplicando a Automated Machine Learning.

#### Automated Machine Learning
Como hemos visto, es necesario hacer una serie de comprobaciones de conceptos claves, como los Missing Values, los Outliers, qué correlaciones hay entre diferentes variables.

![[507.E1_Auto_ML_-_El_futuro_de_la_IA_2.png]]

Esto, en código, lleva bastante tiempo, siempre también teniendo en cuenta que tenemos que visualizar siempre los datos y tomar decisiones y ver qué factores hay que corregir y depurar, qué es el proceso de TLE.

Por tanto, aquí surge Automated Machine Learning como una solución. 

[DataRobot](https://www.datarobot.com/) es el líder a nivel mundial, pero ya existe un grupo bastante nutrido de [Rapidminer](https://rapidminer.com/platform/?product_marketing_c=studio&source_marketing=ppc&campaign_marketing_c=branded) que le está comiendo terreno.

Lo que hace el Automated Machine Learning es sugerir, de forma automatizada, qué columnas, qué tipo de filas y qué tipo de variables, hay que eliminar y las correlaciones entre ellas.

Esto lo podemos traducir en tiempo y el tiempo es dinero. El problema es que las licencias son bastante caras, desde 50000 € por usuario hasta los 150000 € anuales, un coste que no todas las empresas se pueden permitir.

Pero lo que, a priori, era un mundo bastante cerrado de la Automated Machine Learning, ya te permite realizar, de forma automatizada, el EDA, el Exploratory Data Analysis y la ETL, junto con la selección de algoritmos.

También han aparecido ya versiones mediante las cuales con código y de forma gratuita podemos utilizar Auto ML. 

**Cuando hablamos de Auto ML, tenemos que recodar la primera fase**

Entender el contexto, el negocio, la naturaleza del problema. Pero también visualizar los datos, pintarlos. 

Esta parte es muy importante, porque si no visualizamos los datos, no llegaremos a entender si hay Ouliers o correlaciones. 

Hemos visto también matrices de correlaciones, por ejemplo, de relaciones entre variables. ¿Qué ocurre? Que este proceso es bastante lento también, porque, con respecto al código, hay que ejecutar muchas sentencias y a día de hoy ya tenemos herramientas que nos permiten hacer Auto EDA. Auto EDA sería auto Machine Learning, es aplicado al Exploratory Data Analysis.

### Herramientas de Auto Machine Learning
Un buen ejemplo puede ser [Pandas Profiling](https://pandas-profiling.ydata.ai/docs/master/index.html)

![[508.E1_Auto_ML_-_Herramientas_1.png]]

#### Pandas Profiling
**El objetivo principal de esta herramienta, es proporcionar una experiencia de análisis de datos exploratorios (EDA) de una línea en una solución uniforme y rápida.**

- En diez líneas de código, tras cargar nuestro Dataset, nos va a a pintar todos los datos.
- Vamos a visualizar todas las correlaciones, todas las estadísticas de este conjunto de datos.
- Además, nos lo va a generar en HTML.
- En cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos.

> Auto ML es el sueño de cualquier persona que haya trabajado en este sector como científico de datos. Poder utilizar con código Auto Machine Learning es la máxima expresión de la practicidad.

Tenemos a nuestro alcance todas las herramientas que queramos mientras estas van evolucionando de forma muy rápida. 

Usando Auto ML podemos aprovechar las ventajas de otros tipos de software, detrás de los cuales hay corporaciones muy potentes y con mucha inversión, o sea, que podemos utilizar soluciones similares a las que ellos proponen.

  ![[508.E1_Auto_ML_-_Herramientas_2.png]]  

#### Consideraciones a tener en cuenta en Auto ML
**Hacer, manualmente, el trabajo que realiza Auto Machine Learning nos llevaría mucho tiempo, sería un proceso muy lento.**

Con Auto ML solo tenemos que:
1. Subir el Dataset.
2. Realizar la ETL.
3. Trabajar las variables.
4. Decirle que nos pruebe diferentes algoritmos y que seleccione, en un ranking, el que mejor performance tenga sobre los datos de Test, ordenándolos por precisión.

Esto supone un avance total. Probar diferentes algoritmos conlleva un tiempo bastante elevado en tiempos de programación, pero, una vez hemos seleccionado uno, seleccionar los hiperparámetros es la parte más abstracta, técnica y compleja, pues trata de hilar fino. 

Cuando hablamos de ajustar hiperparámetros, simplemente para que os hagáis una idea, es cuando tenemos una precisión del 90%, cómo subir hasta el 95%. 

Este tramo puede ser el más complejo de todos, porque parece que ya estamos, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando que estas herramientas nos pueden resolver muchísimos problemas. Entre las más comunes, sí hay que saber que el machine learning cambia constantemente. Es decir, es un campo que está totalmente, que está totalmente vivo. 

Aquí os presentamos herramientas de Auto ML en código gratuitas:
- Teapot
- H2O
- AutoKeras
- AutoPylearn
- AutoPyTorch

Cuando vamos a crear una red neuronal, tenemos un dataset muy elevado y podemos utilizar deep learning, nuestro principal reto va a ser en decidir cuál va a ser la arquitectura de esta red neuronal, vía capas o vía neuronas. Lo que nos ofrece AutoKeras, que realiza AutoML, y nos revela cuál es la mejor arquitectura posible para nuestro problema. 

A destacar Auto H2O, que gracias una serie de librerías que te muestran perfectamente el concepto de explicabilidad del modelo. 

La explicabilidad del modelo es muy importante porque no nos dice “este modelo es el que mejor predice”, sino nos va a explicar, para nuestro caso de negocio, cuáles son las palancas adecuadas, las variables clave y en qué porcentaje para poder pasar a la acción y poder mejorar desde el punto de vista de negocio.

Por tanto, os animamos a que probéis todos las herramientas y que disfrutéis del concepto del Auto Machine Learning.

## U8. Implementación de la AI en Negocio
### Graphext (Video)
En el siguiente vídeo os presentamos [Graphext](https://www.graphext.com/), una empresa dedicada al Análisis exploratorio de datos.

Gracias a su herramienta no-code, permite a sus clientes ser más autónomos y menos dependientes de otros Data Scientist, permitiéndoles así explorar datos de sus propios clientes y hacer preguntas ad-hoc, de forma independiente y con la facilidad de uso de una hoja de cálculo más el poder de un Notebook.

### Implementación de la IA a Negocio (Video)
![[510.E1_Implementación_de_la_IA_a_Negocio.mp4]]
[Implementación de la IA a Negocio](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866919-u8-victoriano-izquierdo-implementacion-de-la-ia-a-negocio)

En este máster ya os han estado enseñando bastantes cosas sobre datos e inteligencia artificial, ¿no? Y la realidad es que hoy en día sacarle provecho a todas estas técnicas es verdaderamente difícil si no sabes programar. Creo que se da bastante la circunstancia de que hay gente que tiene una capacidad muy analítica para las cosas y sabe hacer buenas preguntas con datos, pero eso no quiere decir que sepa traducir esas buenas preguntas a un lenguaje como Python o R. Creo tanto en esto que llevo cinco años con un equipo de 20 personas montando una herramienta de análisis de datos en la que intentamos ayudar a toda esta gente que se le queda corto el Excel y quiere ir más allá, haciendo ciencia de datos.

Data Science, más allá de agregar y pivotar tablas y hacer gráficos sencillos, busca dotarles de herramientas que les permitan explotar su creatividad y sus intuiciones a través de los datos que empiecen a recoger de lo que ocurre dentro de una plataforma que desarrollen, de sus clientes, de sus productos, de sus proveedores o del mercado. Hay una cantidad de datos públicos y privados que son de gran utilidad. También tengo la convicción de que se puede mejorar lo que no se mide. Y en eso es en lo que os voy a enseñar, lo que hemos estado trabajando durante este tiempo. Todo lo que hemos aprendido de Data Science lo hemos intentado llevar a una herramienta no-code, que es como se le llama ahora a esta corriente de herramientas que intentan hacer cosas que hasta ahora no se podían hacer sin código. Y de eso vamos a hablar.

Entonces, bueno, voy a empezar hablando y enseñando en qué competimos, para que quede bien claro exactamente dónde se posiciona una herramienta como la que desarrollamos, qué no somos y qué sí somos. Aquí en esta diapositiva lo que enseño es un dashboard. Un dashboard o un cuadro de mando, como le decimos en español, es el lugar donde se colocan esas métricas de negocio que a menudo son como la estrella polar de las empresas, hacia dónde quieren ir, cuál es ese número mágico de clientes o de facturación en el que se llega al breakeven, pero también de qué nacionalidad son esos clientes, cuánto te usan por mes... Bueno, hay un montón de variables. Literalmente, hoy en día, una empresa mide cientos de variables distintas y al final se seleccionan unas pocas, a las que se suelen llamar KPIs, como las métricas más importantes. Y eso está bien, y para eso se usan los dashboards.

El problema es que los dashboards normalmente te dicen qué ha pasado, pero no el por qué, ni tampoco te ayudan a hacer predicciones sobre lo que va a pasar. Entonces, obviamente, te dan intuiciones sobre esas dos cosas, sobre el por qué y el qué va a pasar, pero a menudo, a veces, llevan a equivocaciones también. Todos sabemos que la correlación no implica causalidad, pero además la mayoría de las cosas complejas que ocurren nunca son porque una sola variable ha cambiado, sino por un conjunto de factores que se dan en el tiempo. Ante ese tipo de preguntas y respuestas, ha empezado este ámbito que se llama Data Science, que de nuevo es un poco la contraposición de lo tradicional del Business Intelligence, que estaba más basado en esta idea de crear dashboards para todo.

Yo lo vería más como una especie de evolución complementaria, es decir, es interesante seguir teniendo dashboards porque hay métricas básicas que son las que hay que monitorizar a diario, que indican si va bien o va mal, pero en cuanto a entender el por qué cambia, ahí es donde creo que la ciencia de datos puede ayudar bastante. Voy a ilustrar todo esto con varios ejemplos. Además, hay otra cosa que no hacen los dashboards, que es trabajar con datos desestructurados. Una gran cantidad de información que capturan hoy en día las empresas no solo viene en formato de dato tradicional, que sería como filas y columnas, sino que también viene en forma de texto, imágenes o incluso vídeos. Muchísima de la comunicación que tenemos con los clientes, por ejemplo, a través de un chat, se expresa en texto. Y ahí, por ejemplo, se manifiestan un montón de deseos y cosas que nos pueden dar pistas sobre qué es lo que quiere la gente o por qué dejan de ser clientes, y cosas así.

Voy a saltar directamente aquí a la aplicación para que entendáis. Lo primero que hacemos cuando creamos un proyecto es conectarnos a una fuente de datos. Tradicionalmente, en una empresa, lo ideal es que acabes montando lo que se llama un data warehouse, que es una base de datos donde centralizas un montón de tablas que pueden proceder de muchas bases de datos distintas. Desde bases de datos internas en las que registras cosas básicas de la aplicación, hasta CRM como Salesforce o HubSpot, donde tienes información sobre los contratos de los clientes, la comunicación que tiene la gente de ventas, datos que tienen que ver con los eventos, con las acciones que realizan, con cada clic que hacen, y datos de comunicaciones a través de chat y demás.

Lo normal es que cojas esos datos, los pongas en un data warehouse y entonces intentes crear un modelo de tu cliente o de tus productos, ¿vale? En función de ese modelo de datos de esa tabla, como la que vemos aquí, tengo varias que voy a enseñar, ¿vale? Pues bueno, atacaremos un problema u otro, ¿no? Tengo dos datasets con los que quiero empezar. Uno creo que es muy intuitivo e interesante para todo el mundo. Es una encuesta de la estructura salarial en un dataset público que nos puede servir para entender los salarios de los españoles, por qué ganan más o menos, qué características predicen que alguien gane más o menos. Es verdad que podemos pensar que el cliente es España, o sea, los clientes son los españoles y el producto es el Estado, y queremos ver cómo optimizar esos salarios.

Pero también voy a usar este otro, de hecho, es por el que voy a empezar, que es de una empresa de telecomunicaciones canadiense que liberó unos cuantos datos de sus clientes, en el que vamos a entender por qué estos clientes de esta compañía de telecomunicaciones se van. Y, de nuevo, creo que la metáfora es bastante equivalente a cualquier otro tipo de datos. Además de estos dos, luego vamos a ver otros de otra naturaleza. Por ejemplo, vamos a ver ventas de productos, por ejemplo, de Mercadona, y vamos a ver qué productos se compran juntos más habitualmente, y también vamos a ver, incluso, transacciones en Ethereum de blockchain, y vamos a entender, por ejemplo, qué conexiones, qué patrones podemos revelar de dinero que se mueve entre exchanges distintos y direcciones.

Como veis, hay un montón de casos de uso distintos que vamos a intentar cubrir en los próximos 10 o 12 minutos. Empiezo por este. Lo que estamos viendo aquí en la pantalla es una base de datos de clientes de una empresa de telecomunicaciones de Canadá que tiene más de 7.000 clientes, en este caso, lo que han liberado al público para que lo use gente que quiera aprender cosas de ciencia de datos. Entonces, como vemos, tenemos más de treinta variables asociadas a cada una de estas personas y, automáticamente, nada más cargar el dataset, lo que va a hacer es que se crean estos histogramas y distribuciones que representan cómo está distribuida cada variable. En concreto, yo me voy a fijar aquí en cuánto pagan al mes. Si pinchamos aquí, vemos, por ejemplo, que la mediana son 70 dólares al mes. Pero también podemos ver los diferentes cuartiles, que son como el 25% de los que más pagan, que empiezan en 90 dólares al mes, y los que menos pagan son menos de 35, o sea, entre 18 y 35 al mes.

Entonces, claro, algo que inmediatamente me puedo preguntar es: oye, esta gente que paga poco, o sea, estos clientes que pagan poco y son muchos, ¿qué es lo que tienen en común? Entonces, al hacer eso, no sé si veis que conforme cambio esta selección, todo lo demás es reactivo. Y aquí a la izquierda me dice qué es lo que más correlaciona con esa selección. Y como vemos, son estas cosas, por ejemplo, el servicio de internet, que esta gente que paga poco es porque no paga por internet. Y como vemos, la mayoría de los clientes sí que pagan por fibra óptica y ADSL. Pero este grupo del 23% de clientes no paga por ello. Entonces, es una manera rápida de entender relaciones entre variables uno a uno. Pero lo que queremos entender es el churn, que es la variable que codifica si un cliente se va o no, o sea, si se ha ido o no de la empresa, si ha dejado de pagar.

Entonces, si pinchamos en los que churnean, aquí vemos automáticamente que me está diciendo que lo que más correlaciona es el tipo de contrato. Vemos que los clientes que tienen contratos mes a mes, que se renuevan mensualmente, suelen irse mucho más. Esto me indica que son el 55% de los clientes, pero el 88% de los que se van. Mientras que los que tienen contratos anuales o de dos años es más infrecuente que se acaben yendo. Luego vemos que esto está seguido por otras variables. Por ejemplo, si tienen contratado online security, si tienen soporte técnico o los meses que llevan siendo clientes. Aquí vemos que a partir del mes 20, la probabilidad de que un cliente se vaya es mucho más baja.

Entonces, rápidamente nos hacemos una idea, variable a variable, de qué es lo que está más relacionado con el churn. Y lo bueno es que además podemos encontrar rápidamente otra relación. Por ejemplo, aquí vemos que los clientes que churnan en general suelen pagar más de 70 dólares al mes. Pero si me enfoco en los que se van y pagan menos de 30, vemos que el total de cargos ha sido poco. O sea, son clientes que han durado muy poco. Entonces, el poder hacer estas preguntas de manera tan dinámica y sugiriéndote cosas con una interfaz visual, creemos que cambia bastante las cosas respecto a la manera tradicional de tener hipótesis muy buenas, excelentes, y usar los datos solo para validar una hipótesis. Aquí sería más bien que tengas una pregunta muy básica y, con la interfaz, con las cosas que te vamos sugiriendo visualmente, haciendo esa navegación, tú puedas ir teniendo mejores preguntas y refinarlas.

Esto lo hacemos gracias a técnicas de Machine Learning que aplicamos para que hagas un buen uso del Machine Learning. Es muy meta todo, ¿vale? Veréis que tenemos otras secciones distintas. Por supuesto, esto es una tabla en el centro que se va filtrando. A veces, sin más, vas a querer hacer un segmento y decir: ¿qué clientes tengo que paguen mes a mes, que paguen más de esto? ¿Y qué paguen al mes esto? Y entonces, a base de filtrar estas cosas, tendrás una tabla u otra. Pero como veis, aquí tenemos esta sección de comparación, que nos permite también hacer, antes hemos visto simplemente un segmento contra una selección. ¿Y qué pasa si saltamos aquí y comparamos los que pagan mes a mes, versus los que pagan de dos años y los que pagan de un año? Pues automáticamente aquí en la pantalla vemos cómo me sugiere que otras variables distinguen más esos tres segmentos.

Podríamos definir de nuevo cualquier segmento que quisiéramos de clientes, pero vemos que lo que más distingue el tipo de contrato es cuánto tiempo llevan siendo clientes. Es lógico, es difícil vender un contrato bianual a un cliente que lleva poco tiempo contigo, pero un cliente que ya lleva mucho tiempo y está contento es más fácil vendérselo. Y aquí vemos, en el gráfico se ve muy bien, que es en el mes 25, o sea, a los dos años, cuando la probabilidad de que un cliente tenga un contrato que no sea mes a mes es más alta que de mes a mes. Parece que fidelizar a estos clientes para un contrato más largo ocurre a partir del mes 24. Es interesante, porque entonces podríamos ver qué distingue a un cliente que empieza el mes 24 solamente, por ejemplo, en variables sociodemográficas. Podríamos hacer esa pregunta de una manera muy rápida.

Entonces, el análisis de datos es así, o sea, tenemos algunas intuiciones, observaciones, validamos o no que tengan razón y entonces, con las sugerencias que tenemos, nos hacemos preguntas nuevas que probablemente sean mejores. Como esta, hay otras interfaces como Plot, que permiten hacer cualquier relación entre variables. Por ejemplo, si yo digo: oye, pues ahora quiero ver el tipo de contrato, escribo aquí "Contract" y pongo, por ejemplo, "Monthly Charges". Quiero saber de media cuánto pagan según tipo de contrato. Y entonces, automáticamente me dice que tiene sentido que mire estos tres tipos de gráficos. Si le doy a este gráfico, automáticamente me sale ordenado de más a menos cuál sería la distribución de lo que pagan. Aquí vemos que los de mes a mes son los que pagan contratos más altos, después los de un año y después los de dos años. Aquí vemos que la diferencia es de 73 al mes a 64, o sea, me parece que los clientes con contratos más largos están ahorrando una media de 10 euros al mes, pero también vemos que la dispersión es más alta. También vemos que los que tienen contratos de un año, el 25% más alto paga más, y este gráfico de aquí, además, es filtrable también por cualquiera de estos filtros que tenemos aquí.

Entonces, a lo mejor yo digo: oye, solo me quiero fijar en los clientes que además paguen por fibra óptica, entonces hago clic aquí, lo veis, y automáticamente se ve muy bien cómo cambia. Ahora se ve que no, son los de dos años los que pagan más mes a mes. Sin embargo, si cambiaba a DSL, se vería cómo cambia otra vez, y los que no, pues cambia. Entonces, tener esa capacidad de tardar décimas de segundo en ver la hipótesis que tienes, creemos que es fundamental. Hay algo que a mí me obsesiona bastante, que es esta idea del flow, que desarrolló un psicólogo húngaro, que tiene un apellido bastante impronunciable, que viene a decir que hay un estado mental en el que entramos de flow, un estado mágico, en el que tenemos un reto por delante que todavía no sabemos resolver, pero no es tan grande el reto como para desmotivarnos y ser un poco abrumador, como dirían en inglés, ni tampoco es tan fácil el reto como para aburrirnos.

Si lo pensáis, los videojuegos son precisamente el tipo de cosas que se diseñan para mantenerte en ese estado de flow. O sea, tienes un reto por delante y estás ahí como enganchado porque crees que lo puedes superar, pero tampoco es tan fácil como para aburrirte. Entonces, yo creo que este concepto se puede llevar al software y hacer software que nos meta en ese estado de flow, que haga que te metas en un estado creativo, que yo creo que la gente que se dedica a hacer fotografía o edición de vídeo también puede llegar a lograr. Entran y se pueden tirar horas sin sentir hambre ni nada, si están muy enganchados. Y queremos que este software sea eso, pero para el mundo del análisis de datos.

Entonces, bueno, esta sección de plot, como decía, es así. Cuando vamos a algo interesante, podemos exportarlo. Podemos dar exportar, exportarlo en un fondo claro, oscuro e incluso personalizarlo mucho más. Pero también podemos guardarlo dentro de Graphics y guardarlo como un insight. Porque al final aquí lo que se trata es que encontremos insights y un insight al final es como una pieza de conocimiento que nos permite sacar conclusiones de algo, que es el fin último de esto de los datos. Entonces, imaginaos que yo, por ejemplo, aquí veo que los clientes mes a mes pagan de media 10 euros más que los de dos años. Eso puede ser un insight interesante. Entonces lo vamos a guardar y digo: "los clientes con contratos mensuales pagan de media 10 euros más al mes". Entonces le vamos a guardar Insights y automáticamente se crea esta diapositiva en esta sección de Insights como si fuera PowerPoint, y entonces podríamos aquí añadir más notas para que alguien de nuestro equipo venga aquí luego y pueda reproducir este insight tal cual.

Eso es una cosa clave de los datos: poder reproducir los resultados. A menudo se puede llegar a una conclusión equivocada porque se ha hecho una mala interpretación de las variables. Algo que estamos muy obsesionados también en Graphics, a diferencia de que un científico de datos te venga y escriba algo en Python y no se dé cuenta, pero lo estaba interpretando mal porque no tenía en cuenta que tal variable se medía de una manera o la otra. Es muy importante poder reproducir esos resultados rápido, tener acceso a cómo se llegó a esa conclusión. Para eso hemos hecho este botón de Play, en el que pinchas, y automáticamente, tachán, se ve ahí el resultado. Y queda muy, muy bien. O sea, tú ahora puedes llegar aquí y saber muy bien exactamente todo el estado de cómo se encontró esa cosa.

Hemos hablado mucho de exploración básica de datos, que es la parte más importante antes de construir un modelo predictivo. Pero ahora vamos a construir un modelo predictivo, porque queremos predecir qué clientes se van a ir, sobre todo los que todavía no se han ido. Hay una manera rápida de construir el modelo predictivo, que es seleccionando cuál es la variable que queremos predecir y luego las variables que están involucradas. Este es el resultado del modelo. En el resultado del modelo podemos ver que el 76% de las veces, 0,76, se ha clasificado bien si un cliente se iba a ir o no antes de que se fuera. Aquí podemos ver cuáles son las variables que han sido más predictivas al construir este modelo, como cuánto dinero en total se le había cobrado al cliente, cuántos meses llevaba siendo cliente, cuánto es el pago mensual, qué tipo de contrato, el método de pago y demás.

Esto está muy bien y, además, lo podemos poner en producción y conectar con nuestra base de datos y así automáticamente todos los días computar y ver un modelo o cualquier cosa. En este caso estamos prediciendo qué cliente se va a ir, pero podría ser alguien que rellena un formulario y queremos saber la probabilidad de que se convierta en un cliente por las preguntas que ha contestado ese formulario. O podría ser, por ejemplo, también cuánto dinero va a gastar este cliente en total en nuestra plataforma. O sea, se pueden hacer modelos predictivos de muchas cosas. Y los modelos predictivos, al final, son tener un target, una variable que quieres predecir y variables que quieres usar como factores. Pero lo que tiene de especial GraphX es que podemos entender estos modelos, podemos hacer modelos predictivos que se puedan explicar.

Lo que veis aquí en Graph, esto de aquí es un clustering de clientes, basándome en las variables que usé en el modelo. Entonces, cada uno de estos puntitos es uno de estos 7.000 clientes y está conectado a aquellos clientes que son más similares teniendo en cuenta estas variables. Si yo coloreo por estas variables, veremos que los clientes se distribuyen de múltiples formas a través de este clustering, esta reducción de dimensionalidad que se le llama. Creo que algo se toca en el máster sobre ese tema. Entonces, lo bonito que tiene GraphX es que automáticamente te crea estos clústeres que son grupos homogéneos de clientes, teniendo en cuenta todas estas variables. De tal manera que la gente que lleva menos de 10 meses, menos de 10 meses, saldrá de manera muy oscura, púrpura oscuro, saldrán por aquí. Y que además paguen mucho, por ejemplo, gente que lleva poco y paga mucho está como por aquí, en el clúster 13. O gente que, por ejemplo, tiene contrato mes a mes, pues los que pagan mucho y tal, están aquí.

Pero hay una manera más rápida de definir el clúster. Y sobre todo, lo que quiero es ver dónde están los clientes que se van a ir. Entonces, si coloreo por esa variable, la del churn, vemos en rojo los clientes que se han ido. Entonces, a ojo ya, viendo esto visualmente, podemos ver que los clientes que se van suelen estar concentrados sobre todo por esta zona de aquí. Por ejemplo, estos clústeres de aquí tienen muy poca prevalencia de gente en rojo. Entonces, si yo pincho aquí en el rojo, los clientes que se van a ir van a estar en este clúster. Y si yo pincho aquí en el rojo, los clientes que se van a ir lo vemos con mayor detalle. Y si esto lo ponemos aquí en relativo y lo ordenamos por esta variable de churn, yo puedo ver, por ejemplo, que este clúster 13, esta gente de aquí, tiene una prevalencia de irse altísima. O sea, en concreto, el 74% de los clientes que tienen estas características se acaban yendo, siendo normalmente el 26% de los clientes los que se van.

Podemos asumir que la combinación de variables de esta gente es muy predictiva de por qué se va un cliente. Entonces, podemos ver aquí qué definen estos clientes y lo podemos ver con mucha precisión. O sea, estos clientes, por ejemplo, suelen pagar entre 95 y 100 euros al mes, ¿vale? Y suelen llevar menos de 20 meses, ¿vale? Y además están pagando por streaming de películas, ¿no? Por películas en streaming. También pagan por fibra óptica y también pagan por streaming de TV, ¿vale? Y tienen, además, un contrato mes a mes, ¿vale? Entonces, esa combinación de factores para esta teleco es lo peor, ¿vale? Esto quiere decir que hay todavía muchos de estos clientes que no se han ido, ¿vale? Hay unos poquitos, un 1% de clientes que podríamos retener, que todavía no se han ido y tienen exactamente esas características.

Entonces, el dato del otro no lo tengo aquí, pero por lo que he visto, la clave de esto era precisamente que están pagando por streaming de películas y streaming de TV. O sea, están pagando por un servicio extra a la teleco que luego, en realidad, estos usuarios no estaban usando. O sea, esto es un buen ejemplo de que te da una pista de por qué esta gente que paga por estas cosas y paga esta cantidad se va. En este caso, esta teleco no estaba midiendo eso, luego lo midió y se dio cuenta, pero si tú haces que pagues mucho por unos servicios que no usas, es fácil que te llamen por teléfono y te digan: "Oiga, ¿quiere internet a 100 megas? Yo tengo 50, 50 euros en lugar de 100", y dicen: "Bueno, no tiene películas por internet streaming, pero no lo uso de todas maneras". Adelantarte al competidor que va a llamar a tu cliente y le va a ofrecer una oferta mejor, que es verdad que luego te va a pagar menos si tú le bajas lo que paga al mes, pero si le bajas lo que paga al mes y deja de irse, es mejor que se vaya, sobre todo teniendo en cuenta los costes de adquisición de los clientes que cada vez son más caros.

El otro ejemplo sería este que os voy a enseñar, datos de, en este caso, son tuits que escriben a ING. O sea, imaginaos que sois ING y queréis saber de qué se quejan vuestros clientes. Entonces, hemos capturado un montón de tweets en un último año que le han escrito a ING, bueno, en 2019 que le inscribieron a ING. Y entonces, automáticamente, este clustering no está basado en la similaridad a los tweets por otro, o sea, está basado simplemente en la similaridad de los tweets por el texto. Entonces, si hacemos zoom por aquí, por ejemplo, veréis que, si os fijáis, aquí hay un poquito de texto. Todas estas cosas son gente diciendo cosas del estilo: "Probando en Firefox y Safari, borrando el historial y caché, dirección escrita a mano y sigo sin poder acceder. No es un problema puntual, pesa muy a menudo". Y veis que este otro tuit cerca viene a decir como: "He probado en dos navegadores, o he probado en dos navegadores limpiando cookies y tampoco funciona, brillante lanzamiento de no sé qué". Se ve muy bien que es gente diciendo que tras hacer algo que probablemente le hayan dado en soporte, como limpiar las cookies por un navegador, siguen sin poder acceder a la web.

Entonces, todos esos tweets similares conforman un clúster que emerge en verde de una temática, que básicamente es gente que tiene problemas de acceso. Si nos vamos aquí, veréis que aquí hay gente que está quejándose de Apple Pay, "Apple Pay, ¿para cuándo?". Es gente que está pidiendo Apple Pay. "¿Y mi Apple Pay, para cuándo?". Fijaos que aquí lo predicen. "¿Cómo va Apple Pay? ¿Os acordáis de ellos?". Fijaos cómo estos modelos del lenguaje, que están basados en cosas que habéis visto en el máster, como BERT y otros modelos del lenguaje que usan transformers por debajo, son capaces de detectar que dos tweets son de lo mismo, aunque no repitan ni una sola palabra, porque entienden la semántica muy bien.

Entonces, fijaos cómo este clúster rosa emerge, que lo tengo aquí, y además nos dice que está muy correlacionado con otra variable, que está muy correlacionado con la fecha. Entonces, podéis ver que casi todos estos tweets de la gente pidiendo Apple Pay fueron muy fuertes hasta marzo de 2019. ¿Aguantar en qué fecha? La posibilidad de estar en Apple Pay. Se ve fenomenal. Entonces, en este caso, esto es de un cliente, pero podríamos hacerlo de cualquier empresa, porque son datos públicos que están ahí, o de cualquier temática. O sea, podríamos entender muy bien grandes mercados de gente, un montón de, en este caso, tweets, de información que está libre y disponible, de qué va, y entender ahí patrones muy interesantes que están sueltos.

Además de esto, hay cantidad de otro tipo de datos. Por ejemplo, esto de aquí es, a partir de un millón de órdenes de compra en Mercadona, sacar qué productos habitualmente van más juntos en la cesta de la compra. Pues yo qué sé, fijaos que, por ejemplo, si pincho aquí en el hummus, se ve muy bien que la gente suele comprar el hummus con regañás, con aperitivos de triángulo de maíz, con picos artesanos, o sea, muy bien, ¿qué hace falta para untar ese hummus? Pero también, bueno, es curioso, con garbanzos cocidos, esto es gente que ya es muy fan de los garbanzos, con pan de molde. Incluso podemos ver aquí que hay otro clúster, que, fijaos, si busco aquí, por ejemplo, merluza, podríamos ver que los que compran merluza congelada la suelen comprar con calabacín verde, endivia y otros tipos de comida sana.

Y esto a lo largo podríamos correlacionarlo con otras variables sociodemográficas que tuviéramos de los clientes, como el código postal en el que viven, saber la renta per cápita de ese código postal. Y con todo eso adquiriríamos una inteligencia que verdaderamente nos inspiraría un montón para decir dónde abrir una tienda, qué productos poner juntos en la web, o físicamente si es un local, etc. Y por último, dejaros esta visualización que hice el otro día, con todas las transacciones de Ethereum o de cualquier blockchain son públicas, es una de las propiedades que tiene el mundo cripto, que tenemos esta pseudo-anonimicidad. Entonces aquí me he sacado todas las transacciones en un día de Cilium, ¿vale? Y entonces podemos ver todas estas direcciones cómo están conectadas, ¿vale? A través de cuántas transacciones hay entre ellas.

Entonces, sobre esto aplicamos estos algoritmos de clustering, que habréis visto un poco en el curso. Y entonces podéis ver cómo emergen determinadas comunidades donde transita mucho dinero entre ellos. Por ejemplo, veis este clúster 2. Claro, aquí habría que ir y sacar y revelar estas direcciones con las que se corresponden. Es muy probable que muchos de estos nodos tan grandes, que tienen tantas transacciones, se correspondan con exchanges. Podría ser Bit2Me, podría ser Coinbase. Entonces, claro, aquí sería muy interesante, esto es un ejercicio por hacer, de ver más sitios, más sitios disponibles donde hay más información asociada a ciertas direcciones públicas de blockchain. Entonces podríamos entender ciertos patrones. Incluso si cruzáramos series temporales de datos, de crecimiento y bajada del valor de algo con el movimiento y la formación de estos clústeres, a lo mejor podríamos anticipar algo. Probablemente habrá direcciones de cripto que hagan transacciones mucho antes de que se desencadene un efecto.

Creo que esto es un buen repaso de lo que se puede hacer con una herramienta como GraphX. Como veis, no hemos escrito una sola línea de código y hemos hecho cosas impensables de hacer en una hoja de Excel, y que incluso escribiendo código nos llevaría semanas, a veces incluso meses. Creo mucho en este tipo de herramientas, creo que es el futuro de la ciencia de datos. Y nada, tenemos cuentas gratis para usuarios y para estudiantes, así que si vais a nuestra página web podéis verlo en graphx.com y aplicar a esas cuentas de estudiantes.

## Dtalks
### Inteligencia Artificial & Crypto (Video)
![[511.E1_Inteligencia_Artificial_&_Crypto.mp4]]
[Inteligencia Artificial & Crypto](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41911100-dtalk-inteligencia-artificial-crypto)

Speaker 1
Las tres personas que nos encontramos en este escenario tenemos bastantes cosas en común. La principal es alrededor del concepto del dato, el machine learning y la inteligencia artificial. Los tres hemos cursado estudios de machine learning, inteligencia artificial, hemos sido docentes, nos hemos dedicado a esta área. Y por otro lado, ahora nos dedicamos al mundo de la descentralización, de blockchain y las criptomonedas. Es por ello que voy a lanzaros Mark y Pablo. Mi primera pregunta, desde el punto de vista del dato, llevamos mucho tiempo viendo crecer una gran cantidad, una tribu de influencers que se dedican a analizar gráficas de proyectos cripto, básicamente, sobre todo lo que llaman gráficas profesionales, se dedican a dibujar sobre ellas, a intentar buscar patrones que se van a repetir en un cierto tiempo, y son los que todavía denominan más. pro o traders. Me gustaría preguntaros desde el punto de vista del dato, ¿qué opináis de todas estas técnicas?

Speaker 2
Bueno, yo creo que la inteligencia artificial en general, tanto del machine learning como el deep learning, lo que nos ayudan es a poder discernir en qué medida ese tipo de narrativas son más de opinión, más subjetivas o más objetivas. Muchas veces vemos a personas de la industria. dando su opinión muy libremente y muchas veces te planteas si eso está realmente basado en datos. Gracias a las técnicas de inteligencia artificial lo que podemos hacer es verificar ese tipo de opiniones y sobre todo ser conscientes de cuánto podemos hacer gracias a los datos. Es por ello que creo que la importancia en este mundo de capturar y tratar bien los datos es cada vez más importante para poder saber discernir entre subjetividades. y objetividad.

Speaker 3
Marc. Estoy de acuerdo en una cosa, la inteligencia artificial nos puede ayudar a validar todos esos conceptos, lo que sí que a veces se encuentra un poquito en este tipo de gráficos, en este tipo de personas que están comentando esos gráficos, es como un poco falta de método matemático, método estadístico, que realmente valide que lo que están diciendo esos datos es algo que realmente tiene valor. A veces vemos predicciones que se crean ellos mismos sin ningún tipo de intervalo de confianza, simplemente porque ven una tendencia que se repetía hace cuatro años y sin afirmar, sin tener mucha idea exactamente de lo que están diciendo, dicen vale, pues ese cara va a ser un mercado alcista. Pero en verdad no, la muestra no es representativa, los datos no son característicos y sí que es verdad que en la introducción de nuestro, digamos, el análisis de datos, podemos hacer este método como un poco más. específico y un poco más certero y puede contribuir muy profesionalmente a la mejoría en estas predicciones, por ejemplo.

Speaker 1
Estoy de acuerdo con vosotros, yo la sensación que tengo es que simplemente cogen las gráficas y buscan patrones que se van a repetir con otro momentum. A partir de ahí sí que es verdad que casi todas las métricas están apoyadas en la media móvil y las bandas digitales. de Bollinger. Al final son derivados de la media móvil, que es cierto que puede tener una correlación a corto, pero, ¿qué opináis? ¿Creéis que se puede predecir el cambio de momentum? Eso por un lado, eso será la segunda pregunta, pero la primera sería, estoy en lo cierto, aunque simplemente desde el punto de vista técnico hay poco aporte de valor, porque parece que lo único que buscan es repetir patrones y casi lanzar una moneda al azar. ¿Creéis que es así como operan ellos? Esa es la primera pregunta. Y la segunda pregunta, si creéis que se puede predecir el momentum, el cambio con la IA.

Speaker 2
Bueno, nos estamos acostumbrando a ver vídeos de influencers y traders que lo que hacen es pintar patrones muy claros y hacer, vale, pues ahora va a subir porque ha bajado dos veces previamente. En el fondo, el machine learning es lo mismo, pero a gran escala. Es decir, esta gente está validando esa tendencia con muy poquita. los datos. Ve una curva de la semana pasada y cree que es capaz de predecir la que viene. O la realidad es que el Machine Learning nos enseña que no, que necesitas tener muchísimos datos, muchísimos tipos de variables, no solo cuantitativas sino también sociales. Y esa es la clave, ¿no? Poder tener datos. un dataset lo suficientemente grande como para validar si esos patrones son efectivamente reproducibles y si hay una tendencia veraz al futuro. Como decías, las medidas móviles, que las vemos muy comúnmente, tampoco nos dan tanta información, lo que estamos viendo es el pasado, no el futuro. Creo que una parte sí se puede llegar a predecir, puedes capturar tendencias, puedes ver un poco más allá del momento actual, pero este mundo es tan complejo, está tan interconectado con muchísimos tipos de variables, macroeconómicas, sociales, que predecir... a ciencia cierta es muy complicado, no sé cómo lo veis vosotros.

Speaker 3
Efectivamente, como ha comentado mi compañero Pablo, al final no tienes suficientemente datos, la muestra no es del tamaño suficiente, se guían un poco con intuiciones, pero ya digo, no hay suficientemente intervalos de confianza, no hay un patrón que podamos afirmar que funciona, ya ni matin learning, ni algo así. algo más avanzado como Deep Learning, pueden actuar aquí para tener un peso específico. Así que es verdad que hoy en día van saliendo más, por ejemplo, no solo basarnos en series temporales, que podría ser el histórico de datos, sino que ya vamos captando otras tendencias como... análisis de herramientas de inteligencia social, como el análisis de las redes sociales, el crecimiento en las tendencias de búsqueda, y todo eso sí que pueden acercarse un poquito a predecir el ESE Momentum. pero estamos en un punto que ahora mismo tú no puedes predecir si va a salir un gurú y va a decir que cierta criptomoneda es un fiasco y va a desplomar su valor, o todo lo contrario.

Speaker 1
Claro, aquí hay un tema con las criptos en general, que son de reciente aparición, y esto en Machine Learning, y sobre todo en Deep Learning, que es un problema, porque nos lleva a que un año tiene 365 días, con lo cual tenemos muy pocas filas para trabajar modelos de Deep Learning. Deep Learning se quedarían prácticamente fuera. Tenemos la técnica de Data Augmentation. ¿La utilizaríais en lugar de utilizar la apertura y el cierre, jugar con puntos intermedios para multiplicar por 4 o creéis que sería ruido? Venga, al algoritmo.

Speaker 3
Pregunta complicada, Baji. De punto de vista, meter de Taumentation para una serie temporal es algo bastante arriesgado, y más en un mercado como este, donde está demostrado que tampoco hay mucha estacionalidad, es decir, si la moneda está baja en marzo de 2020, no tiene por qué estar baja en marzo de 2021. Por tanto, es un poco arriesgado, aun así,

Speaker 1
podemos probarlo. No me refiero a interpolar valores, coger los propios que tenemos entre la apertura y el cierre para poder tener más puntos de control.

Speaker 2
Yo creo que aquí el caldo, como hemos explicado en las clases, es realmente ser capaz de entender los datos, de tener los ingredientes necesarios para entender las tendencias, para entender los patrones y, sobre todo, los comportamientos sociales, que es uno de los pilares fundamentales de la variación de precios y mercados. Cuanto más tengas, mejor, ese es el punto. es un principio fundamental de la inteligencia artificial. Pero sí que es verdad que si trabajas con un dataset que no es lo suficientemente completo, que lo aumentes, que mejores la calidad de los datos, no va a ser suficiente para poder predecir algo tan complejo como esto. Seguramente si te ayude mucho, seguramente tu modelo mejore mucho en precisión, pero también puedes correr riesgos como overfitting, en función del rango de fechas que cojas, como dices, un año no es demasiado. Yo creo que la salsa es eso. entender bien los datos, entender bien los proyectos y cuál es el impacto de un influencer que pueda decir algo en algún momento, de la comunidad en general.

Speaker 1
Nosotros defendemos mucho el análisis de datos objetivos en tiempo real, porque esto es objetividad, no es opinión, este es el planteamiento que a nivel educativo hemos pretendido implantar. Y estuvimos haciendo experimentos tiempos atrás y veíamos que entre hoy en día... 80, 90 variables, la que mayor correlación tenía con el precio de Bitcoin era Google Trends. ¿Qué puerta nos abre el conocer que tenemos una tal relación con Google Trends? ¿Cómo lo podíamos trasladar esto a un producto o un beneficio para el estudiante o el usuario? ¿Qué veis ahí? ¿Qué visión veis?

Speaker 3
Bueno, por ejemplo, Google Trends en el mundo del data science ha servido para otros casos de uso. Por poner un ejemplo, gracias a Google empezaban a ver qué países eran más propensos sean los siguientes a estar más afectados por el COVID solamente por el número de buscas que realizaban sus usuarios. Entonces, eso es un buen indicador. A veces el mejor algoritmo que funcionaba en estos casos era el de Google y solamente por patrones de búsqueda. Por tanto, yo creo que esa es una variable muy a tener en cuenta. Ahora, ¿cómo? ¿Si positiva o negativamente? Bueno, más que una variable o más que una variable. clara que nos diga, pues va a subir o bajar, yo creo que es un indicador a tener en cuenta. Y en el momento que podemos ver que, por ejemplo, aumentan las búsquedas de Google, es como estar alerta y empezar a expandir por todos los canales, otras redes sociales, subforos, y ver cómo se está, qué se está cogiendo ahí. No solo el número de búsquedas, sino sobre, por ejemplo, extraer los tópicos y ver de qué se está hablando. Eso puede ser, puede ayudar.

Speaker 2
En mi opinión, creo que es bastante justo decir que, Venimos de una tendencia del mundo financiero a analizar mercados en función de métricas cuantitativas. Wall Street nos enseña eso todos los días, que lo único que estamos mirando en los gráficos son las tendencias alcistas o bajistas en función de métricas económicas. estamos empezando a descubrir el potencial que tiene estudiar la sociedad desde un punto de vista de inteligencia artificial. Entonces, creo que hay mucho por descubrir en ese campo, hay un horizonte muy divertido y motivador en cuanto a captura de datos sociales y yo creo que las posibilidades de análisis en el mundo cripto gracias a este nuevo paradigma van a ayudar muchísimo a comprender realmente nuestra sociedad y el mundo en el que vivimos.

Speaker 3
Muy de acuerdo con él, porque al final esto es lo que te abre a otros campos, a llegar a conocer el comportamiento social, a entenderlo, a llegar a predecirlo, a buscar esos nuevos patrones, y creo que las oportunidades que abre en este campo son gigantescas.

Speaker 1
Os voy a proponer un reto a los dos, que es el reto de la semana. Si tuvieses que desarrollar un algoritmo predictivo de cualquier proyecto cripto, vamos a pensar en Bitcoin o en Ethereum, quizás es más fácil pensar en Bitcoin, sabemos que la dificultad es tremenda. Pero, ¿qué 3, 4 variables serían indispensables para ese modelo? Vamos a intentar sacar 5 entre los 3.

Speaker 3
Voy a hacerte un matiz. O sea, ¿un algoritmo predicción del precio o clasificador para ver si sube o baja el precio? Porque ahí la dificultad es diferente.

Speaker 1
Bueno, para ambos, ¿no? Al final es dar un tipo de salida, ¿no? Al final sería bien un clasificador, que sería para ver si sube o baja un regresor. para predecir el precio, pero al final, trabajaríamos con variables similares. ¿Qué variables creéis que pueden afectar este cambio de momentum? Que al final, tenemos claro que hay modelos de machine learning que ha corto, funcionan muy bien, tienen muy buena performance, pero estamos dando cambio en momentum. 5 variables entre los 2 o entre los 3, va.

Speaker 2
Bueno, yo tengo 2 muy claras. A colación de lo que hablábamos de... entender bien la sociedad, sería el nivel de satisfacción en la comunidad. Creo que es una pregunta que no nos hacemos lo suficiente en este mundo, que es, ¿realmente estamos aportando valor a nuestros usuarios?, ¿están percibiendo ese valor? Es difícil capturar la satisfacción de alguien en un proyecto que aún está, por así decirlo, en pañales, pero creo que es muy importante recibir ese feedback y estar en contacto con nuestra comunidad, por tanto, uno sería la satisfacción.

Speaker 1
Claro, estamos hablando de... de variables que añadirías en tu modelo de Machine Learning para hacer la predicción del Bitcoin. Sí, tenía otro y se me ha ido.

Speaker 3
Eso tiene un problema, que a veces la opinión de la gente, sobre todo en el mundo cripto, está muy influenciada por los medios de comunicación. Y pueden estar a hablar bien hoy, que por ejemplo el otro día me llamó mi madre, que solo había escuchado noticias que el Bitcoin iba fatal, que era una engaña. Entonces, eso te das cuenta de que los medios de comunicación pueden. que esa opinión general o satisfacción como una comunidad esté muy abajo o muy arriba. Por tanto, sí que estudiaría eso por un lado, pero sería como un identificador de cuál es el nivel de positividad o negatividad de la sociedad en general en las criptos, pero eso sería como una variable que estaría... relacionada de muchas otras variables, redes sociales, foros, correos, etc.

Speaker 1
Hay otro factor que puede influir mucho, que son los movimientos de las ballenas, que tienen una correlación muy elevada con el precio, ¿cómo podríamos intentar traquear estos movimientos de ballenas?, porque al final blockchain es una red que es trazable, de alguna forma.

Speaker 3
Sí, blockchain, lo bueno que tiene es que, aunque te garantizan anonimada, puedes hacer un montón de cosas con ella. Y yo no sé quién es esa ballena, sí que puedo llegar a saber, dependiendo del tipo de wallet, un movimiento de 20 millones de dólares en bitcoins. Y si ese movimiento se mueve a algún sitio, significa que habrá algún cambio gordo. Desde luego yo creo que eso es una variable, una analítica, un change, si no me equivoco, a analizar bastante importante.

Speaker 2
De hecho, es lo que buscamos un poco con Sandbox, ¿verdad? Hasta ahora el análisis era de la capa superficial del mercado, ahora tenemos la posibilidad de estudiar las entrañas y de ver en tiempo real qué está pasando, cuáles son esos movimientos, quién lo está moviendo y qué tipo de movimiento es. Por ahí también hay un horizonte por explorar, que es lo que estamos intentando en el máster y que seguro que los estudiantes van a aprovecharlo muchísimo.

Speaker 1
Sí, también sobre todo el traqueo de los potenciales cisnes negros, las variables relacionadas con la geopolítica porque al final la geopolítica tiene un impacto sobre los mercados financieros que a veces tiene una correlación directa o inversa con las criptomonedas pero ambas son buenas realmente

Speaker 3
Y, bueno, simplemente, esto no es otra variable, pero poner un apunte, que es algo que también me he dado cuenta, es cuando estamos analizando, por ejemplo, la opinión de la gente, o sea, es muy importante, como he dicho, que sea casi en tiempo real, porque si pasa algo malo, como el otro día pasó la catástrofe de Terra y Luna, claro, todo el volumen de noticias fue a posteriori, pero lo complicado aquí es predecir, o sea, estar cazando a las tendencias de que algo se está moviendo, algo va a pasar, en antelación y para eso es importante implementar modelos de tiempo real para este tipo de...

Speaker 1
Al inicio de este debate hablábamos de que tenemos bastantes cosas en común, yo creo que otra es que los tres nos apasiona la tecnología, es decir, en este caso podemos hablar de dato pero podemos hablar de descentralización, podemos hablar de blockchain, proyectos excitantes en este espacio o metaverso, NFTs, ¿en qué proyectos estáis apostando o apostaríais vosotros? ¿Qué proyectos os apasiona la tecnología? ¿Os llaman la atención que dices, me encanta este para mi wallet?

Speaker 2
Personalmente, la sostenibilidad es muy importante, creo que es un gran problema de esta industria que empezamos a concebir el sistema obviando un poco la parte del consumo. Nuestra sociedad... nuestro mundo requiere de tecnologías limpias, requiere de una reconstrucción tecnológica y personalmente creo que los proyectos que desafíen un poco las tecnologías pasadas y apuesten por nuevas tecnologías que luego puedan extrapolar esas innovaciones a otras industrias creo que son los que tienen mayor cabida en el mercado y seguro un futuro prometedor.

Speaker 3
Vale, me voy a mejorar un poco más, te voy a decir por ejemplo Kaka cardano, porque me parece ahora mismo un proyecto que, aunque parece que nunca acaba de despegarse y veas un poco su historia, desde el principio hacen las cosas despacio, de forma concienciuda, de forma bien en las plataformas, los ecosistemas, las layers, digamos, con un poco más segura, no han tenido ningún problema de hackeo, no han tenido ningún problema así grande que se generalice. Y si la ves crecer, yo creo que para mí es un proyecto, ya no te hablo solo de la moneda, sino todo el ecosistema detrás. que va a pegar fuerte en un futuro.

Speaker 1
Sabemos también que la inteligencia artificial y el trabajar con datos permite detectar anomalías de una forma muy sencilla, es una de las virtudes que tiene. ¿Le veis aplicación de la AI al mundo de la ciberseguridad?

Speaker 2
Bueno, en el mundo bancario se usa muchísimo en términos de predicción de fraude, si algún cliente es propenso a... tomar medidas fraudulentas. En el mundo cripto, a pesar de la concepción de que es un mundo con mucho obscurantismo y mucho anonimato, en el fondo es muy transparente y puedes detectar y trazar perfectamente las transacciones y los movimientos de los usuarios. Claro, la inteligencia artificial aquí abre un campo inmenso en tanto en cuanto podemos crear datasets que nos indiquen si una transacción proviene de orígenes fraudulentos o si son transacciones falsas. que en el fondo vienen del blanqueo de capitales, por ejemplo. Este campo en el mundo blockchain no está muy desarrollado hoy en día, pero creo que en los próximos años vamos a ver un salto inmenso en todo este tipo de técnicas para ofrecer seguridad y confianza a la comunidad.

Speaker 1
Has trabajado, Marc, con datasets de detección de fraude, que normalmente hay que comentar que son desbalanceados porque la proporción es muy escasa. del fraude respecto a las operaciones correctas, a lo mejor puede ser un 1% o...

Speaker 3
Menos mal que es desbalanceado porque si no estaríamos ante un problema.

Speaker 1
¿Y cómo lo resuelves, este desbalanceo, para poder entrenar bien tu modelo?

Speaker 3
A ver, antes de eso, sí que es verdad que la ciberseguridad, la inteligencia artificial puede ayudar, pero también sigue estando el problema del anonimato. Aunque tú puedas seguir esa transacción y detectarse fraudulenta o no, encontrar a la persona es un poco más difícil. Y me estás preguntando, perdón, sobre cómo trataría el desbalanceo de datos. Hay diversas técnicas. Lo ideal en este mundo sería conseguir un dataset balanceado por igual, pero como eso no es posible, pues por ejemplo se puede hacer un data augmentation, del que hablábamos antes, un data reduction o un undersampling, que es la clase mayor... reducirla para que quede un porcentaje más equitativo, como puede ser un 70-30 o alguna cosa así, que con eso ya se puede trabajar, y si no, por ejemplo, ya tenemos algoritmos, XGBoost, DoAlaBoost, CatBoost, otras variantes que ya te permiten, a la hora de hiperparametrizar, conseguir muy buenos resultados para tratar ese tipo de desbalanceos.

Speaker 2
Claro, el problema aquí es la capacidad de cómputo, esos datasets, serán enormes, vamos a necesitar máquinas que sean cada vez más potentes, pero por suerte tenemos servicios que podemos contratar directamente de proveedores como Amazon, Microsoft, donde cualquiera de nosotros puede alquilar una máquina y crear ahí nuestros propios modelos. Ya no estamos sujetos a estructuras de gran coste de empresas muy grandes para poder nosotros aportar valor y sacar insights que son muy importantes. pues eso, aporta muchísimo valor a la sociedad.

Speaker 3
Y no te olvides tampoco de la computación cuántica, que ahora está, digamos, como en pañales, aunque es verdad que ya hay servicios como AWS que te ofrecen sus servicios, y es para algunas características muy determinadas, pero eso va a ser el futuro y también va a ayudar mucho en el campo de la inteligencia artificial, seguridad, tema blockchain, por ciberseguridad, etc., va a ser muy importante en el futuro.

Speaker 1
Yo recuerdo cuando conocí a Marc y comenzamos a estudiar en este ámbito, teníamos muchísimas dudas, porque la inteligencia artificial estaba casi en pañales, incluso para saber qué framework escoger, porque no había un consenso tan claro como ahora. Al final, un poco Python se desimpuso, otras tecnologías, y fue la apuesta que es la que más comunidad tiene en el mundo. Ahora nos encontramos en otro escenario que es muy excitante, respecto a nuevas tecnologías, nuevos frameworks, nuevos lenguajes... ¿qué os apetece estudiar cuando tengáis dos semanas libres que podáis friquear un poquito?

Speaker 2
A mí el mundo del arte me apasiona y estoy viendo nuevas propuestas, con sistemas descentralizados, inteligencia artificial brutales. Hay un artista, por ejemplo, Refika Nadol, es una artista que estudia la mente humana, genera datasets en función de las mentes, por ejemplo, estudia gente con Alzheimer, pone electrodos y construye un dataset con esos impulsos eléctricos. A raíz de ese dataset, él crea imágenes autogenerativas con las que podemos visualizar la enfermedad. Es la primera vez que podemos ponerle cara a una enfermedad y entender un poco esas conexiones, aunque sea solo de forma artística y de una forma que busca la emoción, es un primer paso hacia un potencial médico biotecnológico inmenso. A mí, la verdad, es que ese mundo me apasionaría poder meterme un poco más en profundidad.

Speaker 3
Interesante, no conocía el dato, pero, perdona, respecto a eso, acuerdo ahora un año, año y medio, asistí a una conferencia de... Oscar, no me sale ahora mismo el nombre, pero es el que hizo la película, el compositor de la película de Klaus, que es una película de Disney. Y él lo que utilizaba era, a través del deep learning y la programación, natural language processing, captaba, o sea, ponía imágenes de la película de los usuarios y captaba la emoción que sentía. Y en base a eso, la red neuronal creaba la banda sonora y lo que buscaba esa banda sonora era intensificar esa misma emoción. Entonces, eso se puede utilizar tanto para algo causar terror, como también, imagínate, emociones como tranquilidad o paz, o alguna cosa así, que podrían ayudar también a estos campos de... las personas.

Speaker 1
¿Y hay algún tipo de framework en el ámbito descentralizado blockchain que os apetezca descubrir o aprender o indagar?

Speaker 3
Bueno, si ya entro en mi parte un poco más friki, a mí personalmente sí que me gusta por ejemplo web3, que no se sabe muy bien el origen pero viene web3.js que es una serie de nodos creados para javascript y sobre todo me apetece probar a programación en Solidity, estoy seguro de otros lenguajes que van a saber, porque ahora mismo donde hay una falta de programadores, y ya no solo programadores, de todo el mundo relacionado con arquitectura, es en todo el tema del blockchain. Yo sé que eso es un campo que va a tener muchísimo futuro y ya aparte de eso me parece un campo de estudio muy profundo.

Speaker 1
Se han realizado muchos estudios que... de la inteligencia artificial se llaman papers, donde se muestran resultados increíbles respecto a la predicción de los precios en criptomonedas. Pero la realidad dista bastante de este optimismo. Sabemos que en ventanas temporales de 1 a 3 días tenemos resultados muy interesantes, pero a medida que nos acercamos de esa ventana temporal, hasta los 10 días comienza a perder, a caer estrepitosamente. Se están haciendo muchísimas pruebas con modelos de reinforcement learning, que está abriendo también otra área de estudio, pero desde luego sabemos que la descentralización y la inteligencia artificial han nacido para converger. Muchas gracias.