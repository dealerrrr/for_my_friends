---
title: Redes P2P | Alberto Martín
URL: https://app.web3mba.io/courses/take/bloque-4-tecnologias-descentralizadas/lessons/39251447-1-1-redes-p2p-alberto-martin
Tags/Keywords: #Bloque 4 #tecnologias descentralizadas #B4U1 #redes p2p #Alberto Martín
lang: es-AR
---
# Redes P2P
![[194.B4_Redes_P2P.mp4]]
[Redes P2P](https://app.web3mba.io?wvideo=fzixx1kbn5)

Las redes P2P surgen de la necesidad de conectar ordenadores de forma punto a punto, es decir, de manera directa, sin un servidor intermediario que medie en la comunicación. Este concepto comenzó a desarrollarse en los años 80, cuando, dentro de los protocolos UNIX y GNU, se buscaba la posibilidad de transferir archivos sin depender de un servidor central que pudiera almacenar una copia de esos archivos. Actualmente, quienes utilizan distribuciones GNU, MacOS y similares, suelen emplear herramientas como SSH o SCP, que son métodos para conectarse a un servidor remoto. El origen de todo esto radica en la implementación de las primeras redes P2P, que fueron evolucionando con diferentes características y necesidades a lo largo del tiempo, dando lugar a lo que conocemos hoy. Ejemplos de redes P2P populares en la actualidad son BitTorrent y, a nivel técnico, el protocolo SCP mencionado anteriormente, que en esencia permite realizar una copia de datos de un punto A a un punto B.

Para entender cómo funcionan estas redes, es necesario analizar su funcionamiento desde un enfoque técnico. Si observamos cómo se transmite un mensaje a través de HTTP en Internet, notamos que existen diferentes niveles de jerarquía en los paquetes. En el protocolo TCP/IP, hay siete niveles. En las redes P2P, trabajamos en el nivel 7, que es el de aplicación, donde intentamos crear una red de nodos que tengan direcciones de sus vecinos, lo que nos permite construir una mini Internet donde todos pueden acceder a los datos de los demás. Las principales ventajas de una red P2P son varias. La primera es que no requiere un único punto de fallo, algo que siempre está presente en una red centralizada. Al eliminar el problema del "single point of failure", se abren nuevas posibilidades, como la descarga de archivos de manera distribuida. No es necesario tener una única representación del archivo, sino que puede estar distribuido y ser descargado desde múltiples fuentes simultáneamente. Además, podemos compartir información no solo con un servidor, sino de manera fragmentada y simultánea en muchos lugares.

En España, las primeras implementaciones de redes P2P fueron programas como Kazaa, eMule y Napster, que buscaban redistribuir contenido que tradicionalmente estaba protegido por derechos de autor, dándole una nueva vida a través de la comunicación entre pares. En esa época, se observaron no solo las ventajas, sino también algunas desventajas de estas redes. Al ser distribuidas, es fundamental cuidar la calidad del contenido que circula, ya que un archivo corrupto puede ser distribuido y es responsabilidad de los miembros de la red identificarlo y eliminarlo. Para que esto funcione, se necesita un software que cada cliente pueda descargar y un conjunto de nodos que ejecuten dicho software. Una vez que cada cliente participa, comienza el proceso de conocer a sus vecinos. En esencia, se necesita una red de vecinos, donde uno pueda pedir sal, otro pimienta, y a su vez, ofrecer leche y pan. Para lograr esto, es necesario tener tablas de paginación que contengan índices para descubrir qué tiene cada uno. Esto funciona bien a pequeña escala, pero se complica a mayor escala.

Una de las primeras redes basadas en texto fue el IRC, un caso curioso de redistribución P2P, ya que no comenzó como tal. Inicialmente, había un directorio central que almacenaba el índice de vecinos con sus direcciones, lo que permitía iniciar conversaciones. Sin embargo, llegó un momento en que los usuarios no querían que su comunicación pasara por ese índice central y deseaban un entorno donde los miembros de la comunidad tuvieran el control. Así, se pasó de tener una tabla de índices central a múltiples tablas de índices distribuidas. Ya hemos mencionado algunas ventajas de estos sistemas desde un punto de vista filosófico y de uso, pero también es importante considerar sus ventajas y desventajas a nivel técnico.

Entre las ventajas, destaca que una red P2P no puede ser censurada. En países donde los ISPs están controlados por el gobierno, como en China, hay restricciones para acceder a ciertas páginas web. Al distribuir el contenido de manera P2P, no hay un servidor DNS que pueda denegar el acceso a una ruta específica, ya que es una red de nodos la que proporciona ese contenido. Si hablamos de escalabilidad para compartir archivos de video o audio, como en los casos iniciales de BitTorrent, hemos visto una evolución significativa. Al principio, las descargas eran bloqueadas y se realizaban entre pares, pero ahora podemos tener particiones distribuidas que almacenan fragmentos de diferentes proveedores, lo que permite un acceso más rápido a nuestros archivos en comparación con la descarga desde un servidor centralizado.

Otra ventaja es que el sistema es agnóstico respecto al tipo de archivo que se desea compartir. En la misma red y ecosistema, se pueden enviar desde un tweet hasta descargar una canción o una base de datos de wallets comprometidos. La clave es que solo se necesita establecer una infraestructura y, a partir de ahí, cualquier tipo de información puede fluir libremente. Sin embargo, al considerar las desventajas del sistema, es importante analizar qué sucede con una red de gran escala. El rendimiento de esta red dependerá del tipo de algoritmo utilizado para la distribución de nodos. Según el tipo de archivo que se desee distribuir, algunos tendrán mayor calidad que otros, y la replicación de la información será más lenta en función del número de nodos que se mantengan. Por ello, ciertas redes distribuidas o P2P pueden no ser operativas para determinados casos de uso.

Aunque una red distribuida es prácticamente incensurable, no es un protocolo que, por defecto, garantice privacidad, a menos que esté diseñado específicamente para ello. Por ejemplo, en Alemania, el tráfico de BitTorrent es monitoreado por los ISPs, y se pueden imponer multas por compartir contenido protegido por derechos de autor. Este tipo de tráfico puede estar acompañado de medidas secundarias que aseguren la privacidad, como utilizar una red P2P sobre Tor o a través de una VPN con características específicas. Es fundamental tener en cuenta que la red distribuida ofrece libertad de distribución, pero no garantiza privacidad sobre lo que se comparte.

Por último, es importante mencionar los problemas estructurales que pueden surgir. Todo tipo de red y ecosistema tiene sus pros y sus contras, y en las redes distribuidas P2P, existen casos de uso que pueden ser vulnerables a ataques, como el ataque de eclipse. En un ataque de este tipo, se intenta aislar un nodo, creando un "eclipse" sobre él, lo que puede tener diversas finalidades, como reducir su capacidad de minería o inyectar transacciones fraudulentas. Esto se logra mediante la creación de nodos replicantes que apuntan a vecinos cercanos, y es crucial que las tecnologías en entornos cripto estén diseñadas para ser resilientes a este tipo de ataques.

¿Cómo se relaciona todo esto con las criptomonedas? Todo comenzó con la primera blockchain, Bitcoin. Cuando Satoshi diseñó el sistema, se inspiró en cómo funcionaba BitTorrent. En aquella época, BitTorrent ya utilizaba algoritmos para distribuir la carga entre diferentes nodos, y se adoptó un enfoque similar en Bitcoin. ¿Cómo se traduce esto en la minería? La minería es el proceso de encontrar la solución a un bloque y añadirlo a la cadena. El bloque que se añade se retransmite a todos los nodos. Si tenemos un número fijo de nodos y mineros, el sistema debe ser lo suficientemente robusto para evitar que ningún agente activo que valide transacciones pueda introducir una transacción maliciosa. Esto se relaciona con la regla del 51%, que establece que si más de la mitad de los nodos están de acuerdo en algo, ese algo se considera cierto. Para introducir una transacción maliciosa, sería necesario engañar a esa mayoría en un corto período de tiempo.

¿Qué sucede en un escenario específico? Si tenemos, por ejemplo, 20 mineros y 100 nodos, y 10 de esos mineros controlan el 80% de los nodos, lo que a escala actual es prácticamente imposible, esos mineros podrían intentar inyectar una transacción maliciosa. Si logran que la mayoría de los nodos acepten esa transacción, se generaría una situación en la que algunos nodos la aceptarían y otros no, lo que provocaría un hard fork. Un hard fork es la bifurcación de la blockchain, donde se crean dos redes P2P diferenciadas, cada una con sus propias reglas de consenso. Este tipo de algoritmos y prácticas no se limitan a Bitcoin. Todas las redes blockchain son distribuidas y cada una tiene su propia participación en ciertos algoritmos que han evolucionado con el tiempo.

Es importante mencionar la diferencia entre Bitcoin y Ethereum. Si observamos cómo se realiza la minería en la actualidad, tanto en Bitcoin como en Ethereum, notamos patrones distintos. Bitcoin permite que, con cierta potencia de cómputo, se logre un mejor rendimiento en la minería. En cambio, Ethereum premia la descentralización en la minería. ¿Qué implica tener una minería que prioriza la descentralización en lugar de la eficiencia? Que, evidentemente, es menos eficiente. Al buscar descentralización, se requiere una mayor duplicidad de datos y más mensajes intercambiados para sincronizar todo, lo que ralentiza el proceso y se traduce en tarifas de gas más altas.

El algoritmo utilizado en Ethereum se llama Cadembly, del cual hablaremos más adelante. Es importante destacar que no es el único en este tipo de implementación; otros protocolos como IPFS o Filecoin siguen características similares. Tanto IPFS como Filecoin buscan crear una red P2P donde se distribuyan archivos de manera libre, permitiendo que cada uno de nosotros actúe como un nodo activo con participaciones en esos archivos. Al tener participaciones, se asegura la confidencialidad del archivo completo y, gracias al algoritmo que se centra en la distribución, se garantiza su robustez, no solo para la privacidad, sino también para la imposibilidad de su distribución no autorizada.

Es relevante mencionar que Ethereum no está solo en este tipo de implementación; Cadembly también ha sido adoptado por proyectos como IPFS y Filecoin, que buscan crear redes distribuidas para intercambiar archivos P2P, permitiendo que cualquiera de nosotros sea un nodo activo. Aquí, nuevamente, se resalta la importancia de contar con un algoritmo que favorezca la descentralización masiva, aunque esto afecte un poco el rendimiento. Esto se debe a que, al subir un dato a esta nube distribuida, se busca que esté muy fragmentado para que nadie tenga acceso al archivo completo y, al mismo tiempo, que nadie tenga el poder suficiente para reconstruirlo.

Desde esta perspectiva, comenzaremos a ver poco a poco el uso característico de Cadembly. Este algoritmo ha sido elegido para su implementación en Ethereum, Filecoin y otros proyectos por su robustez a la hora de encontrar caminos entre nodos cuando uno de ellos desaparece, y su capacidad para soportar ataques de denegación de servicio. La característica principal de este algoritmo es que no organiza los nodos de manera aleatoria, sino que los estructura en forma de árbol. Se forma un árbol binario donde se distribuyen particiones de archivos, transacciones o cualquier tipo de información.

Cuando tenemos este árbol de nodos, que se puede imaginar como un sauce, es necesario contar con una posibilidad algorítmica para acceder a cada nodo en caso de que ocurra un fallo. Esta es la ventaja del algoritmo. Es eficaz para prevenir ataques de denegación de servicio, que buscan bombardear una familia de nodos y hacer que no puedan aceptar más transferencias. Con Cadembly, se garantiza que, como máximo, en 20 pasos se puede acceder a cualquier nodo, y si uno falla, se salta y se busca el siguiente. Si ese nodo también está inactivo, se continúa buscando hasta encontrar uno operativo. A menos que toda la red sea derribada, se puede encontrar un nodo sano en un máximo de 20 pasos.

Si profundizamos en la implementación técnica de este algoritmo, es necesario contar con una cantidad de nodos distribuidos que hayan descubierto a sus vecinos. Una vez que se identifican los vecinos, se deben almacenar en un índice, conocido como tabla de hash. Esta tabla de hash es como la primera hoja de un libro, donde se tiene una dirección parcial que permite llegar a un vecino. Si al llegar a un vecino este no puede proporcionar servicio, se salta al siguiente. Este proceso requiere que, a medida que los vecinos fallen, se pregunte a la red por nuevos vecinos. Por ello, hay varias operaciones específicas en cualquier red distribuida que utilice este tipo de algoritmo, que incluyen: "dame mi información", "toma información" y "hola, buenos días". El "hola, buenos días" es un ping; si se hace ping a una dirección, debe responder con un pong, lo que se añade a la hoja de índices. Esto ocurre en tiempo real y de manera distribuida.

Por eso, a pesar de que Ethereum tiene un alto nivel transaccional, gran parte de los mensajes se utilizan para mantener la integridad de la red. Esto se debe a que, en la actualidad, hay intentos constantes de introducir software malicioso, y es la robustez del algoritmo la que nos protege. Cuando se dice que cierta blockchain tiene un consumo eléctrico elevado o que otra tiene un consumo menor, a menudo se basa en las necesidades de esa blockchain, no solo para validar transacciones, sino para mantener la robustez mencionada. Por lo tanto, algoritmos como el que estamos discutiendo tendrán un tráfico mayor que aquellos que permiten la centralización, lo que puede llevar a la conclusión de que Ethereum tiene un consumo eléctrico más alto y un rendimiento inferior en comparación con Solana, por ejemplo. Además, es importante considerar que Bitcoin utiliza un sistema de prueba de trabajo (proof of work), mientras que Ethereum está en transición hacia un sistema de prueba de participación (proof of stake), lo que también influye en las diferencias en el consumo eléctrico y la eficiencia transaccional.