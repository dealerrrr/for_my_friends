---
title: Tratamiento del Dato | Joan Mora
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U2 #datos #Tratamiento de Datos #Joan Mora
lang: es-AR
---
# Tratamiento del Dato
![[476.E1._Tratamiento_del_Dato.mp4]]
[Tratamiento del Dato](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41630362-u2-tratamiento-del-dato-joan-mora)

Dentro del mundo de los datos se considera un concepto que se llama pipeline, en el cual hay diferentes fases. Un ETL consiste en extracción, transformación y carga. Extraction, Transformation and Load. Entonces, la fase de extracción normalmente corresponde a una serie de scripts o tareas que suelen leer bases de datos y cargan toda esa información en memoria. En memoria, lo que se hace es, mediante... en general, se usa Python para esto, pero también SQL, realizar unas transformaciones de estos datos que se leen. Por ejemplo, puedes tener diferentes tablas en una base de datos y quieres unirlas; todo eso se hace en ese momento. Es el momento de solucionar esos problemas, porque luego se realiza una carga dentro de un Data Warehouse, donde no se permiten las ediciones.

Cuando todos los datos van al Data Warehouse, ya están preparados para ser consumidos por especialistas de todo tipo, sean analistas, CEOs o CCOs. Actualmente, se ha puesto bastante de moda el ELT, que se llama así básicamente porque el precio de almacenamiento ha bajado considerablemente en los últimos años y también la potencia de cómputo de los Data Warehouses. Entonces, lo que se hace es extraer, como en el ETL, pero se realiza la transformación dentro del propio Data Warehouse. Se utilizan diferentes herramientas, pero ahora mismo la que está más en auge es DBT, que utiliza lenguaje SQL para conseguir este tipo de transformaciones. Se carga, como siempre, otra vez en el mismo Data Warehouse.

Aunque puedas ver tablas tanto en un Data Warehouse como en una base de datos relacional tradicional, las operaciones que puedes hacer sobre ellas varían bastante. Por ejemplo, normalmente se utiliza un modelo transaccional dentro de una base de datos relacional. Esto implica tener ediciones, inserciones o borrados de forma continua. Sin embargo, en el Data Warehouse, todas estas operaciones no están permitidas; en general, no se permite la edición de filas. Realmente no se hace una edición, sino que lo que se hace es añadir filas. Por ejemplo, tampoco se permiten los borrados a la ligera, aunque en algunas tecnologías sí que se permiten. En general, el coste por almacenamiento en un Data Warehouse es mucho menor que en una base de datos tradicional.

Un Data Lake también permite almacenamiento masivo, pero el dato está totalmente desestructurado y es ideal para guardar información que nos llegue en forma de eventos, que pueden tener cualquier estructura interna y pueden estar totalmente desordenados. Muchos se pueden preguntar dónde podemos usar un pipeline tipo ETL o un pipeline tipo ELT. Realmente no hay tanta diferencia a nivel de lo que se puede conseguir, porque al final siempre estamos cargando los datos a un sitio para que sean consumidos, pero realmente sí que hay que valorar el esfuerzo económico y el esfuerzo de tiempo de desarrollo y de reacción al cambio.

Entonces, por ejemplo, tenemos un supermercado y estamos vendiendo productos a diferentes usuarios o clientes en esa tienda, y todos esos datos llegan a nuestra pipeline. Son extraídos y luego aquí viene la gran diferencia. ¿Qué hacemos? ¿Los transformamos en ese momento o los cargamos directamente? Si cargamos estos datos, realmente tendremos bastantes fallos en nuestra base de datos y estaremos guardando datos que a lo mejor no esperábamos; a lo mejor tienen un campo que realmente no es correcto o tienen un campo que no está dentro del esquema que nosotros estamos buscando. Entonces, una buena medida es transformarlos antes de cargarlos, simplemente para asegurarnos de este tipo de errores. Pero al mismo tiempo, también queremos almacenarlos en un sitio, ahorrándonos ese cómputo y ese tiempo de transformarlos antes de extraerlos y realmente volverlos a guardar. Entonces, es un tema de coste, de potencia de cómputo y de rapidez en la gestión del dato que diferentes negocios pueden valorar de diferentes maneras.

En general, ahora mismo el modelo ELT es más barato que el ETL. En Internex empezamos usando el modelo ETL, pero rápidamente nos dimos cuenta de que tenía mucho sentido migrar a un enfoque tipo ELT, básicamente porque nos costaba mucho más económico todas las gestiones que queríamos hacer y la potencia de las nuevas tecnologías como DBT, junto a BigQuery. En este caso, usamos BigQuery, y era una combinación que prácticamente toda la industria de datos estaba utilizando, y la comunidad también se estaba moviendo a desarrollar nuevos conectores y documentación que nos podía ayudar a distribuir ese conocimiento a través de todo el equipo y poder manejar rápidamente la pipeline que teníamos.

No solo eso, también teníamos un problema de que realmente nuestra extracción de datos estaba bastante acoplada directamente a guardar en BigQuery, así que guardar primero y luego transformar tenía mucho sentido. El primer paso de un pipeline de datos siempre es la extracción; la mayor parte de las veces se hace desde bases de datos, pero no siempre es el caso. Muchas veces quieres extraerlo de acciones que toman los propios usuarios; pueden ser clics, pueden ser cualquier tipo de gizmos, cualquier cosa. Entonces, la extracción desde este tipo de fuentes también es importante. Hoy en día se utilizan CDPs para realizarlo, y tiene un problema fundamental que es poder atender toda esa carga de cómputo en el momento. Todo ese streaming de datos sin perder información, porque muchas veces se pierden eventos y se pierde información por el camino. Entonces, este desafío de tener un sistema robusto durante la extracción de datos es importante.

Es fundamental tener un esquema prefijado porque esto nos ayudará a que luego el dato en nuestro Data Warehouse esté de una manera clara para el resto del equipo. Entonces, lo que se suele hacer es, antes de la ingesta, establecer un protocolo de: "Oye, espero estos datos con estos campos, con estos tipos", y cuando pase por nuestra pipeline, todo lo que no entre dentro de ese marco lo podemos descartar o hacer una transformación para que entre dentro de ese marco. ¿Por qué? Porque en el mundo real pasan un millón de cosas y estamos sujetos a caídas de diferentes sitios o servicios, estamos sujetos a posibles ataques. Entonces, tenemos que evitarnos todas estas posibles causas de problemas en la limpieza de nuestros datos, que es un problema económico bastante grande. Se suele decir que el 70 o 80% del trabajo de un data scientist es limpiar datos, y por eso no es una cuestión trivial.

Cuando llegamos a la visualización del dato, muchas veces el equipo de edición tiene que reportar al equipo directivo o a otro departamento. Es importante tener claras qué herramientas de visualización se van a utilizar y, sobre todo, qué implica, cuánto tiempo de trabajo implica utilizar esas herramientas, porque la visualización no es trivial. Muchas empresas tienen un departamento exclusivo para ello. Tenemos que optar entre tener un equipo para ello o usar una herramienta. Vas a tener que hacer consultas de SQL, vas a tener que contar con algún experto en usabilidad que pueda decir: "Oye, aquí van estos colores, aquí quiero presentar la información de esta manera o de esta otra manera", y tienes que tener claro cuáles son los gastos que vas a tener que hacer en los diferentes casos. Hoy en día hay herramientas magníficas como Amplitude o Tableau para visualizar datos directamente, pero también hay otros mecanismos como Redash que te permiten hacer más cosas, aunque son mucho más costosos y probablemente necesites un equipo dedicado a ello.