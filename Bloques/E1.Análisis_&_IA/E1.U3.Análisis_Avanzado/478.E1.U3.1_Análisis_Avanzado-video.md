---
title: Análisis Avanzado | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U3 #analisis avanzado #Análisis Avanzado #José Peris
lang: es-AR
---
# Análisis Avanzado
![[478.E1.U3.1_Análisis_Avanzado.mp4]]
[Análisis Avanzado](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41658421-u3-1-analisis-avanzado-jose-peris)

Vamos a hablar de varios conceptos clave para el correcto funcionamiento de nuestro proceso de desarrollo de algoritmos. Es decir, son aquellos conceptos que siempre debemos revisar antes de comenzar a entrenar nuestro algoritmo o cuando observamos que la precisión no es la adecuada. Siempre tendremos que volver a revisar si cumplimos con esta serie de características. Para ello, vamos a intentar analizar punto por punto en qué debemos prestar atención.

El primero, como ya hemos mencionado anteriormente, sería el concepto de los valores faltantes (missing values). Como hemos dicho, el primer punto sería visualizar nuestro conjunto de datos (dataset). Al visualizar o solicitar información, dependiendo del framework o del lenguaje de programación que estemos utilizando, lo que haremos será consultar las estadísticas de este conjunto de datos para que nos indique de una forma científica cuántos valores faltantes tenemos. No se trata de comprobarlo de forma manual. Existen diversas formas, según el método que utilicemos, de ver qué porcentaje de valores faltantes tenemos en nuestras columnas o variables. Aquí lo importante, una vez que los detectamos, es saber qué estrategia seguir. ¿Cómo manejamos esta situación? Las estrategias a seguir se pueden dividir en dos grupos: la primera sería eliminar y la segunda, reemplazar.

Aquí hay que hablar de dos escenarios muy diferentes. Uno es cuando tenemos una gran cantidad de datos, estamos hablando de 300,000 o 400,000 filas; en este caso, de alguna forma, estamos sobrados. Entonces, si tenemos un porcentaje de valores faltantes, digamos que afecta a un 20% de las filas, el hecho de que las eliminemos no va a importar demasiado. Pero existe otro escenario en el cual tenemos pocos datos. Imaginemos que tenemos 3,000 o 4,000 filas. Si aquí estamos hablando de eliminar filas donde hay valores faltantes, tendríamos un problema, porque si ya íbamos justos, estaríamos yendo aún más justos, lo cual puede afectar la performance de nuestro algoritmo. Por tanto, esta sería la primera medida: eliminar filas, decisión que tomaríamos en base a nuestra cantidad de datos.

Otra estrategia sería si vemos que una variable tiene una gran cantidad de valores faltantes, por ejemplo, un 40% de las filas. En este caso, lo mejor es eliminarla porque no habrá forma de reemplazar estos valores. Esto nos lleva a hablar de la segunda estrategia, que sería reemplazar. Esta estrategia la aplicaríamos cuando estamos en el escenario de que tenemos pocos datos. Por tanto, lo que tenemos que hacer es exprimir al máximo estos datos. ¿Reemplazar? Se puede reemplazar por la media, se puede reemplazar por la mediana, se puede comenzar con una constante o hay otro tipo de estrategias. Por ejemplo, si en un conjunto de datos un factor muy importante es la edad, y resulta que tenemos muchos valores faltantes en la variable edad y no sabemos la edad de una cantidad considerable de usuarios, pero imaginemos que también tenemos información sobre sus estudios o su ocupación.

Imaginemos que estamos trabajando con un conjunto de datos sobre estudios universitarios y tenemos información sobre lo que han estudiado o a qué se han dedicado después. Con esta información, podríamos suponer, en base a la ocupación o al nivel de estudios, el rango de edad. No es necesario que tengamos la edad exacta; podemos determinar si esta persona, a lo mejor, está en primaria, secundaria, estudios universitarios, trabajando o jubilada. Aquí tendríamos cinco grupos, y lo que estaríamos haciendo es crear una nueva columna donde podríamos transformar el parámetro de edad. De esta forma, podríamos resolver el problema de los valores faltantes. En definitiva, yo lo resumiría con una frase: comprender el contexto y ser creativo.

El segundo concepto clave es la validación cruzada (cross-validation). La validación cruzada puede ser un concepto al principio un poco confuso. A mí me gusta representarlo y visualizarlo como el concepto de mezclar las cartas. Cuando jugamos a un juego de cartas y finaliza una partida, lo que hacemos es mezclar las cartas. Ahí lo que estamos buscando es heterogeneidad. Imaginemos que hemos terminado una partida y los cuatro ases o tres ases están sobre la mesa, porque seguimos una escala. Si simplemente juntamos las cartas, lo que sucederá es que estarán los ases juntos, los caballos juntos; es decir, habrá homogeneidad. Cuando jugamos a cartas, necesitamos mezclar, heterogeneidad.

¿Y por qué es importante el concepto de validación cruzada? Recordemos que hemos hablado de que partimos en entrenamiento y en test, 80-20 o 70-30. ¿Pero qué ocurre? Vamos a pensar sobre este concepto de entrenamiento y test. Imaginemos que intentamos hacer un algoritmo predictivo, un clasificador que quiere predecir si un paciente va a sufrir coronavirus o no, en base a ciertos datos sociológicos y de historial médico. Vamos a visualizar cómo capturamos los datos, ya que hemos pasado por procesos de vacunaciones masivas. Imaginemos que cogemos un centro sanitario de una capital y medimos diversos parámetros médicos de estos usuarios.

Imaginemos que hacemos una convocatoria masiva porque necesitamos personas de todas las edades. Primero llega un autobús de un parvulario, llegan todos los niños y se les toman todos los parámetros médicos, que se almacenan en una base de datos. Luego llega un instituto, y lo mismo ocurre con todos los alumnos. Después llega gente de una empresa y, finalmente, otro autobús con personas de una residencia de ancianos. ¿Qué ocurriría aquí? Imaginemos que hacemos un 80-20 para entrenamiento y test. ¿Qué pasa? El primer autobús que ha llegado es el de los niños de preescolar y el último es el de los ancianos. Ahora, si partimos el 80-20, probablemente dejaremos fuera el autobús de los ancianos.

¿Qué ocurriría? Que este algoritmo que estamos entrenando con unos datos tiene un sesgo porque no ha visto casos de personas ancianas y no ha considerado un rango de edad. Por lo tanto, aquí hay un sesgo. ¿Qué deberíamos hacer? Mezclar las cartas de forma que en este 80% de entrenamiento tengamos todos los rangos de edad mezclados y en el 20% de test tengamos ejemplos de todos los rangos de edad. Este es el concepto de validación cruzada y se basa en el concepto de k, que son el número de particiones que hacemos. Es una forma de evitar sesgos porque, al final, no hay que olvidar que un algoritmo no es más que una opinión codificada y puede tener sesgos humanos.

El siguiente concepto clave es el sobreajuste (overfitting), que es un concepto que a priori es bastante abstracto, pero tiene mucho sentido. Como hemos dicho, los algoritmos de Machine Learning e Inteligencia Artificial aprenden de datos históricos del pasado. El sobreajuste simplemente significa que han aprendido demasiado de esos datos del pasado, que se han ajustado demasiado a esos datos. El objetivo central de un algoritmo es poder generalizar bien en sus predicciones. No es importante que realmente tenga una precisión súper elevada; es importante que sepa generalizar bien.

Es decir, ¿por qué? Para que no haga predicciones aberrantes o incoherentes. En el caso del sobreajuste, imaginemos un ejemplo visual muy sencillo: imaginemos que vais a haceros un vestido a medida con un sastre. Este sastre os mide vuestras dimensiones corporales y no deja ninguna holgura; simplemente construye un traje totalmente ceñido a vuestro cuerpo, porque esas son vuestras dimensiones. Esto nos haría caminar rígidos; siempre dejamos una holgura, un margen de confianza. Cuando hablemos del sobreajuste, hay que empezar a pensar en algo rígido, en algo que no generaliza bien, que no nos deja margen, que no nos deja tolerancia.

Ahora imaginemos que vais a comprar una cama. Es cierto que en una cama algunos van a dormir de lado, otros hacia arriba y otros boca abajo. Fijémonos en que una cama es rectangular y nos deja un margen, una tolerancia. Fijémonos en la gráfica de la derecha y veremos un ejemplo clásico de sobreajuste. Estamos hablando de que, al final, los datos son una nube de puntos. ¿Qué ocurre con un punto hasta aquí, otro hasta aquí, otro hasta aquí? Cuando un algoritmo aprende con sobreajuste, lo que está ocurriendo es que va prediciendo cada punto aprendido; se ha entrenado de forma demasiado exacta. No nos interesa que se ajuste a cada punto; nos interesa que pase por la media de los puntos. Esto se relaciona con el concepto de media y varianza; es decir, nos interesa que nuestro algoritmo no haga una predicción exacta de cada punto, sino que esté en la media correcta de todos los puntos.

¿Por qué? Porque esto lo que nos permitirá es que sepa generalizar bien. Si un algoritmo se entrena con sobreajuste, lo que ocurrirá es que, en el momento en que tenga que hacer la predicción de ciertos valores que se salen un poco de este patrón, no realizará predicciones muy acertadas. Dicho de una forma sencilla, puede pasar de predicciones del 99% a predicciones del 55%. Esto no nos interesa, ya que se ha entrenado en sobreajuste y se ha ajustado demasiado a los datos históricos. Por tanto, cuando ve un dato nuevo extraño, no sabe cómo comportarse. Nos interesa más que el algoritmo tenga una precisión más estable, por ejemplo, del 85% o 89%, pero que cuando vea casos atípicos sepa generalizar bien.

Es decir, cuando hablamos de sobreajuste, estamos hablando en Machine Learning y en Inteligencia Artificial de que el 100% no existe, por lo cual siempre asumiremos un error. Este error, tanto en regresión como en clasificación, queremos tenerlo controlado y que al final permita generalizar de forma coherente, porque el error ya es asumido. La pregunta es: ¿cómo detectar el sobreajuste? Se detecta cuando, al entrenar el algoritmo, los resultados que arroja sobre los datos de entrenamiento muestran precisiones muy elevadas, estamos hablando del 99%. Por tanto, aquí debemos prestar atención a que nuestro algoritmo se ha ajustado demasiado a los datos, lo que significa que no será flexible ante datos nuevos. Aquí es donde debemos iterar, revisar y aplicar técnicas para evitar el sobreajuste.

¿Qué técnicas nos ayudarán a evitar este sobreajuste? Lo primero es la validación cruzada, como hemos mencionado, partir en un número de particiones y mezclar las cartas. También podemos aplicar aumento de datos (data augmentation), que es una técnica que aumentará de forma artificial nuestra cantidad de datos. Podríamos probar también otras técnicas como el dropout o el early stopping, que al final buscan heterogeneidad. El dropout consiste en dejar fuera ciertos datos de forma aleatoria; al final, lo que buscamos es tener esta heterogeneidad y que los algoritmos aprendan de forma correcta.

Otro concepto clave sería el concepto de valores atípicos (outliers). En primer lugar, hay que mencionar que debemos hacer una buena práctica para detectar un valor atípico. Para ello, lo que tenemos que hacer es visualizar y graficar nuestros datos. Un valor atípico es un punto de datos que tiene un valor muy atípico, es decir, que se desvía mucho de la media. Intentemos pensarlo en el mundo real. Imaginemos que vamos a elaborar un algoritmo predictivo de ventas para un e-commerce que venda todo tipo de productos, similares a los que puede tener Amazon. Capturaremos la base de datos y comenzaremos a analizar los valores.

Resulta que vemos que, imaginemos, que todos los días entre semana se venden de media, y nos encontramos que un día, un viernes de un mes concreto, ese día se vendieron diez veces más. Lo primero que haríamos sería visualizar ese dato, detectarlo y comprobar si ese valor es real o no, porque podría pasar que, en lugar de mil, en la base de datos se ha insertado manualmente diez mil. Entonces estaríamos hablando de una anomalía, y habría que corregirlo. Pero resulta que hacemos las comprobaciones, hablamos con el cliente y nos dice que sí, que efectivamente ese día se vendieron diez mil unidades.

A nivel de variables, diríamos: ¿qué pasó ese día? ¿Había una promoción? No. Imaginemos que el hecho que provocó esto fue que en Amazon se terminaron las existencias de un producto muy demandado. Por tanto, como en Amazon se terminó, los clientes llegaron hacia nosotros. Este hecho ha sucedido y, obviamente, es importante. Pero si pensamos en términos matemáticos y nos imaginamos esa gráfica, y estamos hablando de valores de mil o cercanos a los 1,900, 1,100, 1,200 y, de repente, 10,000, este valor sucedió, pero recordemos que un algoritmo puede ser visto de una forma abstracta, conceptual, como una línea recta o curva que tiene una fórmula matemática.

¿Qué ocurre? Que esta línea se distorsionaría enormemente por este punto, por este valor tan elevado. Entonces, es aquí cuando hay que volver al concepto de generalizar. Como queremos que generalice bien, en este caso es más útil eliminar el valor atípico porque generará una distorsión en todo el conjunto. Es aquí donde es muy importante remarcar la necesidad de entender la naturaleza del problema que estás abordando. Pero no siempre es la mejor estrategia eliminar los valores atípicos; es decir, tenemos que entender, como hemos dicho varias veces, la naturaleza del problema.

Por ejemplo, en los casos de predicción de los mercados de valores, en el caso de acciones, en una bolsa de valores o en casos de criptomonedas, los valores atípicos son muy importantes; es decir, hay que prestarles mucha atención. De hecho, las métricas que hay que utilizar en estos casos son métricas que están preparadas para prestar especial atención a los valores atípicos. ¿Por qué? Porque en la gráfica, por ejemplo, de un proyecto de criptomonedas, de un activo digital, los valores atípicos son muy importantes: picos y valles potentes que, al final, indican que ese día ocurrió algo significativo. Por tanto, respecto a los valores atípicos, hay que analizar la naturaleza del problema para determinar la estrategia que seguiremos.

Otro concepto muy importante es la normalización y estandarización, también conocido como escalado de características (feature scaling). Como hemos comentado anteriormente, podemos trabajar con múltiples variables. Estas pueden ir desde 4 hasta 10, 12 o incluso hasta 40. Aquí es muy importante entender que estas variables se encuentran en diferentes escalas. Por ejemplo, si estamos hablando de productos del sector inmobiliario, podríamos decir que la variable euros puede ir desde 0 hasta 4 millones de euros, o 10 millones de euros, por ejemplo, pero la variable cuartos de baño puede ir desde 0 hasta 5 como máximo.

¿Qué tenemos aquí? Escalas muy diferentes, escalas muy desproporcionadas. Entonces, la normalización y estandarización lo que pretende, de una forma muy sencilla y conceptual, es reducir esta escala manteniendo la proporción. Lo que vamos a hacer al normalizar es transformar todos estos valores para que estén entre el rango de 0 y 1. ¿Y por qué vamos a hacer esto? Porque al final estamos hablando de que el algoritmo, para que lo visualicemos conceptualmente, es una curva o una recta, es una gráfica, y lo que vamos a hacer es facilitarle las cosas, es decir, que entienda de una forma mucho más ordenada estos valores.

Al final, lo que ocurre es que, cuando aplicamos una normalización, lo único que sucederá es que, en lugar de ver 8 millones de euros, veremos 0.9, o en lugar de ver 100 mil euros, veremos 0.1. Este es el cambio que apreciaremos, pero lo que lograremos es que, durante todo el proceso de entrenamiento, el algoritmo nos arroje resultados mucho más coherentes. También hay que recordar que el proceso de normalización hay que hacerlo y deshacerlo, porque después, a la hora de interpretar resultados, hay que recordar que tenemos todos estos valores en esta escala.

La estandarización es un proceso similar. Lo único es que se utiliza la estandarización cuando estamos resolviendo problemas que asemejan la distribución de los datos a la campana de Gauss. De esta forma, simplemente colocamos el 0 en la mediana y el rango lo hacemos desde 1 hasta -1. Podemos aplicar una, podemos aplicar las dos, pero, en general, es una buena práctica intentar llevar los datos a la escala correcta cuando observamos grandes diferencias de escala o magnitud.

Y, por último, otro concepto muy importante que podríamos denominar codificación one-hot (one-hot encoding) es de suma importancia porque hay que recordar que podemos tener variables. Hemos visto tipos de datos; podemos tener timestamp, podemos tener enteros, podemos tener double, que son al final números, pero también podemos tener strings. Imaginemos que, en el ejemplo inmobiliario, estamos hablando de la ubicación de una vivienda. Puede ser centro, puerto, periferia o zona montañosa. Aquí tenemos palabras; puede haber cuatro valores de palabras. Nunca hay que olvidar que, al final, en todo este campo, lo que necesitan los algoritmos son realmente números, son ceros y unos. Al final, todo se basa en esto, por lo cual no nos va a admitir palabras.

El concepto de One-Hot Encoding lo que hace es que, si una variable puede tener cinco valores, por ejemplo, si puede ser playa, montaña, etc., la desplegará en cinco. Es decir, playa, montaña, periferia, centro; la variable ubicación se convertirá en cinco columnas, en cinco posibles valores, y a cada fila le asignará un 1 o un 0. Imaginemos que la siguiente casa está en la montaña. En la variable ubicación playa tendrá un valor 0, en ubicación montaña tendrá un valor 1 y en el resto tendrá un valor 0. Al final, lo que hacemos es desplegar. El punto clave aquí es saber cuántos valores puede tomar nuestra variable; si son 5, se transformará una variable en 5 variables. De esta forma, le permitiremos al algoritmo trabajar de forma óptima, ya que muchos de ellos solo aceptan números.