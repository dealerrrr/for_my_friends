---
title: El futuro del Machine y Deep Learning | José Peris
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U7 #futuro del machine learning y deep learning #Machine Learning #Deep Learning #José Peris
lang: es-AR
---
# El futuro del Machine y Deep Learning
![[506.E1_El_futuro_del_Machine_y_Deep_Learning.mp4]]
[El futuro del Machine y Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866177-u7-el-futuro-del-machine-y-deep-learning-jose-peris)

Cuando hablamos del futuro de la inteligencia artificial y del Machine Learning, es imposible no mencionar un concepto que se llama Automachine Learning. Este concepto está generando muchas dudas en este sector, porque realmente estamos hablando de inteligencia artificial que desarrolla inteligencia artificial. A priori, puede parecer una amenaza para la humanidad, pero no tiene nada que ver con esto. Simplemente, estamos hablando de que, como dijimos, la razón de ser de este campo de la IA y del Machine Learning es eliminar tareas repetitivas de bajo valor. Por tanto, se ha aplicado el mismo concepto al desarrollo de este tipo de tecnologías.

De una forma muy rápida, tenemos plataformas que nos permiten realizar tareas que en otras ocasiones duraban meses. Debemos ponernos un poco en la piel de un científico de datos; tenemos básicamente dos grupos, dos formas de afrontar un desarrollo. La primera sería lo que se llama desarrollo a bajo nivel, donde estaríamos hablando de código, específicamente de Python, que es el lenguaje oficial, por decirlo de alguna forma, de esta comunidad, aunque están apareciendo otros, como Go, que también pretenden ganar terreno. También tenemos R, que se utiliza más en el ámbito de la bioinformática. En definitiva, Python es el que tiene la mayor comunidad.

Pero también tenemos otro grupo, que sería el de los programas de alto nivel. Esta es una pregunta muy común entre los estudiantes: ¿qué son los programas de alto nivel? Son aquellos que son user-friendly, es decir, son interfaces de usuario que son amables y fáciles de entender. En pocas palabras, estamos hablando de escribir código versus tener herramientas que son prácticamente drag and drop, que son más familiares y que trabajan con el concepto de nodos.

¿Qué herramientas tenemos que usar en este segundo grupo de drag and drop que son mucho más entendibles para la gente común y que realmente ayudan mucho en la fase inicial del aprendizaje? Tendríamos herramientas como KNIME, ORANGE y BICML. ¿Qué podría comentarles sobre estas herramientas? Pues bien, básicamente KNIME está muy bien para el aprendizaje en fases tempranas y para poder prototipar, defenderse y entender de una forma esquemática lo que son los flujos de un algoritmo respecto a la ETL y en la fase de prueba, de ensayo y error con diferentes algoritmos. Estamos hablando de un concepto de nodos.

Obviamente, no son programas profesionales, sino que nos sirven de puente para poder llegar a pensar y aplicar todos los conceptos tanto teóricos como prácticos. Después tendríamos programas como Orange, que es del paquete Anaconda, donde se ubica Python. Han creado una interfaz mucho más amable y agradecida, que también nos permite prototipar muy rápido y de una forma muy intuitiva. En definitiva, nos permite acortar tiempos y, sobre todo, eliminar barreras al aprendizaje.

BKML sería un concepto que ya se sale de lo que son los nodos y realmente es un concepto más por ventanas. Estas serían las tres herramientas; las dos primeras son gratuitas, mientras que BKML es de pago. Sin embargo, este concepto, sobre todo el de nodos, es el que están utilizando ya plataformas como Azure o IBM Watson, en este caso Azure de Microsoft y IBM Watson, que son plataformas drag and drop.

¿Qué significa esto de conectar nodos? Conectar nodos implica que nos evitamos muchos errores de sintaxis de código, pero también significa que no tenemos tanta capacidad de edición como en el código. Es decir, al final, el código será la tecnología que siempre irá más rápido, será la más profesional y la más escalable de cara a ponerlo en producción. Pero sí es cierto que con el nacimiento de Azure o IBM Watson se abrió una puerta para trabajar con drag and drop que nos puede dar mucha agilidad a la hora de trabajar.

Desde este punto, es cuando empezaron a surgir nuevas tecnologías como Data Robot, H2O, Data IQ, Data Bricks y Rapid Miner, donde se centraron nuevamente en los nodos, pero yendo un paso más allá, aplicando Automation Learning. Automation Learning, al final, como hemos visto, para que tengan una idea muy sencilla, implica hacer una serie de comprobaciones y conceptos clave, como los missing values, los outliers y qué correlaciones hay entre diferentes variables. Esto en código lleva bastante tiempo, siempre teniendo en cuenta que debemos visualizar los datos, tomar decisiones y ver qué factores hay que corregir, que es el proceso de ETL.

Esto lleva mucho tiempo en código, dependiendo del nivel que tengas a la hora de saber escribir y depurar código. Por tanto, aquí surge una solución que es el Automation Learning. Podríamos decir que el líder a nivel mundial es DataRobot, pero ya existe un grupo bastante nutrido; RapidMiner, por ejemplo, también está haciendo bastante sombra. El AutoML lo que hace es que todos estos insights, porque al final estas plataformas te revelan insights que están en los datos, ya te están marcando y sugiriendo qué columnas habría que eliminar potencialmente, qué tipo de filas y qué correlaciones tienen entre ellas, todo esto de forma automatizada.

Esto, al final, lo podemos traducir en tiempo, y al final el tiempo es dinero. Claro, ¿qué problema presenta esto para el usuario? Básicamente, el problema es que las licencias aún son bastante caras. Estamos hablando de que una licencia de AutoML, de estos softwares de pago, puede oscilar entre 50.000 euros por usuario hasta 150.000 euros anuales. Este es un coste que no todas las empresas se pueden permitir, pero sí es cierto que lo que a priori era un mundo bastante cerrado, del AutoML, tiene la ventaja de que permite realizar de forma automatizada el EDA, lo que es el Exploratory Data Analysis, y la ETL, junto con la selección de algoritmos.

Lo que ha sucedido es que han aparecido ya versiones mediante las cuales, con código y de forma gratuita, podemos utilizar AutoML. En esta sesión de hoy, vengo a exponerles las principales herramientas de código gratuitas que podemos utilizar para trabajar con AutoML. Cuando hablamos de AutoML, debemos pensar que, como les he dicho siempre, la primera parte es entender el contexto, el negocio y la naturaleza del problema. Pero también debemos visualizar los datos; debemos pintarlos. Esta parte es muy importante porque si no visualizamos los datos, no llegaremos a entender si hay outliers, si hay correlaciones, y hablábamos también de matrices de correlaciones, por ejemplo, de relaciones entre variables.

¿Qué ocurre? Que este proceso es bastante lento también porque, a nivel de código, hay que ejecutar muchas sentencias y, a día de hoy, ya tenemos herramientas que nos permiten hacer AutoEDA. AutoEDA sería Automachine Learning aplicado al Exploratory Data Analysis. ¿Qué herramientas tenemos? En Python, tenemos la librería Pandas Profiling, que desde mi punto de vista es la más potente y la mejor resuelta. ¿Y por qué? Porque estamos hablando de que, básicamente, en 10 líneas de código, cargando nuestro dataset, nos va a ejecutar, nos va a pintar todos los datos referentes, nos va a visualizar todas las correlaciones y todas las estadísticas de este conjunto de datos, y además nos lo va a generar en HTML.

Con lo cual, estamos hablando de que en cuestión de minutos vamos a tener unos insights muy potentes de este conjunto de datos. No solo tenemos Pandas Profiling, también tenemos Sweetviz, que de una forma diferente, con otro planteamiento, nos arroja prácticamente la misma cantidad de datos. El objetivo del AutoEDA, así como del Automachine Learning, será siempre ahorrar tiempo. Les dejamos aquí dos notebooks para ejecutar con Google Colab, donde pueden probar estas herramientas de forma muy sencilla, simplemente subiendo sus datasets. Verán que es una forma muy ágil de comenzar a trabajar con una base sólida.

Respecto al Automation Learning, creo que cualquier persona que haya trabajado en este sector como científico de datos, este es el sueño de cualquier científico: poder utilizar con código Automation Learning. ¿Por qué? Porque, básicamente, diríamos que es la máxima expresión de la practicidad. ¿Por qué? Porque al final, en código, tenemos acceso y podemos utilizar todas las herramientas que queramos, que van evolucionando de forma muy rápida. Pero si podemos utilizar Automation Learning, aprovechar las ventajas de otro tipo de software, que en este caso son corporaciones muy potentes con mucha inversión, y poder utilizar características similares a las que ellos proponen, esto realmente es algo muy práctico.

Con Automation Learning, hay consideraciones a tener en cuenta. Al final, Automation Learning nos va a hacer una... En el caso del código, no tenemos tantas herramientas para utilizar en el proceso de ETL, pero en el momento en que tenemos que probar diferentes algoritmos, debemos pensar que hacerlo prácticamente uno a uno sería un proceso muy lento, porque hay que invocar el algoritmo, probarlo y comprobar resultados. Aquí está la ventaja que nos da Automachine Learning: simplemente subimos los datos, realizamos la ETL, trabajamos las variables y automáticamente decimos que nos prueben el mejor algoritmo. Por ejemplo, le podemos pedir que nos pruebe 100 algoritmos y que nos ordene en un ranking los que mejor rendimiento tienen sobre los datos de prueba, ordenándolos por accuracy y por predicción. Esto representa un avance total, además de que ya te ofrece y te propone los hiperparámetros, es decir, nos soluciona dos partes muy pesadas.

Al final, probar diferentes algoritmos lleva un tiempo considerable en programación, pero una vez hemos seleccionado uno, elegir los hiperparámetros es la parte más abstracta, técnica y compleja; al final, estamos hablando de hilar fino. Cuando hablamos de ajustar hiperparámetros, simplemente, para que tengan una idea, es cuando tenemos una precisión del 90% y queremos subirla hasta el 95%. Este tramo es el más complejo de todos, porque parece que ya estamos cerca, pero hay que afinar muchísimo para poder lograr el objetivo que tenemos marcado a nivel de precisión. Por tanto, estamos hablando de que estas herramientas nos pueden resolver muchísimos problemas.

Entre las más comunes, hay que saber que el Machine Learning cambia constantemente; es un campo que está totalmente vivo. Aquí les presentamos herramientas de AutoML en código gratuitas, como puede ser, por ejemplo, TPOT. También mencionaríamos H2O o H2O, dependiendo de cómo lo queramos leer, y AutoKeras. AutoKeras lo que está haciendo es auto-deep learning. Por decirlo de alguna forma, Keras es un framework de deep learning. Normalmente, cuando nos enfrentamos a crear una red neuronal, y para retomar un poco los conceptos anteriores, cuando tenemos muchos datos, cuando tenemos un dataset muy grande, podemos utilizar deep learning. Nuestro principal reto será decidir cuál será la arquitectura de esta red neuronal, ya sea por capas o por neuronas.

Lo que nos ofrece AutoKeras es realizar AutoML y revelar cuál es la mejor arquitectura posible para nuestro problema. También tenemos otra herramienta, Scikit Learn, que es una librería de Machine Learning de Python muy potente. Además, tenemos PyTorch y, seguramente, para el momento en que vean estas clases, habrá salido alguna más. Por nuestra parte, les adjuntamos la documentación para que puedan trabajar con ellas. A mí, personalmente, AutoH2O es la que más me gusta. Y el motivo es que, además, ofrece una serie de librerías que te muestran la explicabilidad del modelo, como los SHAP values, por ejemplo.

La explicabilidad del modelo es muy importante, porque no solo se trata de decir que este modelo es el que mejor predice, sino que también nos va a explicar, en nuestro caso de negocio, cuáles son las palancas o variables clave y en qué porcentaje para poder pasar a la acción y mejorar desde el punto de vista empresarial. Por tanto, les animamos a que prueben los notebooks que les dejamos aquí y que disfruten del concepto de Automation Learning.