---
title: Diego Bonilla | Generar Imágenes con Stable Diffusion
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #practica #Generar Imágenes #Stable Diffusion #Diego Bonilla
lang: es-AR
---
# Generar Imágenes con Stable Diffusion
![[502.E1_Generar_Imágenes_con_Stable_Diffusion.mp4]]
[Generar Imágenes con Stable Diffusion](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948289-p6-3-diego-bonilla-generar-imagenes-con-stable-diffusion)
[Generar Imágenes con Stable Diffusion (PDF)](503.E1_StableDiffusion-230120-152902.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/1KwcbPDiRL_RArlVdWvopIZ-_nEVICn1K?usp=sharing)

Hola y bienvenidos a un nuevo curso de W3. En este curso vamos a abordar Stable Diffusion y diferentes modelos de traducción de texto a imagen o de generación de imágenes a partir de texto. La idea que se había planteado para esta clase era comenzar con un poco de la trayectoria que han seguido estos modelos, ver un poco sus inicios, cómo han comenzado, las primeras redes que realizaban este tipo de tarea y luego también pasar directamente a dónde está el estado del arte en este momento, los modelos actuales, qué tipo de imágenes pueden generar y para qué se pueden usar.

Como ya sabéis, esto también interesa tanto a nivel de investigación como a nivel de deep learning e inteligencia artificial. Evidentemente, es un tema extremadamente interesante, con mucho futuro y que ha tenido un desarrollo significativo en estos últimos meses, sobre todo en los últimos dos años. Entonces, veremos un poco por qué existe ese interés y luego también cómo se puede usar y qué significa para el arte, sobre todo porque, claro, ahora que se está poniendo de moda, por ejemplo, la generación de arte digital, de NFTs, de arte digital y de compra y venta de estas piezas de arte, este tipo de modelos también ayudarían o cambiarían un poco ese tipo de mercados. De tal forma que cualquiera que tenga conexión a internet, y la mayoría de las veces de forma gratuita o pagando muy poco dinero, puede generar imágenes indistinguibles de imágenes reales o de piezas de arte generadas por cualquier autor en cualquier estilo a lo largo de la historia, básicamente con este tipo de herramientas.

Por lo tanto, son bastante útiles de conocer y evidentemente van a ser muy prácticas y muy utilizadas en el futuro. Por último, vamos a ver una práctica, unos pequeños ejemplos hechos por mí para vosotros en Collab. Está todo bastante preparado, simplemente iremos ejecutando y explicando cómo generar imágenes de forma gratuita desde Collab con muy buena calidad. Eso lo veremos al final.

Para empezar, como ya hemos comentado, vamos a ver un poco de dónde vienen este tipo de modelos. La verdad es que son relativamente recientes. Por ejemplo, uno muy famoso fue Stack GAN, que fue del 2017 y representó un hito, por así decirlo, para este tipo de modelos porque hasta ese momento no se habían generado imágenes de tanta calidad. Evidentemente, para hoy en día no es una calidad muy buena, pero para aquel entonces la verdad es que era impresionante. Como no sé si se puede ver por la imagen, digamos que el modelo tiene dos etapas. La primera etapa generaba un pequeño boceto muy borroso, con mucho ruido, del texto que se intentaba generar y luego, en una segunda etapa, lo que hacía era, con el boceto y con el texto, intentar realizar una especie de super resolución o mejorar la resolución y la calidad de la imagen, teniendo en cuenta también lo que se quería generar.

Digamos que, pensando desde un punto de vista estadístico, es un problema que se llama "one to many" porque un mismo texto puede generar una cantidad casi infinita o infinita de imágenes. Por lo tanto, en este modelo no hay una respuesta correcta. Sí que existen respuestas incorrectas o, mejor dicho, respuestas lejanas a la idea que se quería generar, pero no existe solo una respuesta correcta. Esto hace que al entrenarlo sea muy complicado y complejo. En este caso, se entrenó esta red neuronal, la Stack GAN, utilizando la metodología GAN, que es una forma de generar imágenes en la que se contrasta la generación con un discriminador que te dice si lo que se ha generado es realista o no, por así decirlo. Intenta determinar si lo que se ha generado es falso o verdadero y, con la salida de este discriminador, también hay un bucle en el que se entrena al propio generador para que aprenda a engañar al discriminador. Al final, este bucle y este juego lo que hacen es generar imágenes con mucha calidad y con no demasiados datos, ya que es casi imposible o muy difícil, o teóricamente no se podría hacer overfitting de este tipo de modelos, ya que no ven los datos originales.

Esto sería en 2017. Vamos a dar un salto de cuatro años al 2021. En enero de 2021, la compañía que comenzó un poco esta nueva moda de generación de imágenes a partir de texto fue OpenAI. OpenAI, en enero del año pasado, 2021, lanzó su primera versión de Dall-E, que era un modelo que utilizaba un tipo de generación de imágenes que se llama VQ-VAE, que es Vector Quantized Variational Autoencoder. Este modelo utiliza una representación discreta de los datos en lugar de continua, como por ejemplo pueden ser las GANs o otros tipos de modelos más utilizados para este tipo de generación de imágenes. Esto lo que hace es que tenga muchísima más representación en ese espacio embebido y, además, que dé capacidad a muchas más variedades diferentes.

Entonces, gracias a esta nueva representación, digamos que fue más, entre comillas, fácil o fue un poco el desencadenante de que la calidad de las imágenes fuera mucho mejor. En este caso, se demuestra en la imagen de la izquierda, que es la del 2021, y la de la derecha se correspondería a su variante más actual en abril de 2022. Creo que es un poquito anterior, pero bueno, el paper revisado, la última revisión que se hizo, fue en abril de 2022, en el que OpenAI lanzó su segunda versión. El salto de calidad fue increíble, no solo en términos de calidad, sino también en tamaño y en entendimiento del texto. Un poco lo que cambió de un modelo al siguiente es que el siguiente no utiliza la metodología que hemos hablado de los VQ-VAE, sino que utiliza un proceso de generación de imágenes, en este caso o de datos en general, que se llama modelo de difusión.

El modelo de difusión es una técnica en la que a una imagen conocida se le añade ruido poco a poco, paso a paso, en el que en cada paso se le va añadiendo más ruido. En cada paso que se realiza, lo que se crea es una renderización que ve un poco cómo se ha añadido el ruido y que aprende a deshacer ese ruido, todo esto por pasos. Entonces, al final, por ejemplo, tenemos 700 pasos en los que se ha añadido ruido 700 veces. ¿Qué pasa? Que al final, después de añadir ruido 700 veces, te queda una imagen con muchísimo ruido, tanto ruido que es indistinguible de una imagen simplemente de ruido, de tal forma que la imagen original queda completamente oculta y eclipsada por el ruido. Pero como hemos tenido una renderización que ha aprendido o le hemos enseñado a deshacer cada paso, lo que vamos a hacer es empezar desde el paso 700 y deshacer, creando de esta forma hasta que deshaga todo el ruido, vaya quitando el ruido en estos 700 pasos. Entonces, al final nos queda una imagen sin nada de ruido, como las que vemos en la pantalla. Esto es lo que nos permite, dado que después de todos esos 700 pasos te queda ruido, básicamente porque la imagen ya ha sido completamente destruida. Lo que vamos a hacer es simplemente coger ruido aleatorio y pedir que deshaga los pasos.

Este proceso, a partir de ruido aleatorio, puede ser capaz de generar imágenes que no existen en nuestra base de datos, que no ha sido entrenado con ellas, por así decirlo. Entonces, lo que hacemos al realizar este proceso inverso de difusión, que se llama "quitar ruido", es condicionarlo con contexto. Si simplemente le ponemos una condición textual a este proceso de difusión inverso, lo que hacemos es condicionar que, digamos, genere o quite el ruido de esta imagen sabiendo que en esta imagen hay, por ejemplo, un cerro en un campo en el estilo de Monet, por ejemplo. Por lo tanto, es capaz de generar datos con muchísima más calidad y con un proceso de generación de imágenes mucho más estable que utilizando, por ejemplo, las GANs, como hemos visto en la diapositiva anterior. Esto permite también trabajar con datasets mucho más grandes, como por ejemplo el que fue entrenado Dall-E, que son billones de imágenes, lo que también permite tener información de una gran cantidad de diferentes objetos, artistas, animales, situaciones, y todo un poco.

Sin embargo, menos conocido, en mayo de 2022, Google también lanzó un par de modelos de este estilo. Google lo que hizo fue utilizar otro tipo de arquitectura que no vamos a detallar, pero que es un poco parecida a la anterior que hemos visto de VQ-VAE combinada con GANs, aunque era muchísimo más robusta en algunos problemas que tenía OpenAI. Dall-E tiene un gran problema, por así decirlo, que es que no sabe escribir. Como se puede ver en la imagen de la izquierda, estos serían textos generados por OpenAI y por Dall-E, que podéis usar todos pagando una pequeña cuota en su API. Digamos que si le pones un cartel que, por ejemplo, genere un cartel en el que se escriba "deep learning", ya veis que genera un poco de todo menos lo que le estamos pidiendo. Mientras que, por ejemplo, si a la imagen de Google le pedimos que genere una tienda en la que se pueda leer un letrero que diga "texto imagen", es bastante legible de forma humana. Mientras que en esta parte de aquí sería también la generada por Dall-E, en la que no ha conseguido escribir el texto y a veces parece como una especie de combinación rara de letras o que intenta hacerlo, pero no lo consigue. Esto es un poco de la potencia del modelo, que no permite tener tanto entendimiento de geometría. Sobre todo, le cuesta bastante, y es algo que también le cuesta a la mayoría de los modelos. La imagen también, que es la geometría del entendimiento de una escena a nivel geométrico, es algo extremadamente complicado que, obviamente, sabemos desde el nacimiento. Pero digamos que al decir, por ejemplo, "quiero una foto en la que haya 35 pelotas de ping-pong", a este tipo de redes le cuesta muchísimo porque tiene que entender, por ejemplo, dónde empieza y dónde acaba una pelota de ping-pong a nivel de píxel o a nivel de concepto, que se vean todas por la imagen y que no haya ningún tipo de contaminación entre conceptos. Entonces, claro, a partir de creo que eran de 7 u 8 objetos, se lía. Si le pedimos, por ejemplo, a partir de 10 pelotas de tenis o lo que sea, en principio se va a empezar a liar. Pero bueno, en texto pasa algo parecido, en el que, digamos, que en texto tienes que entender el orden de lo que se está escribiendo, aparte de también entender que luego un humano lo pueda leer. Es algo que OpenAI, Dall-E, perdón, no lo entendía y, por ejemplo, el de imagen, en este caso, sí que lo consigue entender.

Luego, Google sacó otra versión de un modelo de texto-imagen. Estos, como os digo, no están públicamente accesibles y no parece que los vayan a lanzar en ningún momento. Son modelos mucho más grandes que Dall-E, pero aparte de eso, también, digamos que la calidad de las imágenes, en principio, es mejor. Luego, bueno, ya podéis ver por las imágenes que no solo conceptos un poco más abstractos, como una cobra hecha de sushi o lo que sea, que a lo mejor también Dall-E podría llegar a generar, o por ejemplo, un wombat tomándose un cóctel con camiseta hawaiana, y todo eso también Dall-E lo podría llegar a generar. Pero en el texto, como vemos en la de la derecha, el texto es de muy buena calidad y cumple con todos los requerimientos de diseño y, sobre todo, de estilo que se le pide. Entonces, la verdad es que es un buen paso hacia esa dirección de entendimiento de la escena. Sobre todo se ve al final que el texto que se escribe es una excusa para saber si o determinar si un modelo es capaz de entender cosas muy complicadas, como por ejemplo en la escritura. Entonces, si es capaz de escribir texto que nosotros podamos leer, es una muy buena señal.

Una cosa a tener en cuenta también, que ha sido bastante comentada a la vez que se iban publicando este tipo de papers, es que, evidentemente, todo este research se está haciendo desde América principalmente. Por lo tanto, los datos de internet en general están un poquito, o sea, bueno, tienden hacia la cultura americana y europea. Entonces, es razonable decir que existe un sesgo en este tipo de modelos, dado que, evidentemente, hay un punto de referencia en el que se crean estos modelos. Por lo tanto, evidentemente, siempre que quieras, yo qué sé, por ejemplo, medir la palabra "exótico", en cualquier parte que te vayas de la tierra, la palabra "exótico" va a significar cosas completamente diferentes. En la imagen que vemos a continuación, la palabra "exótico" tiene una definición muy europea y muy, digamos, americana. Una persona exótica tampoco te va a salir como las que estamos acostumbrados a ver, y sale desde un punto de vista muy diferente al de, por ejemplo, cualquier otra persona de otro país. También, este por ejemplo, el punto de vista de una persona exótica puede ser uno de los menos perjudiciales, pero sí que otras sí que fomentan los estereotipos que tenemos en este tipo de países, lo cual es un poco evidente. Lo que estaba comentando en la diapositiva anterior es que, evidentemente, si los datos son basura, si le das basura, te da basura. Entonces, si los datos que le hemos dado son internet entero y, por lo que sea, está plagado de este tipo de tendencias americanas y europeas, evidentemente lo que saca este modelo va a ser ese tipo de cosas que ha aprendido. Esto puede ser, evidentemente, perjudicial. También, evidentemente, si a este tipo de modelos, en vez de simplemente decirle "quiero un terrorista", le dices "quiero un terrorista rubio con ojos azules", evidentemente te va a sacar un terrorista rubio con ojos azules. Entonces, esto es muy fácil de mitigar; cualquier persona lo puede evitar. Pero bueno, esto simplemente es lo que saca el modelo de Dall-E en este caso con palabras o con conceptos muy generales.

Hemos hablado mucho de Dall-E, que es de pago, y hemos hablado también de los modelos de Google, que no son accesibles para el público en general. Ahora lo que vamos a ver son modelos que son gratis o también otros modelos que son más baratos. En algunos de ellos veremos que tienen una prueba gratuita, pero que luego, si se quiere generar más, habrá que pagar un poco después. Otros no dejan prueba gratuita y otros son 100% gratuitos. De estos, vamos a ver un poco el estilo de imágenes que generan, los pros y contras de cada uno y un poco, pues también para que, si en algún momento alguien quiere hacer algún tipo de proyecto sobre esto, también sepa qué puede esperar de ejecutar un mismo texto en varios de estos sistemas.

Primero, mencionar otra vez más a Dall-E. Dall-E no tiene prueba gratuita, pero puedes generar 115 imágenes por 15 dólares y permite hacer una gran variedad de herramientas dentro de su página web. Este ejemplo de aquí sería simplemente poner texto, decir, en este caso, yo qué sé, "un par de ositos de peluche químicos en óleo", y generaría unas 4 o 5 imágenes. Pero también le podemos dar una imagen, como por ejemplo la de Vermeer, la de la mujer con el pendiente, y te genera variaciones de la imagen que le vayamos a dar. Esto se puede utilizar posiblemente para logos, para retratos, para otro tipo de arte, para que te genere, por ejemplo, piezas similares a las que tú quieres o fotografías, por ejemplo, de un diseño de una habitación y todo eso. No solo eso, sino que también sobre la misma imagen, otra herramienta que tiene Dall-E es que podemos estirar la imagen de forma coherente. Entonces, al estirar la imagen, esta es el cuadro original. La pieza que está aquí, también en el recuadro, es capaz de extender la imagen en todas las direcciones de forma arbitraria. O sea, no tiene final, simplemente el número de dinero que os queréis gastar en la que se puede ir generando parches por parche, viendo hacia dónde va la imaginación, por decirlo de alguna forma. Dall-E copiará el estilo, asegurará que haya una transición buena entre la imagen que le vayáis a dar y el resto, y que tenga sentido. Todo eso, lo dicho, es una herramienta muy potente. La calidad de las imágenes que genera es muy buena, pero es de pago y el modelo no es libre, por lo que no podemos acceder al modelo y ver qué pasa dentro del modelo. Simplemente podemos acceder a las salidas, lo cual, evidentemente, para la mayoría de las personas es más que suficiente. Pero a la hora de crear en la comunidad, por ejemplo, no hay ninguna comunidad de esto.

Otro modelo disponible es MidJourney, que es también muy famoso. Creo que es, en mi opinión, el segundo más famoso después de Dall-E. En este caso, se utiliza solo por Discord, en el que tú, como un bot de Discord, puedes pedirle que genere imágenes. En la forma gratuita, creo que se pueden generar unas cuantas, unas 20 o 30, no recuerdo el número exacto, y a partir de ahí son 10 dólares por 200 imágenes, bastante más barato que Dall-E. Al final, por imagen son unas cuantas fracciones de centavos. Por lo tanto, es interesante para la gente que lo necesite. La diferencia con Dall-E es que ha sido entrenado con artistas famosos de todo tipo, tanto artistas clásicos como artistas actuales y artistas, sobre todo, muy famosos de arte digital. Por eso también tiene una tendencia muy clara a sacar siempre una salida muy artística, mientras que Dall-E tiende a sacar algo más realista. Por ejemplo, la pieza de la izquierda es un poco una exageración de lo que saca de normal Dall-E. MidJourney tiende a crear muchas fractales, pero muy artísticas, mientras que también es capaz, como se ve en la pieza de la derecha, de generar algo que es muy imaginativo, por así decirlo, y no existe en la vida real, pero de forma más o menos realista.

MidJourney, también dentro de la aplicación de Discord, se le puede pedir que, una vez que genera cuatro imágenes parecidas a lo que tú le dices, puedes elegir una de ellas, por ejemplo, y aplicarle un algoritmo de super resolución. Por eso es capaz de sacar este tipo de imágenes. Aparte de que Dall-E solo puede generar imágenes de 1024 x 1024 píxeles, MidJourney puede generar imágenes de una resolución arbitraria. Luego, además, si se quiere más resolución, se le puede aplicar un algoritmo de super resolución varias veces hasta poder multiplicar por 8 la resolución y tener una imagen, creo que en 2K. Se puede llegar a tener una calidad suficiente como para hacer un póster o cualquier cosa que tenga buena calidad.

Por último, vamos a ver Stable Diffusion, que es un modelo que es 100% gratis y además está públicamente accesible. Bueno, se me olvidó comentar que MidJourney tampoco tiene acceso público al modelo. En este caso, sí. En este caso, nos dan el modelo, los pesos del modelo, el código suficiente como para ejecutar el modelo y ya está. Nos dicen que hagamos lo que queramos con eso. No está capado de ninguna forma, puede generar cualquier tipo de imágenes que le digamos. No tiene ningún filtro, ya que lo estamos ejecutando de forma local. Otra cosa es que elijamos ejecutarlo utilizando alguna API o utilizando Hugging Face, que en ese caso sí que va a estar capado. Pero si nosotros somos los que nos descargamos los pesos, no tiene ningún tipo de filtro de ninguna forma. Está entrenado con artistas modernos e imágenes de fotografía de imágenes normales y, por la experiencia que tengo, es muy bueno generando, sobre todo, piezas realistas, especialmente imágenes de personas. Como ya estáis viendo en las imágenes, estas son increíbles y cuesta mucho saber que no han sido hechas por inteligencia artificial. De hecho, en la mayoría, bueno, sobre todo en la de la izquierda, a menos que se te diga, es muy complicado. También, conceptos como, por ejemplo, el del medio, que evidentemente no es un retrato, por así decirlo, es un concepto imaginativo y la verdad es que también lo defiende muy bien. Este es el que vamos a usar luego en la práctica, así que tampoco voy a entrar mucho en detalle de cómo funciona. Evidentemente, se llama Stable Diffusion porque utiliza el modelo o la técnica de difusión que hemos comentado anteriormente, que también utilizaba Dall-E. Luego lo veremos, generaremos unas cuantas imágenes utilizando este modelo de aquí.

Stable Diffusion es gratis y está disponible. Lo que ha hecho la gente es crear una comunidad alrededor de este algoritmo o esta red. Por lo tanto, la gente, friki como puede ser cualquier programador muy interesado en la inteligencia artificial y que sepa de esto, coge ese modelo y, por ejemplo, ya que lo puede utilizar gratis desde donde quiera y como quiera, lo que voy a hacer es, por ejemplo, crear una especie de red social como Lexica. Es una red social de gente que le gusta mucho hacer ingeniería de texto para ver qué tipo de imágenes se puede llegar a generar. En esta red social, la gente sube textos y las imágenes que han llegado a generar con ese texto. Entonces, la cosa es que yo puedo ir ahí y coger una imagen que me gusta muchísimo y decir, en plan, a ver qué texto han utilizado para esto y me lo quedo. Pero esta imagen también me gusta, entonces, por ejemplo, puedo combinar ese texto con el que tengo para llegar a uno nuevo y generar algo súper chulo. También he usado esto para si tengo muy claro el estilo que quiero para una generación de una imagen, pero no sé muy bien qué ponerle o no sé exactamente a qué artista se puede atribuir este tipo de estilo o no conozco exactamente qué ingeniería hay que aplicar para escribir el texto exacto para que salga algo que yo quiera, algo muy decente o de muy buena calidad. Entonces, me voy a esta red social, como el que se mete en Instagram, y estoy un rato scrolleando buscando exactamente qué es lo que más se parece a lo que quiero generar. Digamos que también ayuda mucho a la inspiración, a coger ideas y todo eso.

Luego está Dream Studio, que creo que es la más famosa de todas, que es la que nos permite utilizar este modelo de forma online. No solo es la generación de imágenes a partir de texto, sino que también te permite hacer la edición de imágenes. Dream Studio, si no recuerdo mal, tiene una aplicación para poder utilizar el modelo de forma local. Es muy recomendable usarlo si se tiene una GPU muy potente; si no, evidentemente no se va a poder generar imágenes o va a tardar muchísimo tiempo en generar cualquier imagen. Pero digamos que lo bueno de utilizarlo de forma local es que no tenemos el filtro que sí que puede tener Dream Studio en su modalidad online. Creo que la aplicación para ser descargada, si se quiere usar en local, se llama Art Room. Creo que Dream Studio no es la compañía, es la página web, pero la compañía, si no me equivoco, se llama Stability AI. Entonces, si se quiere utilizar online, sí, porque por lo que sea no me apetece descargarme un modelo y usarlo en local, ya sea porque no tengo tiempo o porque no tengo un ordenador suficientemente potente, se puede utilizar en su modalidad online con Dream Studio. Y si, por lo que sea, sí que me interesa quitarme el filtro y que no tenga ningún tipo de límite ni tener que depender de la conexión a internet, nada de eso, Art Room nos permite descargar el modelo y utilizarlo sin ninguna de esas limitaciones.

Por tercera, una tercera creación que he visto por ahí es Plant Mania, que permite crear textos para generar buenas imágenes. Digamos que tú le escribes un texto como lo podría escribir yo o otra persona que no sabe cómo escribir buenos textos para este tipo de modelos. Por ejemplo, yo quiero una imagen de una fotografía de un gato. Entonces, una vez que le das esta idea, te escribe un texto muchísimo mejor y mucho más capacitado para que, una vez que luego lo pongas en Stable Diffusion, la imagen que genere tenga muchísima más calidad que si no le dices solo una cosa en general. Porque una cosa que vemos en estos modelos, sobre todo en este tipo de difusión, es que cuanta más información le des, es mucho mejor. En cambio, si le das muy poca información o algo muy general, las imágenes que genera son de peor calidad. Por lo tanto, aplicaciones como esta son muy útiles porque saben exactamente qué tipo de texto escribirle, en qué formato, en qué orden, porque son cosas que afectan bastante a la generación final de las imágenes. Esta herramienta habrá más, seguramente. Yo sí que conozco alguna más, pero esta es la más famosa y una que yo he usado bastante y he tenido muy buenos resultados, así que la recomiendo.

Por último, Photoshop sí que ha tenido ya implementaciones o ideas o conceptos, no sé si del propio Adobe de Photoshop, pero para implementar este tipo de herramientas en Photoshop. Esto sería, evidentemente, para aquellos que hagan algo de arte digital o edición de fotografía, el poder, por ejemplo, quitar cualquier cosa de la imagen utilizando este tipo de modelos o completar una imagen si falta un trozo. O, por ejemplo, simplemente decirle a esta persona "quiero que tenga el pelo rubio ahora" o "esta persona quiero que tenga un traje en vez de camiseta", y todo eso lo genere de forma automática en segundos y que quede tan realista como si hubiera sido una foto de verdad. Es algo muy interesante que está a punto de venir. También, un poco gracias a esto, para finalizar esta diapositiva, explicar un poco el hecho de que estas ideas han sido gracias a que Stable Diffusion ha sido lanzado al público sin ningún tipo de restricción. Al final, MidJourney o Dall-E no tienen este tipo de ventajas, ya que no han lanzado el código. Claro, el resto de personas que queremos crear algo con esos modelos no se nos permite, ya que este código está cerrado. Pero en cambio, los de código abierto sí que al final crean una comunidad que explota. Evidentemente, esto ha convertido a este modelo en algo muy famoso y muy utilizado, que la gente ha ido mejorando. Al final, en este caso, es la comunidad lo que ha hecho que este modelo sea muy grande.

Por último, simplemente como curiosidad, digamos que por el auge de generación de imagen a partir de texto también han surgido otras ramas, por así decirlo, que surgen de esta nueva línea de investigación o de esta línea muy avanzada de investigación que generan otro tipo de datos sintéticos a partir de texto. Se ha visto un poco también, a partir de imágenes, que también fuera del ámbito del arte y de la creación artística ha habido, por ejemplo, generación de imágenes de rayos X para aumentar datasets de reconocimiento de tumores o de otro tipo de problemas médicos. También se ha utilizado en otro tipo de generación de imágenes, pero que también se puede generar otro tipo de datos con metodologías muy similares a las que hemos visto con imagen, pero por ejemplo en audio, en vídeo, en imágenes sensoriales tridimensionales, etc. En este caso, solo vamos a verlas porque son también más interesantes, más curiosas, pero también más famosas.

En audio, Meta, en la antigua Facebook, creo que creó un modelo hace muy poco, hace creo que un mes, que se llama AudioGen, que como su nombre indica, es un modelo que genera audio a partir de texto. Esto está muy bien porque, claro, imagina que si, por ejemplo, yo soy un productor de sonido y quiero exactamente meter un sample en algún audio que esté creando, va a haber una página web en un futuro no muy lejano en la que yo puedo escribir, yo qué sé, "sonidos de tacones de alguien caminando sobre parque" y "un bebé llorando de fondo", y que te genera un audio de una longitud que tú quieras sobre eso. Pero no solo eso, sino que también este tipo de modelos, no recuerdo si era este modelo en particular, pero que le podemos dar los primeros tres segundos de audio de una canción y te la continúa hasta el infinito, lo cual es una pasada. Evidentemente, a nivel de composición, ya no solo es capaz de hacer lo mismo que hemos visto con imágenes, que es prolongar una imagen hasta el infinito, sino que también con audio. Y, evidentemente, una combinación de ambas, decir en plan "vale, simplemente contexto", decirle "quiero que me genere una canción de cuatro minutos y medio que sea folk mezclado con electrónica y que en el estribillo haya un piano". En un futuro, esto no sé si existe ahora, creo que no, pero no es ninguna tontería y es algo que vamos a ver, si no el año que viene, en el siguiente. Pero me extrañaría mucho que no lo veamos ya a principios del año que viene. Bueno, por contexto, esto se está grabando en diciembre de 2022. Entonces, la gente que lo esté escuchando en el futuro ya me dirá si tengo razón.

Por último, en vídeo, como también como curiosidad, Google creó Fenaki, que es un modelo de generación de vídeo a partir del texto, lo cual ya es muy fuerte porque al principio crearon uno que podía generar unos GIFs de una duración determinada. Tú lo escribías, por ejemplo, "que sea un oso panda volando en el cielo" y te creaba un GIF de unos, creo que eran unos tres o cinco segundos de eso. Pero luego crearon uno que se llama Fenaki, que lo que podía hacer era seguir un guion, de que el texto fuera cambiando y, digamos, que el vídeo iba cambiando de forma adaptativa. Todos los frames tenían sentido entre ellos. Bueno, yo quería entrar para enseñaros luego aquí. Luego podéis estar, obviamente, toqueteando todo lo que queráis, pero un poco por encima, podéis ver que el texto va cambiando. En este ejemplo de aquí, por ejemplo, te dice "un oso panda fotorrealista que está nadando en el océano en San Francisco". Luego dice que se va debajo del agua, luego que ve peces de colores y de repente que es un panda el que está debajo del agua. Entonces, podemos ver cómo está por encima, se sumerge, aparece un pececillo por ahí y de repente se convierte en un panda. Podéis ver que la transición es perfecta, o sea, es extremadamente buena. Lo mismo con otros, como puede seguir en plan de que primero esté buceando en el océano, que luego pase a la tierra a andar por la playa y luego que aparezca, por último, una hoguera mientras que la cámara se aleja de él, hace un plano alejado y cómo sigue exactamente el guion. Con esto, por ejemplo, han creado un vídeo de dos minutos, que es el vídeo más largo de la historia, siguiendo todo este guion de aquí, que podría ser, por ejemplo, la introducción a una película. Las imágenes, como podéis ver, no son extremadamente buenas, así que se ven muchos efectos por ahí, pero bueno, yendo un poco a la diapositiva de las primeras que hemos visto, ya habéis visto que Dall-E, desde la primera versión hasta la actual, tardó un año, menos de un año. Entonces, por lo tanto, no me parece una locura pensar que todo esto, dentro de menos de un año, podremos darle el guion de una película y que pueda generar una película entera en HD o incluso en 2K, en la que solo falte poner la voz de los actores o la música y ya está.

Esto es un poco lo que quería comentar. Ahora sí, si queréis, nos vemos en el Collab y ejecutamos unos cuantos ejemplos para poder ver las imágenes que podemos llegar a generar utilizando este modelo de difusión.

Hola de nuevo, aquí estamos en el Collab. Aseguraos, cuando entréis en Google Collab, que cogéis una sesión con GPU. Yo ya me he conectado de antemano. Lo de la GPU es importante, ya que no es limitante, pero sí que nos va a permitir generar imágenes más rápidas y de mejor calidad que si utilizamos la CPU. Una vez que instalemos las librerías, vamos a loguearnos con nuestra cuenta de Hugging Face. Esto es importante porque vamos a utilizar la librería de Stable Diffusion de Diffusers, que es una que va a cargar un modelo de Hugging Face. Por lo tanto, vamos a tener que tener acceso a la API de Hugging Face, que es súper fácil. Simplemente os tenéis que loguear. Ahora veremos cómo hacerlo. Simplemente vamos a ejecutar de momento las celdas en orden y aquí nos va a pedir que escribamos el token. Va a ser muy fácil. Para crear el token, yo lo voy a tener aquí. Yo tengo mi cuenta de Hugging Face hecha. Simplemente, clicando una vez que tengáis la cuenta hecha, clicando en el link que aparece, os va a salir directamente la clave del access token. Yo simplemente con copiarlo lo tenéis aquí, le dais a login y dicen que el login fue exitoso.

Una vez creado el login exitoso, vamos a cargar el modelo directamente desde Hugging Face. Aquí, muy importante ver, o una curiosidad, es ver que estamos utilizando el modelo de FP16 o de float 16. Normalmente, el modelo es de float, float es un tipo de variable en Python que representa números con decimales. En este caso, utilizamos una precisión de 16 bits. Lo normal es utilizar el modelo de, digamos, que el modelo base es de 32 bits, pero para quitarle un poquito de carga de computación a Google, quitando muy poca, un poquito de precisión a la generación de imágenes, nos va a permitir generar imágenes muy buenas con requisitos de software un poco más ligeros. Vamos a cargar el modelo, va a tardar un ratillo mientras se cargan todos los pesos de la nube. Son 16 archivos. Un poco por hablaros de estos archivos, es tanto el modelo de reconocimiento de texto, el modelo de generación de imágenes y luego también hay un modelo que es de detección de imágenes que no sean "not safe for work" (NSFW). Esto lo que nos va a detectar son, pues lo que hablábamos antes de cuando utilicemos Stable Diffusion a partir de otra compañía, en este caso Hugging Face, están obligados también a tener un filtro de, digamos, de cosas que generaría la gente si no le dieras ningún tipo de filtro. En este caso, es un modelo que detecta si en algún caso generamos una imagen que tenga algún tipo de contenido explícito de cualquier forma.

Disculpad que había habido un problema con la versión de la librería de Diffusers. Ya está solucionado. Vosotros vais a tener el repositorio, perdón, el Collab como toca. Yo tenía un pequeño fallo. Si por lo que sea no os funcionara, os pasa el mismo error que a mí, simplemente ejecutando esta línea aquí, lo que va a hacer es instalar una nueva versión de estas librerías. Por tanto, en principio, el error a mí me ha desaparecido. Lo dicho, hemos cargado el modelo a partir de la librería de Diffusers que se conecta a Hugging Face introduciendo nuestras credenciales de Hugging Face y nos hemos descargado ya la pipeline que contiene una serie de modelos ya en el orden que toca. Entonces, lo único que vamos a hacer es meter la pipeline en la GPU para que, como hemos dicho, tarde menos tiempo en realizar estos cálculos. Entonces, por tanto, esto simplemente diciéndole que la pipeline entera la pase a la GPU.

Ahora, simplemente escribiendo un texto en el que queramos, en este caso el clásico de este tipo de modelos es escribir "este". Para empezar, simplemente lo vamos a llamar a la pipeline para que, dado el texto y un número de pasos, la altura y el ancho de la imagen y un número de aquí. Ahora vamos a ver lo que significa que te saque la primera imagen y luego la mostraremos por pantalla. Lo que hace esto de aquí, el número de steps, es la cantidad de pasos que se hace para quitarle el ruido. Cuanto más pasos, mejor calidad, pero también va a tardar mucho más tiempo. Aunque ponerle más de ciento y pico es demasiado, 50 es la verdad que genera imágenes de muy buena calidad y tarda bastante poco. Entonces, por tanto, eso. Luego, el tamaño 512 por 512 es un buen tamaño para una imagen y luego el guidance scale es, digamos, cuánto quieres que se parezca la imagen al texto que tú le has dado. Si es muy alto, por ejemplo, 14 o 15, te va a generar imágenes muy raras. Si se pone alto, tipo 12, va a tener menos capacidad de imaginación, por así decirlo. Y, evidentemente, también va a pasar lo mismo por el otro lado, pero al revés. Digamos que 7.5 es un buen número para decirle "escríbeme lo que te he dicho, pero también con un pelín de imaginación".

Si le damos, por ejemplo, para esta imagen, vamos a ver que va a tardar 50, que es el número de pasos que le hemos dicho. Va bastante rápido dentro de la generación, como ya hemos visto, como tiene que quitarle el ruido de forma paulatina. Es lo que tiene que ir poco a poco. En este caso, bueno, aparece el astronauta y el caballo, pero no ha sido una buena. Vamos a intentarlo otra vez. Así genera una imagen con un poquito más de sentido. Como hemos dicho, cada vez que lo ejecutamos, lo que hace es coger ruido aleatorio, por lo que cada generación va a ser completamente diferente. Esta es mucho mejor. A partir de ahora, si se quiere, por ejemplo, decir "en el estilo de Monet", vamos a ver si consigue, por ejemplo, captar el estilo de Monet. Bueno, es mucho estilo de Monet, pero seguramente si le damos otra vez a esto, generamos un texto mejor, es capaz de entenderlo. Esto también lo que hemos comentado, también hay un buen trabajo de saber exactamente qué tipo de texto preguntar. Esta imagen, bueno, es un poquito Van Gogh, pero no está mal. Pero eso, que es muy importante saber también cómo escribirle este texto para que genere exactamente lo que queramos. Pero bueno, esto simplemente es una pequeña práctica, una pequeña prueba, una explicación de cómo se puede utilizar de forma gratuita, completamente gratuita, este tipo de modelos y, a partir de ahí, hacer cualquier proyecto que os apetezca. Al final, la imaginación también, en este caso, ahora sí que no hay ninguna excusa, ya que la imaginación es el límite, ya que esto es más fácil de implementar y más chulo no puede ser.

Bueno, muchas gracias por asistir a esta sesión y nos vemos en otra.