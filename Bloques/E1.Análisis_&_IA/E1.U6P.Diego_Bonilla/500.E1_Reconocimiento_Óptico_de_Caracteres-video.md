---
title: Diego Bonilla | Reconocimiento Óptico de Caracteres
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #practica #Reconocimiento Óptico de Caracteres #OCR #Diego Bonilla
lang: es-AR
---
# Reconocimiento Óptico de Caracteres
![[500.E1_Reconocimiento_Óptico_de_Caracteres.mp4]]
[Reconocimiento Óptico de Caracteres](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41948267-p6-2-diego-bonilla-reconocimiento-optico-de-caracteres)
[Reconocimiento optico de caracteres (PDF)](501.E1.Reconocimiento_optico_de_caracteres-230120-152820.pdf)

**Diego Bonilla** | [Linkedin](https://www.linkedin.com/in/diego-bonilla-salvador/) | [Github](https://github.com/diegobonilla98) | [Link Práctica](https://colab.research.google.com/drive/14kHTyVrfLQa2g2znTjS2Yiqkqy54O0Ci?usp=sharing)

Hola y bienvenidos a la práctica de reconocimiento óptico de caracteres. La idea que tenía para esta práctica era ver un poco lo que significa el reconocimiento óptico de caracteres o OCR, así como poner en contexto qué significa esta tecnología, profundizando en en qué tipo de industria se utiliza, cómo funciona internamente y, finalmente, saltar directamente a un caso práctico de una librería muy utilizada. Actualmente, si no me equivoco, es la que más se usa, que es Tesseract. También veremos qué funcionalidades tiene y qué tipo de herramientas dispone para el usuario. Luego, abordaremos casos prácticos.

En los casos prácticos, actualmente me encuentro en un proyecto con un banco español en mi empresa, Cognizant, donde estamos llevando a cabo un caso en el que se utiliza el OCR como la principal columna sobre la que se asienta el proyecto. Tanto mis compañeros como yo lo estamos usando en el día a día, y puedo ofrecerles de primera mano un caso práctico que se está utilizando hoy en día. Por último, veremos una práctica en la que nos iremos a Google Colab. He creado unas cuantas celdas en Python y veremos cómo se ejecutan y las diferencias que tiene el OCR con diferentes imágenes, así como su funcionamiento.

En este caso, como ya hemos comentado, el OCR de la lección ha sido Tesseract, debido a su amplia utilización hoy en día y a lo fácil que es de usar. Bueno, para empezar y poner también en contexto, ¿qué es el reconocimiento óptico de caracteres? Es una tecnología que convierte texto que procede de documentos digitales que no tienen texto extraíble per se, como podría ser un Word o un PDF, y lo extrae. En este caso, el Deep Learning es la metodología más utilizada. No siempre ha sido así, pero actualmente se utilizan sistemas inteligentes para detectar el texto y extraerlo de una imagen. Esto puede incluir documentos escaneados con un escáner o PDFs que no tengan texto extraíble.

Los usos principales del reconocimiento óptico de caracteres son crear flujos de trabajo automatizados digitalizando documentos de diferentes unidades comerciales. Por ejemplo, podemos ver aquí un caso donde se utiliza OCR para identificar la matrícula de un coche. Sin embargo, no solo se limita a la matrícula, sino que también es capaz de detectar entidades, como el estado en el que se encuentra el vehículo, la matrícula y otra información adicional, como la marca. Así que, en el OCR, no solo se trata de saber dónde está el texto y ser capaz de reconocerlo, sino que también hay un fuerte reconocimiento de entidades y reconstrucción de documentos, es decir, saber qué papel juega ese texto en el documento.

Por lo tanto, todo esto se puede integrar en un flujo de inteligencia artificial o en un flujo de información que proporciona una gran cantidad de datos. También es importante tener en cuenta que todo este reconocimiento de entidades se realiza de forma anónima; no es necesario que haya alguien detrás etiquetando estos documentos. Por lo tanto, no solo se elimina una gran cantidad de trabajo y se automatiza, sino que también se vuelve más seguro, ya que se elimina el factor humano en este caso. Así, todo lo que implica el escaneo automático de DNIs, pasaportes o documentos de extractos bancarios se convierte en algo completamente anónimo, ya que no hay nadie detrás en este proceso.

Además, el reconocimiento óptico de caracteres también puede complementarse con otros sistemas, como el reconocimiento de entidades, como estamos viendo en las imágenes que reconocen tanto el texto como qué parte forma, si es el título, el precio o una tabla. La reconstrucción se refiere a la capacidad de restablecer el orden lógico de los documentos o las partes de un documento, y por último, la clasificación, que podría ser, por ejemplo, identificar si un documento es una matrícula, un DNI o un pasaporte, abarcando todo lo que implica esa parte.

Más específicamente, un motor que se encarga de realizar este tipo de trabajo es Tesseract, que actualmente es el más utilizado, ya que es muy fácil de usar, además de ser de código abierto y gratuito. Tesseract es de HP; fue creado en HP, pero luego fue patrocinado por Google, como hemos mencionado, y es open source. Actualmente, soporta 116 idiomas y 37 alfabetos diferentes. No solo detecta el texto, sino que también es capaz de devolver la jerarquía del texto. Esto significa que puede indicar, por ejemplo, si un texto depende de otro anterior, si está dentro de una tabla, si uno es el título y el resto es contenido que lo sigue. Esto es algo que puede detectar.

También es capaz, dado que detecta varios idiomas, de leer de izquierda a derecha, así como de derecha a izquierda y en diferentes formatos de texto. Por ejemplo, puede manejar texto en columnas, texto que forma un círculo o tablas, y muchas otras configuraciones que se le pueden dar a Tesseract para que detecte tipos especiales. Sin embargo, Tesseract tiene un problema: está entrenado, digamos que es muy eficaz en documentos, como podemos ver en la imagen, pero no es capaz de detectar texto "salvaje", que es el término que se utiliza. Esto se refiere a texto que, por ejemplo, se obtiene al fotografiar un coche y que Tesseract identifique el texto de la matrícula; le costará mucho trabajo. Necesita un documento que esté recortado, rotado y que se asemeje mucho a un PDF para detectar el texto de manera fiable.

Por lo tanto, incluso los propios desarrolladores de Tesseract recomiendan aplicar un preprocesamiento a la imagen antes de pasarla por el motor Tesseract. Este preprocesamiento puede incluir la mejora del contraste para que la diferencia entre el fondo del documento y las letras sea muy marcada y se diferencie claramente. También puede incluir una corrección de perspectiva. Si, por ejemplo, hemos tomado una foto del documento, se puede ajustar para que las cuatro esquinas del documento se agranden y se elimine esa perspectiva, de modo que parezca que lo hemos escaneado. Algunas aplicaciones móviles realizan esto automáticamente, pero si no se lleva a cabo este preprocesamiento, Tesseract no será capaz de detectar casi nada de texto o le costará mucho.

En el mismo sentido, también se presenta el problema de la rotación. Si le proporcionamos un documento que está rotado, tampoco será capaz de detectarlo, ya que asume que el texto está completamente recto una vez que le damos la imagen. Esto es un poco lo que hemos visto, y también se explica otro tipo de preprocesamiento que se realiza, como la reducción de ruido, que es una técnica bastante sencilla de aplicar. Básicamente, consiste en reducir el ruido de la imagen. Se pueden aplicar filtros que disminuyen el ruido de la imagen sin alterar demasiado o intentando no eliminar muchas partes del texto.

En la imagen que vemos a continuación, se presenta un flujo de procesamiento de una imagen con un OCR. La imagen sería la entrada, y primero se realizan algunos preprocesamientos a la imagen que hemos comentado, como aplicar un filtro, eliminar la rotación, convertir a blanco y negro o mejorar el contraste, y eliminar el ruido. Luego, internamente, en este caso de Tesseract, se lleva a cabo toda esta parte automáticamente. Simplemente tenemos que invocar a Tesseract a partir de aquí. Internamente, Tesseract lee las líneas, detecta cuántas líneas hay, y a partir de las líneas, las palabras, utilizando los espacios, y luego los caracteres que componen cada palabra. En este punto, se detecta cada carácter, se realiza un posprocesamiento utilizando conocimiento interno de gramática y se devuelve el texto.

Luego, el texto también puede requerir una etapa de posprocesamiento, ya que evidentemente no se puede adaptar a todos los casos que existen, y puede que necesites ajustarlo para cada caso y cada tarea que quieras realizar con la salida, adaptándola a lo que estés tratando o a tu caso específico. Estos son algunos parámetros que le podemos dar a Tesseract, como lo que hemos comentado, que puede detectar el texto en muchas formas, tanto de izquierda a derecha como en columnas, en partes de un círculo o en tablas. Esto se refiere al modo de segmentación de página, o PSM, que es un parámetro que le podemos proporcionar. Podemos indicarle que sea una sola columna, que sea un bloque uniforme de texto alineado verticalmente, que simplemente sea una palabra, que sea una línea de texto que forma un círculo o que sea simplemente un carácter. Los modos 11 y 12 son un poco más generales, ya que consideran todo el texto posible sin que forme parte de ninguna jerarquía o segmentación de página. Todas estas son más opciones que nos ofrece Tesseract para poder adaptarnos a nuestro tipo de problema.

Como he mencionado anteriormente, en mi empresa actual, Cognizant, estamos llevando a cabo un proyecto muy grande de inteligencia artificial pura, de lectura, escaneo y reconocimiento de documentos para una empresa de banca española. Como hemos comentado, este es el proyecto más grande de Inteligencia Artificial en el sur de Europa, y utilizamos las salidas del OCR para extraer información de cada documento. Las imágenes que aparecen son, evidentemente, imágenes obtenidas de internet, no son del proyecto actual. También podemos unir diferentes documentos que provienen de distintas partes y unirlos a un mismo usuario o a una misma empresa. Luego, podemos dirigir cada documento a donde corresponde, porque sabemos lo que contiene y cuáles son los pasos siguientes para ese documento. Además, esto es algo que se puede escalar, no solo a la clase que estamos viendo ahora, sino que también somos capaces de escalarlo a otro tipo de tareas, todo esto partiendo de un sistema que detecte los caracteres, es decir, que detecte el texto de los documentos y la jerarquía también de los documentos de manera eficaz.

Efectivamente, esto es algo que interesa mucho actualmente, específicamente, pero no se limita solo a la banca, porque como hemos mencionado, es algo que se puede automatizar muy fácilmente, eliminando una gran cantidad de gastos humanos en recursos. Además, al eliminar esa parte humana, también se reduce considerablemente la latencia y se aumenta la seguridad. En este caso, ya hemos visto que todo el reconocimiento de caracteres y demás son algoritmos, por lo que es completamente anónimo y no se retiene ninguna información que, tal vez, un humano sí podría tener.

En el resto de la sesión, vamos a ver el Colab. Aquí tenéis el enlace que colocaré cuando estemos todos. En esta práctica, lo que he preparado es, digamos, un par de formas de leer el texto de un documento, tanto si el documento es un PDF como si es una imagen. Veremos que si es un PDF, a lo mejor tiene texto extraíble. Entonces, accederemos a la metainformación del PDF con una librería llamada PDFminer. Así podremos leer el PDF y extraer el texto. Lo bueno de esta librería es que, si hay texto extraíble, también puede ser capaz de extraer las imágenes, por ejemplo. Esto nos proporcionaría mucha información. Además, junto al texto, te devolvería su posición, así como el texto y otra información relevante. Es muy útil, ya que muchas veces no necesitamos un OCR si los documentos que nos proporcionan son en PDF. Sin embargo, también hay que tener en cuenta que no todos los PDFs tienen texto extraíble.

Para el resto de los casos, tanto imágenes como PDFs sin texto extraíble, instalaremos Tesseract en Colab y veremos rápidamente algunos ejemplos de casos en los que funciona muy bien, así como casos en los que no funciona tan bien, cómo corregir esos casos y, tal vez, aplicar alguna técnica de mejora, y ver cómo afecta eso a la salida del modelo. Nos encontramos aquí en Colab; ya he preparado todo por partes, está separado por los diferentes ejercicios que vamos a realizar. Así que, nada, todas las celdas se ejecutarán de arriba hacia abajo. Esto ya lo he ejecutado yo, pero vosotros tendréis que instalar las dependencias; seguramente os aparecerá un mensaje que tendréis que leer y hacer un reinicio de la rutina de Colab, así que le dais y ya estaría.

Aquí es donde vamos a importar todas las librerías. Si no estáis familiarizados con Python, no pasa nada; simplemente, todas las funciones que vayamos a usar deben estar dentro del programa, y si no, no sabréis dónde buscarlas. Todas las imágenes y los archivos y PDFs que usemos en esta demostración no estarán en Colab, ni tendréis que acceder a ningún sistema de archivos. Simplemente, utilizando la función wget, podemos acceder a cualquier documento que esté en la web y descargarlo para incorporarlo automáticamente al sistema de archivos que viene con él. En mi caso, ya he ejecutado algunas de estas celdas. Este, por ejemplo, es el PDF, si no recuerdo mal, el de "Atención". Así que nos lo descargamos y parece que todo está correcto.

Aquí, como es un PDF, podéis ver que puedo acceder al PDF y seleccionar el texto. Entonces, ¿por qué no intentar acceder a la metainformación que tiene el propio PDF y extraer la información que contiene? Porque a lo mejor no tengo que recurrir a un OCR si puedo acceder directamente a esa información. Aquí, comentándolo línea por línea, cargaremos el archivo; en este caso, utilizaremos la función open de Python y le daremos el flag de read bytes para que lo lea como un buffer de bytes. Utilizaremos la librería PyPDF2. Entonces, le daremos el objeto que ha leído los bytes a la función FileReader. En este caso, ya nos devolverá un objeto de la librería. En ese objeto, podemos acceder a sus atributos, como por ejemplo el número de páginas, pero también podemos acceder a la información que tenga ese documento. En este caso, lo guardaremos en una variable llamada "info", y luego lo mostraremos por pantalla. De esta forma, el número de páginas será el número de páginas, el título será el título, el autor será el autor, ¿vale? Así iremos viendo qué información podemos extraer directamente del PDF y lo comprobaremos con el propio PDF para verificar si es correcto. Así que vamos a ejecutarlo.

Ha sido muy rápido, evidentemente. Vemos que el número de páginas son 11 y podemos ver aquí que efectivamente tiene 11 páginas el documento. El título, o "title", es "Attention is all you need", efectivamente así es el título. El autor, pues varios autores que podemos ver que están aquí todos referenciados. El "subject", esto ya no lo pone en el PDF en sí, sino que es parte de la información que tiene el documento. El creador vemos que no está definido y el productor tampoco. Creo que automáticamente se asigna a la librería que hemos utilizado. Pero bueno, el resto de la información es muy útil, tanto el autor como el título del documento, como el número de páginas. Personalmente, la información que utilizo en el proyecto en el que estamos ahora, sobre todo, es el número de páginas para poder iterar sobre ellas.

Antes de comentar que hay librerías como PyPDF2 que extraen la metainformación del texto, también hay otras que convierten los PDFs a imágenes para luego poder procesar los textos. Por otra parte, si no hay suficiente texto extraíble en los documentos o no hay texto directamente, aquí voy a comprobar antes que se esté grabando todo. Un momentito, vale, disculpad. Aquí es donde vamos a utilizar justo la variable de número de páginas, que como hemos visto nos devuelve un número entero, en este caso 11, y vamos a hacer un bucle for que significa que, para cada página, vamos a iterar sobre ellas y leeremos el contenido. Disculpad el corte, he visto un pequeño fallo que había en el código y lo he corregido. Fallos del director. Entonces, una vez creado el objeto de la página utilizando el objeto que nos ha devuelto antes la función y la función getPage, le damos como argumento el número de la página y nos devolverá un objeto que contiene información de esa página. En este caso, nos interesa el texto, así que llamaremos sobre ese objeto a la función extractText. En mi caso, he decidido imprimir un salto de línea y unas cuantas barras para separar las páginas, que nos imprima también el número de la página y luego el contenido de la página. Después, cerraremos el PDF cuando hayamos terminado de leerlo. Vamos a ejecutarlo.

Vale, pues podéis ver que se ejecuta efectivamente todo. Entonces, vamos a ir al principio. Vale, pues tampoco vamos a leerlo aquí, pero bueno, página 0, vale, empieza a contar desde 0, así que acabará en la 10. Pues eso, "Attention is all you need", los diferentes autores, el abstract, vale, todo esto se corresponde a esto de aquí y digamos que ya tenemos un texto que es modificable por un ordenador. A partir de aquí, puedo buscar, por ejemplo, palabras clave, como haremos a continuación, y obtener información diferente que antes, evidentemente, no podía. Vamos a limpiar este output.

Ahora vamos a ir con imágenes. En este caso, evidentemente, una imagen no contiene texto extraíble ni legible por una máquina a priori. Entonces, vamos a decodificar ese texto, vamos a leerlo utilizando Tesseract para poder acceder a modificar ese texto o ver qué tal lo ha hecho. Esto es una pequeña función que utiliza la librería Matplotlib para mostrar imágenes por pantalla. No voy a profundizar en qué significa esto, pero básicamente, como estamos utilizando OpenCV, que es una librería de visión por computadora para leer las imágenes como matrices de números, OpenCV las lee en formato BGR (blue, green, red), pero luego, para mostrarlas por imagen, las presenta en RGB. Por lo tanto, hay que cambiar los colores, y esto es lo que significa. No mostramos el eje porque, en este caso, estamos mostrando imágenes y lo mostramos en pantalla.

Una vez ejecutada esta función, para que la meta en memoria, vamos a descargarnos la primera imagen, que es esta de aquí. Es una imagen bastante clara, se lee todo bastante bien, no es un escaneo ni nada, es una imagen completamente digital. No tiene muy buena resolución, pero es más que suficiente, así que esto es un caso perfecto. Aquí cargaremos las imágenes, utilizaremos la función de OpenCV para leer la imagen, la mostraremos por pantalla y luego le daremos esta configuración a Tesseract. La configuración, si lo recordáis de lo explicado en el PDF, es el Page Segmentation Mode 11, que significa que el PDF extrae el texto que pueda sin preocuparse de cómo esté distribuido por la imagen, por así decirlo, y luego le especificaremos que el idioma es el inglés. Aquí ya podéis hacer modificaciones para ver qué pasaría si le quito el idioma inglés, si funciona peor o mejor. No siempre funciona mejor, ni siempre funciona peor; depende de cada caso.

Lo que sí es obvio es que, si queréis detectar caracteres especiales de un idioma, siempre es bueno indicárselo, porque si no, será muy difícil o directamente no los detectará. Directamente, llevamos a la función, que es una función muy sencilla de Tesseract, donde le damos la imagen y utilizamos imageToString, que, como indica la función, convertirá una imagen a un string de palabras o texto. Ejecutamos, nos muestra la imagen y aquí nos muestra el texto que ha extraído. Podéis comprobarlo vosotros, pero está perfectamente extraído; no se ha equivocado en ninguna parte. Efectivamente, es un texto muy fácil y nos ha extraído sin mucha dificultad. Sin embargo, no solo devuelve el texto; hay otra función llamada imageToData, a la que le volvemos a dar la imagen y nuestra configuración como entrada, y vamos a ver qué devuelve este tipo de función.

Aquí vemos que devuelve un dataframe en el que nos proporciona información de nivel, número de página, número de bloque, número de línea, número de párrafo, número de palabra, left, top, width, height, confidence y text. Esto es un dataframe o un diccionario que nos devolverá mucha información por cada palabra. ¿Vale? Veis que la palabra "this", que es la primera de todas, nos indica que está un poco a la izquierda, de derecha a izquierda, y la confianza es del 96%, lo que significa que está muy seguro de que esa es la imagen. Vamos a abrir todo lo que hay aquí, porque esto ya son detalles internos de párrafos o caracteres especiales. La siguiente información nos dará la posición que tiene ese texto en la imagen, los píxeles en los que se encuentra ese texto. Todo esto nos indicará que fue parte de un nivel número 5, el número de página número 1, el bloque 1, el párrafo 1, y nos proporcionará información sobre la jerarquía que contiene más información sobre el texto, indicando que no solo ha detectado "this", sino que también te dice exactamente dónde está localizado y qué jerarquía forma con respecto al resto del texto. Vamos a limpiar un poco este output.

Ahora vamos a probar con una imagen de peor calidad. Vamos a ver qué imagen es. Ya nos lo he impreso aquí por pantalla. Es una imagen que podemos ver que también es texto, pero la calidad es mucho menor. Lo primero es que está un poco rotada, además no hay mucho contraste; no era como la imagen anterior, donde el negro y el blanco se diferenciaban mucho. Aquí hay unos cuantos grises en medio y, además, la calidad también es muy baja. Vamos a ver si es capaz de detectar. Bueno, aquí no lo he mencionado, pero volvemos a llamar a la misma configuración y a la misma función que antes, nada nuevo, e imprimimos el texto. Vemos que sí es capaz de detectar las primeras líneas, pero luego la palabra "dog" se ha perdido. Hacer un análisis de esto es un poco complicado, porque evidentemente todo esto es de alto nivel, pero por alguna razón no ha detectado la palabra "dog". En este caso, si era una abstracción casi perfecta, vemos que ha fallado en dos caracteres. También podemos ver la posición y la jerarquía que ha estudiado el texto, y podemos observar que, si antes esta jerarquía era la misma, ahora nos devuelve caracteres. Ups, disculpad, voy a cargar la imagen un segundo. La confianza que nos daba del texto era del 95%, 96%, 96%, 96%, lo que indica una precisión muy alta. Vemos que ahora esa nueva confianza se convierte, por ejemplo, en muchos 96, pero en la palabra "D", ya ha bajado a 80, y directamente "DOG", la "O" y la "G" y el punto no los ha detectado. Así que podemos ver que es bastante sensible a cualquier deformación de la imagen.

Ahora vamos a probar con una imagen manuscrita, que para nosotros es muy fácil de leer, pero vamos a ver si Tesseract es capaz de detectar texto manuscrito. Vale, y vemos que falla bastante. Esto es porque, como hemos mencionado en la presentación anterior, todo lo que se salga de documentos, por así decirlo, de uso, no es capaz de detectar, ya sea por rotación, mala calidad, manuscritos o cualquier tipo de letra que no sea la típica de un documento. Así que, todo eso no es capaz de detectarlo. Ha hecho su mejor esfuerzo, pero no ha sido capaz.

Vale, ahora vamos a hacer un pequeño ejercicio, por último, para finalizar la extracción de entidades en la que vamos a leer esta página y la vamos a imprimir. Es un documento legal, extraído de internet, que no significa nada, está en formato imagen, así que no podemos extraer ningún texto a priori. Pero digamos que quiero tener un sistema que automatice esto, que los usuarios me puedan enviar estos documentos en foto y yo pueda acceder al nombre, al número de teléfono y a dónde reside. Entonces, vamos a crear un pequeño flujo de imagen en el que hagamos exactamente esto. ¿Cómo haré ese flujo? Pues cuando extraiga el texto de la imagen, podremos ver que tanto la palabra "resident name" como "fake person" en este caso están muy juntas, es decir, están seguidas en la extracción del texto. Entonces, si busco la palabra clave "resident name", me llevará automáticamente al nombre de esa persona.

Lo primero, como hemos dicho, es convertirlo a texto. Aquí nos lo ha convertido todo a texto. El texto que ha detectado no es muy bueno en este caso, porque la calidad del documento no es muy buena para empezar, pero lo que es necesario sí que lo ha extraído bien. Ahora vamos a realizar unas cuantas funciones de Python. Voy a explicarlas un poco por encima. Esto nos ayudará a eliminar caracteres especiales, como saltos de línea o diferentes caracteres del propio string de Python. Luego, separaremos el texto por saltos de línea. Disculpad, el string no eliminaría saltos de línea, efectivamente. Aquí separaremos el texto por saltos de línea, es decir, cada una de las entidades, cada una de las líneas, formará parte de un índice diferente en una lista. Luego, cada uno de los índices de esa lista lo separaremos por espacios. De esta forma, ya tendremos una lista que contiene todas las palabras del documento. Esto simplemente es para convertirlo a la raíz de palabras.

Ahora vamos a filtrar, vamos a eliminar todas las palabras que no tengan texto, porque, por alguna razón, a lo mejor ha habido dos espacios seguidos, así que no hay texto en esos espacios. Vamos a ver cómo se limpia el texto. Como hemos dicho, ya tenemos un array con todas las palabras, un poco limpio. Entonces, quiero extraer, como hemos comentado antes, el nombre, el número de teléfono, la ciudad y el código postal. Podemos ver que en la imagen tenemos que buscar "resident name", "phone number" y también "zip", que estaría aquí. Así que vamos a hacer eso mismo, buscarlo, y la función que tiene para buscar es "index", que es una función de Python. Entonces, buscaré "name" y me quedaré con el índice de este array que contiene la palabra "name", y lo mismo con el resto. Ahora, sabiendo que el nombre me espero que tenga dos palabras, que con el teléfono me espero a lo mejor dos palabras también, con la ciudad de residencia también me espero unas cuantas palabras predefinidas y el código postal es solo una palabra, puedo acceder a esa información dentro de la lista. En este caso, el índice también está un poco hecho a propósito para que funcione bien, porque he comprobado que funciona correctamente. Pero no he extraído esa información de forma automática. Es decir, he proporcionado una imagen de entrada, completamente sin texto extraíble, y tendría que haber una persona detrás accediendo a esa información y leyéndola. Pero se le ha dado a Tesseract y, con un poco de programación básica en Python, he sido capaz de extraer las entidades que quiero. ¿Vale? Este es un ejemplo muy básico. Efectivamente, no es la mejor forma de hacerlo, pero es un ejemplo que demuestra la capacidad y la facilidad que tienen estos algoritmos, con muy pocas líneas de código, para crear un sistema automatizado de extracción de esas entidades.

Esto sería el laboratorio y con esto concluye la práctica de OCR. Cualquier duda o sugerencia que tengáis, me podéis escribir, como hemos comentado, tanto en LinkedIn como en GitHub, y también dentro de LinkedIn está mi correo electrónico si os resulta más conveniente.