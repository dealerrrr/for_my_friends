---
title: Deep Learning | Rafa López
URL: https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez
Tags/Keywords: #Especialidad 1 #E1 #Analisis e IA #IA #AI #Inteligencia Artificial #E1U6 #Deep Learning #Rafa López
lang: es-AR
---
# Deep Learning
![[494.E1_Deep_Learning.mp4]]
[Deep Learning](https://app.web3mba.io/courses/take/especializacion-1-analisis-IA/lessons/41866162-u6-1-1-deep-learning-rafa-lopez)

Para definir lo que es la inteligencia artificial, primero hay que describir un poco lo que entendemos por inteligencia, que asociamos a nosotros como seres humanos. A mí me gusta definir la inteligencia normal, la natural o la que poseemos, como una combinación de diferentes aptitudes, como pueden ser la autoconciencia, la creatividad, la resolución de problemas, el pensamiento lógico, la planificación o incluso el aprendizaje. Y para, digamos, de alguna manera... entender la inteligencia artificial, tenemos que saber que, a día de hoy, las técnicas que nosotros llamamos inteligencia artificial son un subconjunto de tareas que son muy efectivas en el aprendizaje, es decir, a partir del conocimiento que le aportan los datos, aprender a resolver tareas.

Por tanto, la inteligencia artificial, desde un punto de vista algorítmico-tecnológico, no es una inteligencia general como la que tenemos nosotros, que es capaz de resolver todas estas tareas que he mencionado o que tiene todas estas características que he mencionado previamente. En cambio, es una serie de algoritmos que logran aprender a resolver tareas a partir de datos de entrada, y generalmente, una gran cantidad de estos datos. De hecho, estas técnicas que aprenden a partir de datos se engloban en un campo que se llama Machine Learning, el cual consiste en diferentes tipos de algoritmos y sistemas informáticos que son capaces de aprender a resolver tareas a partir de datos de entrada. Estos datos pueden ser supervisados, es decir, con la respuesta asignada al problema que buscamos, o no supervisados, es decir, datos que no tienen a priori la respuesta al problema que buscamos, pero el sistema aprende a encontrar respuesta a ese problema evaluando los patrones de estos datos.

Concretamente, lo que ha permitido que la inteligencia artificial se expanda de forma exponencial en los últimos años ha sido un subconjunto de técnicas del Machine Learning que se llaman Deep Learning. Estas técnicas se basan en la utilización de algoritmos específicos basados en redes neuronales artificiales para realizar estas tareas de aprendizaje a partir de datos. Por tanto, cuando hablamos de inteligencia artificial en las innovaciones que hemos tenido en los últimos años, en realidad estamos hablando de técnicas de Deep Learning, de esta serie de algoritmos basados en redes neuronales que nos permiten aprender a automatizar tareas a partir de grandes bases de datos, que generalmente están supervisadas o etiquetadas.

En realidad, tanto los algoritmos de Machine Learning como los de Deep Learning son algoritmos que están desarrollados para aprender a resolver tareas a partir de datos. Sin embargo, los algoritmos de Deep Learning no son diferentes, sino que son una subclase del Machine Learning. Estas subclases están basadas en redes neuronales, de las cuales hay diferentes tipos que posteriormente veremos. Este tipo de algoritmos, basados en redes neuronales artificiales, son una inspiración o una imitación algorítmica, o incluso una simplificación algorítmica, de las redes neuronales que funcionan en nuestro cerebro. Aprenden, es decir, son algoritmos que se basan en estas redes para la resolución de tareas de forma automática.

Aunque ahora mismo estamos muy habituados a escuchar hablar de inteligencia artificial y parece un término relativamente nuevo que empezó a popularizarse a partir del año 2012, en realidad, las primeras redes neuronales surgieron en la década de los 50 y 60. En ese momento, la tecnología no estaba suficientemente avanzada como para poder sacarle el máximo provecho que estas técnicas pueden proporcionar. Por dos motivos principales: estas técnicas requieren de alto poder de computación y además requieren de una gran cantidad de datos. Por tanto, estos dos factores, tanto la capacidad de computación como la cantidad de datos, han crecido de forma exponencial desde los años 60, porque la digitalización se ha ido expandiendo en todos los dominios. Así, en 2012 se llegó a un punto en el que tanto la capacidad de cómputo como la cantidad de datos disponibles era suficiente para que estas redes neuronales funcionasen. A partir de entonces, hemos podido sacar su máximo potencial.

Desde su surgimiento hasta la actualidad, ha habido diferentes intentos, o diferentes hitos, mejor dicho, en el campo de la inteligencia artificial, en los que han ido, progresivamente, superando a los seres humanos en diversas tareas. Por ejemplo, en el año 1996, Deep Blue de IBM derrotó a Garry Kasparov en ajedrez, quien era el campeón mundial en ese momento. Por lo tanto, en esa tarea concreta, la inteligencia artificial ya nos lleva muchos años superando. También podemos ver hitos muy recientes, como por ejemplo, en el año 2016, una inteligencia artificial, concretamente desarrollada por DeepMind, que es una compañía adquirida por Google, derrotó al campeón mundial del Go, un juego mucho más complejo que el ajedrez en términos de árboles de posibilidades. Además, en el año 2019, el algoritmo de OpenAI derrotó al mejor equipo del mundo en el videojuego Dota 2, que requiere coordinación de diferentes jugadores y mucha intuición, lo que demostró otro hito. Incluso en 2020, otro algoritmo de DeepMind resolvió un problema que hasta entonces no había podido resolverse: predecir la forma que adquieren las proteínas en 3D a partir de una secuencia de instrucciones genéticas, lo que, según los expertos en el campo, será extremadamente prometedor para hacer nuevos avances.

En general, a partir de estos hitos y esta evolución, hemos visto cómo, progresivamente, las compañías más grandes del mundo han dejado de ser compañías energéticas o dedicadas a la industria, y han transicionado a ser compañías basadas en los datos y, por lo tanto, en la inteligencia artificial. Ahora, las compañías más grandes del mundo son Google, Facebook, Apple, etcétera, compañías puramente basadas en datos, software e inteligencia artificial. Se prevé que esta evolución exponencial del valor que la inteligencia artificial va a aportar al mercado y a los consumidores seguirá creciendo. Por tanto, este campo es de extrema relevancia para su estudio, para no quedarse atrás en esta carrera por optimizar procesos y digitalizar las compañías.

El Deep Learning es un campo que ha crecido mucho en los últimos años; se han desarrollado muchas nuevas tecnologías, y cada vez es un campo que, de hecho, está adquiriendo un ritmo vertiginoso, porque cada vez se dedican más recursos y, por tanto, más novedades aparecen de forma más rápida. Por tanto, es un poco difícil mantenerse al día con todo lo que va surgiendo. Los tipos principales de redes neuronales que podemos encontrar, o los que están más establecidos y que ya han demostrado resultados muy significativos en los últimos años, son las redes neuronales totalmente conectadas, que serían las redes neuronales originales, basadas en neuronas individuales que se conectan a través de distintas capas. Luego tendríamos las redes neuronales recurrentes, que se desarrollaron específicamente para trabajar con datos de series temporales, es decir, datos que tienen una dependencia temporal entre unos instantes del dato y otros.

Después, tendríamos las redes neuronales convolucionales, diseñadas específicamente para trabajar con datos que están ordenados en dos o tres dimensiones. También tendríamos una subtipología de entrenamiento de redes neuronales, que serían las redes neuronales generativas adversarias. Estas redes, principalmente convolucionales, tienen la característica de que sirven para generar datos de forma artificial que son muy realistas. Finalmente, tenemos un tipo de red neuronal que ha surgido en los últimos años, los transformers, que son un tipo de red neuronal que, a través de la ingesta de grandes cantidades de datos y el desarrollo de modelos con muchos parámetros, son capaces de realizar tareas como la generación de texto y de imágenes con un realismo sin precedentes. Este tipo de redes son capaces de trabajar tanto con entradas de texto como con entradas de imagen, y, por lo tanto, pueden combinar información de ambos mundos: el del lenguaje natural, es decir, el texto, y el de la imagen. Son muy efectivas trabajando con datos no estructurados.

Una neurona es un núcleo de cálculo muy simple. Simplemente es una función que tiene como entrada un número o varios números y aplica una función de activación específica, que puede ser determinada por el programador, para obtener un número diferente al de entrada. Esta neurona, como se mencionó, tiene una o varias entradas. Estas entradas tienen asociada un peso. Por lo tanto, si una neurona tiene diversas entradas, como por ejemplo en un problema de predicción de precios de casas, estas variables de entrada a una neurona específica podrían ser diferentes características de la casa, como el número de habitaciones, los metros cuadrados, el número de baños, etcétera. Esta información entra a esta neurona ponderada por unos pesos, que es el conocimiento que adquiere la red neuronal, es decir, qué importancia le da a cada una de las características de entrada. Cuando combina todos estos números con estos pesos, les aplica una función de activación que convierte este número en otro, o puede ser el mismo, dependiendo de la función de activación que se utilice y el número de entrada. Esto, al final, introduce en el sistema no linealidades que permiten adaptarse a entornos de datos muy complejos.

Sabiendo ya lo que es una neurona, podemos ver cómo se crea una red neuronal. Es muy simple: una red neuronal es un conjunto de neuronas ordenadas por capas. En la capa 1 puedo tener 10 neuronas, en la capa 2 puedo tener 100 neuronas, en la capa 3 puedo tener otras 100 neuronas, etcétera, hasta la capa de salida, que es la que nos dará la respuesta a nuestro problema, que en este ejemplo sería el precio de la casa que queremos predecir. Por lo tanto, lo que vemos es una estructura por capas, y al final lo que vamos a conseguir con esto es que cada neurona de una capa esté conectada con todas las neuronas de la capa anterior. Esto nos permite transformar los datos de entrada, estas variables relacionadas con una casa, en el ejemplo que estamos poniendo, para obtener relaciones de muy alto nivel, muy complejas, que quizás no son intuitivas, pero que nos llevan a una mejor predicción del precio de la casa que estamos evaluando.

Las redes neuronales recurrentes, por su parte, están diseñadas para entender o procesar información de datos que tengan una relación temporal. Por ejemplo, en una frase, el orden de las palabras puede cambiar el significado de la misma. Por tanto, las redes neuronales recurrentes están diseñadas para trabajar exactamente con este tipo de datos, intentando entender cómo se relacionan unas palabras con otras y estableciendo concretamente qué significado aporta también la posición de una palabra en una frase. Estas redes neuronales procesan secuencialmente las palabras de entrada, teniendo en cuenta, a la hora de analizar cada una de las palabras, cuál ha sido la palabra anterior y cuál ha sido el resultado del proceso de la misma.

Como aplicaciones de las redes neuronales recurrentes, tenemos, por ejemplo, los sistemas de traducción. De hecho, hace unos pocos años, el sistema de Google Translate mejoró drásticamente en muy poco tiempo. ¿Por qué fue? Porque se pasó de utilizar técnicas más convencionales para la traducción a utilizar técnicas basadas en redes neuronales. Esto permitió que los resultados de traducir un texto, del español al inglés, por ejemplo, sean extremadamente robustos y prácticamente no se cometan incoherencias textuales. Cada vez que se incrementan los datos y los modelos son más grandes, este error y esta, digamos, falta de similitud con lo que diría una persona, se van reduciendo paulatinamente.

También tenemos, por ejemplo, el reconocimiento de voz, que no es más que la conversión de audio a texto y la posterior interpretación de este texto. Así como el texto escrito, el audio también es una secuencia temporal, y por lo tanto, las redes neuronales recurrentes pueden utilizarse para transcribir o convertir este audio en una cadena de texto que sea posteriormente interpretada por otra red recurrente para emitir acciones específicas en los asistentes de voz.

Por otra parte, tenemos las redes neuronales convolucionales, que están específicamente diseñadas para analizar datos organizados en dos o tres dimensiones, como es el caso de las imágenes. Al final, una imagen es una matriz bidimensional de píxeles ordenados en filas y columnas. Las redes neuronales convolucionales nos permiten utilizar una operación que se llama convolución, la cual nos permite aplicar diferentes filtros a las imágenes para poder analizarlas, procesarlas y extraer características de las mismas, tanto de bajo nivel al principio, es decir, características como cambios de tono en la imagen, colores homogéneos, bordes, etcétera, como características de alto nivel. Si, por ejemplo, estamos entrenando un sistema de detección de caras, las características de más alto nivel serían los ojos, la nariz, la boca, etcétera. Es decir, la red neuronal convolucional aprende a ver cómo se combinan todas estas características de bajo nivel, como bordes y contornos, para convertirlas en características de alto nivel, como boca y ojos, y seguir procesándolas hasta obtener representaciones de lo que sería una cara.

Por lo tanto, al final, las redes neuronales convolucionales trabajan en comprender la información que tienen estas matrices bidimensionales que son las imágenes. La convolución es una operación matemática que nos permite, dado un filtro, que no es más que una matriz generalmente de nueve números, de tres por tres, aplicar esta operación con una ventana deslizante, es decir, recorriendo toda la imagen de entrada para obtener al final una imagen diferente, procesada con una información específica resaltada. Las aplicaciones que podemos ver en redes neuronales convolucionales son extremadamente variadas. Por ejemplo, una de las más conocidas es el reconocimiento facial. Gracias a una red neuronal convolucional, podemos detectar las caras que hay en una imagen, es decir, dónde están esas caras dentro de la imagen, y posteriormente hacer una identificación de estas caras con respecto a una base de datos. Por ejemplo, el reconocimiento facial en China está en la orden del día, y por tanto, todos sus ciudadanos están altamente identificados de forma automática gracias a estos sistemas. Las aplicaciones de la inteligencia artificial, como cualquier otra herramienta, pueden ser positivas o negativas, dependiendo del análisis y de las consecuencias que tengan. Sin embargo, son herramientas extremadamente potentes. De hecho, volviendo al caso de China, se descubrió a un fugitivo entre 60,000 personas en un evento, gracias a este reconocimiento facial. Esto nos hace darnos cuenta del potencial que tiene esto, de la precisión a la que estos algoritmos están llegando, ya que en un contexto muy caótico, como es un evento de 60,000 personas, la inteligencia artificial, concretamente la basada en redes neuronales convolucionales para detección de rostros, es capaz de identificar a un fugitivo.

Adicionalmente, el uso de las redes neuronales convolucionales para el análisis y proceso de imágenes médicas es amplísimo, ya que nos permiten, por ejemplo, hacer segmentaciones volumétricas de imágenes en 3D, como los TAC o las resonancias magnéticas, para obtener volúmenes de regiones de interés, como pueden ser pulmones, hígado, cerebro, etcétera. Además, estas redes neuronales también se utilizan para extraer información y detectar diferentes patologías, como el COVID, nódulos y una gran variedad de enfermedades pulmonares, o incluso de cualquier otra parte del cuerpo. Estas aplicaciones están creciendo de forma constante; cada día, cada año, son más numerosas las diferentes empresas y startups que se dedican a este sector.

Evidentemente, otra aplicación muy común de las redes neuronales convolucionales son los coches autónomos. Al final, un coche autónomo es un vehículo que tiene que replicar la conducción de una persona, y para ello es primordial la evaluación de la información del entorno. Esta información se evalúa mediante diferentes sensores, pero uno de ellos son las cámaras. Por tanto, las cámaras producen imágenes que son el objetivo ideal para ser analizadas con redes neuronales convolucionales, que detectan desde objetos en la vía, como coches, peatones, ciclistas, etcétera, hasta las líneas del carril, semáforos, entre otras muchas cosas. Por lo tanto, también son clave para este sector.

He mencionado algunas aplicaciones de las GAN, pero también podemos ver otras, como por ejemplo, la alteración de imágenes médicas. Esta sería una aplicación negativa, ya que las redes neuronales también se pueden utilizar con fines nocivos, y por lo tanto, hay que estar prevenidos ante esta clase de situaciones. En un estudio, lo que hicieron fue evaluar cómo, al alterar imágenes médicas mediante estas GAN, los científicos que realizaron el estudio eran capaces de engañar a los radiólogos expertos en observar las imágenes médicas. Lo que hicieron fue incluir en imágenes de sujetos sanos nódulos pulmonares y en imágenes de sujetos con cáncer, eliminar de forma artificial estos nódulos. Los resultados mostraron que tanto en la inclusión de tumores artificiales como en la sustracción de tumores reales, se obtenían altas tasas de éxito a la hora de inducir a error en el diagnóstico clínico, de hecho, en más del 95% en ambas tareas, una de ellas llegando al 99%, concretamente la de inclusión de tumores artificiales.

Otra de las aplicaciones no positivas para la sociedad que podríamos evaluar de las redes neuronales generativas serían, por ejemplo, los deepfakes, en los que somos capaces de generar, de forma artificial, vídeo y audio de una persona concreta, para hacer creer al resto que esa persona ha dicho unas declaraciones concretas o ha realizado una acción específica, induciendo error a la población sobre lo que esa persona ha hecho y, por lo tanto, dañando su imagen. De hecho, hay decenas de vídeos en Internet en los que podemos ver esta clase de situaciones.

Las redes neuronales generativas adversarias son un tipo de red neuronal que nos permite generar datos artificiales. Para resolver esta tarea de forma óptima, entrenamos dos redes neuronales: una red neuronal que es la encargada de generar estos datos artificiales y otra red neuronal que es la encargada de inferir o discriminar si estos datos son reales o falsos. Por lo tanto, cuando hacemos diversas iteraciones de entrenamiento, la red neuronal generadora se vuelve muy buena creando imágenes que se parecen a la realidad, porque intenta engañar a la red discriminadora, que es la red que intenta detectar si estas imágenes son reales o ficticias. Así, entramos en un proceso, un círculo virtuoso, en el que cada vez la imagen generadora produce mejores imágenes o más parecidas a la realidad, y la red discriminadora se vuelve cada vez más efectiva detectando estas imágenes que son falsas. Por tanto, se establece un juego de superación entre ambas redes, logrando que la red generadora, finalmente, que es nuestro objetivo, genere imágenes muy similares a las imágenes reales.

Las GAN al final se pueden utilizar para una diversidad de tareas que tengan como objetivo generar información de una distribución de datos específica que antes no existía, como por ejemplo la generación de caras. De hecho, hay una página web que cada vez que la refrescas te muestra una cara de una persona que no existe, y es extremadamente realista. Por lo tanto, las GAN nos permiten generar datos de cualquier distribución con la que las hayamos entrenado y, por tanto, replicar toda esta información. También pueden servir para, por ejemplo, pintar, añadir color a imágenes que antes no lo tenían. Por ejemplo, dada una imagen en blanco y negro, se puede conseguir una imagen a color a través de técnicas de coloreado de imagen. Nos permiten realizar cualquier tipo de técnica que tenga como salida una imagen que debería ser realista, es decir, la generación de datos que son realmente plausibles, como por ejemplo, la coloración de una imagen, la generación de una cara artificial, la generación de imágenes que sean realistas para tener más datos con los que entrenar otras redes neuronales, etcétera.

Ahora tenemos los transformers, que son una arquitectura que ha revolucionado el campo de la inteligencia artificial en los últimos dos años, ya que han iniciado un proceso de transformación en el procesamiento del lenguaje natural. Esto permite superar con creces los resultados que se obtenían con las redes neuronales recurrentes por dos motivos. Los transformers pueden obtener, sin un coste computacional disparado, la relación de cada una de las palabras de un texto con el resto de palabras. Por lo tanto, se pueden obtener dependencias de largo alcance entre unas palabras y otras, aunque estén alejadas en una frase, algo en lo que las redes neuronales recurrentes no eran tan óptimas. Además, nos permiten hacer este tipo de relaciones y entrenamientos en paralelo, lo que nos permite incluir bases de datos mucho más grandes en estas redes neuronales y, por lo tanto, hacerlas crecer mucho más en términos de parámetros y, finalmente, de desempeño.

Uno de los ejemplos más disruptivos de los últimos años de transformers ha sido el GPT-3, que no es más que un generador de texto tan preciso y realista que puede ser utilizado en multitud de diferentes tareas de generación de texto, como puede ser la creación de contenido, la comprensión de contenido a través de un resumen del mismo, la traducción de ese contenido, el diseño incluso de aplicaciones que se basen en esta generación de texto para diversas aplicaciones finales, la programación automática desde un punto de vista informático, es decir, se han creado herramientas que permiten a los desarrolladores tener una generación automática del código que desean realizar, o al menos una sugerencia que después pueden adaptar, haciendo mucho más eficiente el trabajo, y toda clase de tareas de generación de contenido online que se puedan imaginar, porque tenemos una herramienta que es capaz de generar texto extremadamente veraz, por lo tanto, podemos hacer que esta herramienta específica escriba sobre multitud de temas.

Otra aplicación que ha tenido un gran impacto en los últimos años de transformers es la generación de imágenes a partir de un texto descriptivo. Es decir, podemos generar cualquier tipo de imagen dada una descripción específica. Esto nos permite, de alguna manera, acelerar o hacer más eficiente el proceso creativo, ya que, dado un texto, podemos, en cuestión de segundos, tener una imagen o incluso una enumeración de distintas imágenes artificiales que son creadas a partir de ese texto, lo que puede inspirar a distintos grupos de diseño. Incluso se han creado NFTs a partir de este arte hecho por redes neuronales, que al final aportan un valor de mercado muy relevante.

Finalmente, lo que pondría el foco es que la inteligencia artificial, y por lo tanto el Deep Learning, se centra en automatizar una serie de tareas específicas a partir de datos concretos. Estamos muy lejos de esta inteligencia artificial general, que puede estar en el imaginario colectivo por las películas, que es capaz de tomar decisiones autónomas y realizar una multitud de tareas. No, lo que tenemos son sistemas específicos que resuelven tareas concretas de forma aislada, no un sistema global que puede resolver diversas tareas. Aunque la inteligencia artificial, como hemos comentado, tiene ya muchos años, es decir, se empezó a desarrollar en la década de los 50 y 60, su uso decayó debido a que no teníamos ni los datos ni el poder de computación necesarios. En esta era de desarrollo de inteligencia artificial, no se había dado cuenta de que había un problema. Esto nos ha dado la oportunidad de ver el desarrollo de inteligencia artificial, que está proporcionando un gran valor de mercado. Por lo tanto, no estamos hablando de una tecnología de moda que pasará y su uso desaparecerá, sino de una tecnología que está aportando valor de mercado, que cada vez aporta más valor y, por tanto, es una tecnología que está impactando e impactará nuestras vidas de forma muy pronunciada.

Sin embargo, teniendo en cuenta todo lo mencionado y el valor de mercado que ya está produciendo, hay que tener en cuenta que solo estamos al principio. Este último boom, esta última serie de desarrollos en la inteligencia artificial, se han empezado a producir mayoritariamente a partir de 2012. Por tanto, solo llevamos una década viendo el potencial que estas técnicas pueden tener, pero el cambio, la adopción y la evolución de estas técnicas están creciendo de forma exponencial. Cada vez veremos aplicaciones que nos sorprenderán más. Incluso ya somos capaces de ver todas estas aplicaciones que son capaces de generar información tan real que pueden engañar al ojo humano. Esto es solo el principio; por tanto, este es un campo apasionante que, sin duda, seguiremos con detenimiento.